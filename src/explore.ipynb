{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c311b1",
   "metadata": {},
   "source": [
    "# EXPLORED EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95d00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# IMPORTS\n",
    "# -------------------------------\n",
    "\n",
    "# Main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Checking correlation between attributes\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CLASSIFICATION MODELS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# REGRESSION MODELS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n",
    "\n",
    "# Models optimization\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402175fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0) LOAD RAW DATAFRAME\n",
      "- ✅ DataFrame loaded sucessfully!\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 0) LOAD RAW DATAFRAME\n",
    "# -------------------------------\n",
    "print(\"STEP 0) LOAD RAW DATAFRAME\")\n",
    "\n",
    "# Set inputs\n",
    "data_separator = \",\"\n",
    "input_path = \"/workspaces/linear-regression-2-project/data/raw/internal-link.csv\"\n",
    "\n",
    "# Read DataFrame\n",
    "df_raw=pd.read_csv(input_path, sep = data_separator)\n",
    "\n",
    "print(\"- ✅ DataFrame loaded sucessfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e7ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1) EXPLORE DATAFRAME\n",
      "- ℹ️ Shape of the original DataFrame: (3140, 108)\n",
      "- ℹ️ Content of the original DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>TOT_POP</th>\n",
       "      <th>0-9</th>\n",
       "      <th>0-9 y/o % of total pop</th>\n",
       "      <th>19-Oct</th>\n",
       "      <th>10-19 y/o % of total pop</th>\n",
       "      <th>20-29</th>\n",
       "      <th>20-29 y/o % of total pop</th>\n",
       "      <th>30-39</th>\n",
       "      <th>30-39 y/o % of total pop</th>\n",
       "      <th>...</th>\n",
       "      <th>COPD_number</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>diabetes_Lower 95% CI</th>\n",
       "      <th>diabetes_Upper 95% CI</th>\n",
       "      <th>diabetes_number</th>\n",
       "      <th>CKD_prevalence</th>\n",
       "      <th>CKD_Lower 95% CI</th>\n",
       "      <th>CKD_Upper 95% CI</th>\n",
       "      <th>CKD_number</th>\n",
       "      <th>Urban_rural_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>55601</td>\n",
       "      <td>6787</td>\n",
       "      <td>12.206615</td>\n",
       "      <td>7637</td>\n",
       "      <td>13.735364</td>\n",
       "      <td>6878</td>\n",
       "      <td>12.370281</td>\n",
       "      <td>7089</td>\n",
       "      <td>12.749771</td>\n",
       "      <td>...</td>\n",
       "      <td>3644</td>\n",
       "      <td>12.9</td>\n",
       "      <td>11.9</td>\n",
       "      <td>13.8</td>\n",
       "      <td>5462</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1326</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>218022</td>\n",
       "      <td>24757</td>\n",
       "      <td>11.355276</td>\n",
       "      <td>26913</td>\n",
       "      <td>12.344167</td>\n",
       "      <td>23579</td>\n",
       "      <td>10.814964</td>\n",
       "      <td>25213</td>\n",
       "      <td>11.564429</td>\n",
       "      <td>...</td>\n",
       "      <td>14692</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>20520</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5479</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>24881</td>\n",
       "      <td>2732</td>\n",
       "      <td>10.980266</td>\n",
       "      <td>2960</td>\n",
       "      <td>11.896628</td>\n",
       "      <td>3268</td>\n",
       "      <td>13.134520</td>\n",
       "      <td>3201</td>\n",
       "      <td>12.865239</td>\n",
       "      <td>...</td>\n",
       "      <td>2373</td>\n",
       "      <td>19.7</td>\n",
       "      <td>18.6</td>\n",
       "      <td>20.6</td>\n",
       "      <td>3870</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>887</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>22400</td>\n",
       "      <td>2456</td>\n",
       "      <td>10.964286</td>\n",
       "      <td>2596</td>\n",
       "      <td>11.589286</td>\n",
       "      <td>3029</td>\n",
       "      <td>13.522321</td>\n",
       "      <td>3113</td>\n",
       "      <td>13.897321</td>\n",
       "      <td>...</td>\n",
       "      <td>1789</td>\n",
       "      <td>14.1</td>\n",
       "      <td>13.2</td>\n",
       "      <td>14.9</td>\n",
       "      <td>2511</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>595</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>57840</td>\n",
       "      <td>7095</td>\n",
       "      <td>12.266598</td>\n",
       "      <td>7570</td>\n",
       "      <td>13.087828</td>\n",
       "      <td>6742</td>\n",
       "      <td>11.656293</td>\n",
       "      <td>6884</td>\n",
       "      <td>11.901798</td>\n",
       "      <td>...</td>\n",
       "      <td>4661</td>\n",
       "      <td>13.5</td>\n",
       "      <td>12.6</td>\n",
       "      <td>14.5</td>\n",
       "      <td>6017</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1507</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fips  TOT_POP    0-9  0-9 y/o % of total pop  19-Oct  \\\n",
       "0  1001    55601   6787               12.206615    7637   \n",
       "1  1003   218022  24757               11.355276   26913   \n",
       "2  1005    24881   2732               10.980266    2960   \n",
       "3  1007    22400   2456               10.964286    2596   \n",
       "4  1009    57840   7095               12.266598    7570   \n",
       "\n",
       "   10-19 y/o % of total pop  20-29  20-29 y/o % of total pop  30-39  \\\n",
       "0                 13.735364   6878                 12.370281   7089   \n",
       "1                 12.344167  23579                 10.814964  25213   \n",
       "2                 11.896628   3268                 13.134520   3201   \n",
       "3                 11.589286   3029                 13.522321   3113   \n",
       "4                 13.087828   6742                 11.656293   6884   \n",
       "\n",
       "   30-39 y/o % of total pop  ...  COPD_number  diabetes_prevalence  \\\n",
       "0                 12.749771  ...         3644                 12.9   \n",
       "1                 11.564429  ...        14692                 12.0   \n",
       "2                 12.865239  ...         2373                 19.7   \n",
       "3                 13.897321  ...         1789                 14.1   \n",
       "4                 11.901798  ...         4661                 13.5   \n",
       "\n",
       "   diabetes_Lower 95% CI  diabetes_Upper 95% CI  diabetes_number  \\\n",
       "0                   11.9                   13.8             5462   \n",
       "1                   11.0                   13.1            20520   \n",
       "2                   18.6                   20.6             3870   \n",
       "3                   13.2                   14.9             2511   \n",
       "4                   12.6                   14.5             6017   \n",
       "\n",
       "   CKD_prevalence  CKD_Lower 95% CI  CKD_Upper 95% CI  CKD_number  \\\n",
       "0             3.1               2.9               3.3        1326   \n",
       "1             3.2               3.0               3.5        5479   \n",
       "2             4.5               4.2               4.8         887   \n",
       "3             3.3               3.1               3.6         595   \n",
       "4             3.4               3.2               3.7        1507   \n",
       "\n",
       "   Urban_rural_code  \n",
       "0                 3  \n",
       "1                 4  \n",
       "2                 6  \n",
       "3                 2  \n",
       "4                 2  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ℹ️ Info of the original DataFrame (dataType and non-null values):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3140 entries, 0 to 3139\n",
      "Data columns (total 108 columns):\n",
      " #    Column                                                                         Non-Null Count  Dtype  \n",
      "---   ------                                                                         --------------  -----  \n",
      " 0    fips                                                                           3140 non-null   int64  \n",
      " 1    TOT_POP                                                                        3140 non-null   int64  \n",
      " 2    0-9                                                                            3140 non-null   int64  \n",
      " 3    0-9 y/o % of total pop                                                         3140 non-null   float64\n",
      " 4    19-Oct                                                                         3140 non-null   int64  \n",
      " 5    10-19 y/o % of total pop                                                       3140 non-null   float64\n",
      " 6    20-29                                                                          3140 non-null   int64  \n",
      " 7    20-29 y/o % of total pop                                                       3140 non-null   float64\n",
      " 8    30-39                                                                          3140 non-null   int64  \n",
      " 9    30-39 y/o % of total pop                                                       3140 non-null   float64\n",
      " 10   40-49                                                                          3140 non-null   int64  \n",
      " 11   40-49 y/o % of total pop                                                       3140 non-null   float64\n",
      " 12   50-59                                                                          3140 non-null   int64  \n",
      " 13   50-59 y/o % of total pop                                                       3140 non-null   float64\n",
      " 14   60-69                                                                          3140 non-null   int64  \n",
      " 15   60-69 y/o % of total pop                                                       3140 non-null   float64\n",
      " 16   70-79                                                                          3140 non-null   int64  \n",
      " 17   70-79 y/o % of total pop                                                       3140 non-null   float64\n",
      " 18   80+                                                                            3140 non-null   int64  \n",
      " 19   80+ y/o % of total pop                                                         3140 non-null   float64\n",
      " 20   White-alone pop                                                                3140 non-null   int64  \n",
      " 21   % White-alone                                                                  3140 non-null   float64\n",
      " 22   Black-alone pop                                                                3140 non-null   int64  \n",
      " 23   % Black-alone                                                                  3140 non-null   float64\n",
      " 24   Native American/American Indian-alone pop                                      3140 non-null   int64  \n",
      " 25   % NA/AI-alone                                                                  3140 non-null   float64\n",
      " 26   Asian-alone pop                                                                3140 non-null   int64  \n",
      " 27   % Asian-alone                                                                  3140 non-null   float64\n",
      " 28   Hawaiian/Pacific Islander-alone pop                                            3140 non-null   int64  \n",
      " 29   % Hawaiian/PI-alone                                                            3140 non-null   float64\n",
      " 30   Two or more races pop                                                          3140 non-null   int64  \n",
      " 31   % Two or more races                                                            3140 non-null   float64\n",
      " 32   POP_ESTIMATE_2018                                                              3140 non-null   int64  \n",
      " 33   N_POP_CHG_2018                                                                 3140 non-null   int64  \n",
      " 34   GQ_ESTIMATES_2018                                                              3140 non-null   int64  \n",
      " 35   R_birth_2018                                                                   3140 non-null   float64\n",
      " 36   R_death_2018                                                                   3140 non-null   float64\n",
      " 37   R_NATURAL_INC_2018                                                             3140 non-null   float64\n",
      " 38   R_INTERNATIONAL_MIG_2018                                                       3140 non-null   float64\n",
      " 39   R_DOMESTIC_MIG_2018                                                            3140 non-null   float64\n",
      " 40   R_NET_MIG_2018                                                                 3140 non-null   float64\n",
      " 41   Less than a high school diploma 2014-18                                        3140 non-null   int64  \n",
      " 42   High school diploma only 2014-18                                               3140 non-null   int64  \n",
      " 43   Some college or associate's degree 2014-18                                     3140 non-null   int64  \n",
      " 44   Bachelor's degree or higher 2014-18                                            3140 non-null   int64  \n",
      " 45   Percent of adults with less than a high school diploma 2014-18                 3140 non-null   float64\n",
      " 46   Percent of adults with a high school diploma only 2014-18                      3140 non-null   float64\n",
      " 47   Percent of adults completing some college or associate's degree 2014-18        3140 non-null   float64\n",
      " 48   Percent of adults with a bachelor's degree or higher 2014-18                   3140 non-null   float64\n",
      " 49   POVALL_2018                                                                    3140 non-null   int64  \n",
      " 50   PCTPOVALL_2018                                                                 3140 non-null   float64\n",
      " 51   PCTPOV017_2018                                                                 3140 non-null   float64\n",
      " 52   PCTPOV517_2018                                                                 3140 non-null   float64\n",
      " 53   MEDHHINC_2018                                                                  3140 non-null   int64  \n",
      " 54   CI90LBINC_2018                                                                 3140 non-null   int64  \n",
      " 55   CI90UBINC_2018                                                                 3140 non-null   int64  \n",
      " 56   Civilian_labor_force_2018                                                      3140 non-null   int64  \n",
      " 57   Employed_2018                                                                  3140 non-null   int64  \n",
      " 58   Unemployed_2018                                                                3140 non-null   int64  \n",
      " 59   Unemployment_rate_2018                                                         3140 non-null   float64\n",
      " 60   Median_Household_Income_2018                                                   3140 non-null   int64  \n",
      " 61   Med_HH_Income_Percent_of_State_Total_2018                                      3140 non-null   float64\n",
      " 62   Active Physicians per 100000 Population 2018 (AAMC)                            3140 non-null   float64\n",
      " 63   Total Active Patient Care Physicians per 100000 Population 2018 (AAMC)         3140 non-null   float64\n",
      " 64   Active Primary Care Physicians per 100000 Population 2018 (AAMC)               3140 non-null   float64\n",
      " 65   Active Patient Care Primary Care Physicians per 100000 Population 2018 (AAMC)  3140 non-null   float64\n",
      " 66   Active General Surgeons per 100000 Population 2018 (AAMC)                      3140 non-null   float64\n",
      " 67   Active Patient Care General Surgeons per 100000 Population 2018 (AAMC)         3140 non-null   float64\n",
      " 68   Total nurse practitioners (2019)                                               3140 non-null   float64\n",
      " 69   Total physician assistants (2019)                                              3140 non-null   float64\n",
      " 70   Total Hospitals (2019)                                                         3140 non-null   float64\n",
      " 71   Internal Medicine Primary Care (2019)                                          3140 non-null   float64\n",
      " 72   Family Medicine/General Practice Primary Care (2019)                           3140 non-null   float64\n",
      " 73   Total Specialist Physicians (2019)                                             3140 non-null   float64\n",
      " 74   ICU Beds_x                                                                     3140 non-null   int64  \n",
      " 75   Total Population                                                               3140 non-null   int64  \n",
      " 76   Population Aged 60+                                                            3140 non-null   int64  \n",
      " 77   Percent of Population Aged 60+                                                 3140 non-null   float64\n",
      " 78   COUNTY_NAME                                                                    3140 non-null   object \n",
      " 79   STATE_NAME                                                                     3140 non-null   object \n",
      " 80   STATE_FIPS                                                                     3140 non-null   int64  \n",
      " 81   CNTY_FIPS                                                                      3140 non-null   int64  \n",
      " 82   county_pop2018_18 and older                                                    3140 non-null   int64  \n",
      " 83   anycondition_prevalence                                                        3140 non-null   float64\n",
      " 84   anycondition_Lower 95% CI                                                      3140 non-null   float64\n",
      " 85   anycondition_Upper 95% CI                                                      3140 non-null   float64\n",
      " 86   anycondition_number                                                            3140 non-null   int64  \n",
      " 87   Obesity_prevalence                                                             3140 non-null   float64\n",
      " 88   Obesity_Lower 95% CI                                                           3140 non-null   float64\n",
      " 89   Obesity_Upper 95% CI                                                           3140 non-null   float64\n",
      " 90   Obesity_number                                                                 3140 non-null   int64  \n",
      " 91   Heart disease_prevalence                                                       3140 non-null   float64\n",
      " 92   Heart disease_Lower 95% CI                                                     3140 non-null   float64\n",
      " 93   Heart disease_Upper 95% CI                                                     3140 non-null   float64\n",
      " 94   Heart disease_number                                                           3140 non-null   int64  \n",
      " 95   COPD_prevalence                                                                3140 non-null   float64\n",
      " 96   COPD_Lower 95% CI                                                              3140 non-null   float64\n",
      " 97   COPD_Upper 95% CI                                                              3140 non-null   float64\n",
      " 98   COPD_number                                                                    3140 non-null   int64  \n",
      " 99   diabetes_prevalence                                                            3140 non-null   float64\n",
      " 100  diabetes_Lower 95% CI                                                          3140 non-null   float64\n",
      " 101  diabetes_Upper 95% CI                                                          3140 non-null   float64\n",
      " 102  diabetes_number                                                                3140 non-null   int64  \n",
      " 103  CKD_prevalence                                                                 3140 non-null   float64\n",
      " 104  CKD_Lower 95% CI                                                               3140 non-null   float64\n",
      " 105  CKD_Upper 95% CI                                                               3140 non-null   float64\n",
      " 106  CKD_number                                                                     3140 non-null   int64  \n",
      " 107  Urban_rural_code                                                               3140 non-null   int64  \n",
      "dtypes: float64(61), int64(45), object(2)\n",
      "memory usage: 2.6+ MB\n",
      "\n",
      "- ℹ️ Ordered info by number of non-null values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Non-Null Count</th>\n",
       "      <th>Null Count</th>\n",
       "      <th>Dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fips</th>\n",
       "      <td>fips</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOT_POP</th>\n",
       "      <td>TOT_POP</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-9</th>\n",
       "      <td>0-9</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-9 y/o % of total pop</th>\n",
       "      <td>0-9 y/o % of total pop</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19-Oct</th>\n",
       "      <td>19-Oct</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CKD_prevalence</th>\n",
       "      <td>CKD_prevalence</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CKD_Lower 95% CI</th>\n",
       "      <td>CKD_Lower 95% CI</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CKD_Upper 95% CI</th>\n",
       "      <td>CKD_Upper 95% CI</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CKD_number</th>\n",
       "      <td>CKD_number</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urban_rural_code</th>\n",
       "      <td>Urban_rural_code</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Column  Non-Null Count  Null Count  \\\n",
       "fips                                      fips            3140           0   \n",
       "TOT_POP                                TOT_POP            3140           0   \n",
       "0-9                                        0-9            3140           0   \n",
       "0-9 y/o % of total pop  0-9 y/o % of total pop            3140           0   \n",
       "19-Oct                                  19-Oct            3140           0   \n",
       "...                                        ...             ...         ...   \n",
       "CKD_prevalence                  CKD_prevalence            3140           0   \n",
       "CKD_Lower 95% CI              CKD_Lower 95% CI            3140           0   \n",
       "CKD_Upper 95% CI              CKD_Upper 95% CI            3140           0   \n",
       "CKD_number                          CKD_number            3140           0   \n",
       "Urban_rural_code              Urban_rural_code            3140           0   \n",
       "\n",
       "                          Dtype  \n",
       "fips                      int64  \n",
       "TOT_POP                   int64  \n",
       "0-9                       int64  \n",
       "0-9 y/o % of total pop  float64  \n",
       "19-Oct                    int64  \n",
       "...                         ...  \n",
       "CKD_prevalence          float64  \n",
       "CKD_Lower 95% CI        float64  \n",
       "CKD_Upper 95% CI        float64  \n",
       "CKD_number                int64  \n",
       "Urban_rural_code          int64  \n",
       "\n",
       "[108 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - ℹ️ Final DataFrame unique attributes (unsorted):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Unique_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fips</td>\n",
       "      <td>3140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TOT_POP</td>\n",
       "      <td>3074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-9</td>\n",
       "      <td>2723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-9 y/o % of total pop</td>\n",
       "      <td>3136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19-Oct</td>\n",
       "      <td>2743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>CKD_prevalence</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>CKD_Lower 95% CI</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>CKD_Upper 95% CI</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>CKD_number</td>\n",
       "      <td>1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Urban_rural_code</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Column  Unique_Count\n",
       "0                      fips          3140\n",
       "1                   TOT_POP          3074\n",
       "2                       0-9          2723\n",
       "3    0-9 y/o % of total pop          3136\n",
       "4                    19-Oct          2743\n",
       "..                      ...           ...\n",
       "103          CKD_prevalence            43\n",
       "104        CKD_Lower 95% CI            39\n",
       "105        CKD_Upper 95% CI            46\n",
       "106              CKD_number          1894\n",
       "107        Urban_rural_code             6\n",
       "\n",
       "[108 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - ℹ️ Ordered unique attributes (fewest unique first):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Unique_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Urban_rural_code</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Active Patient Care General Surgeons per 10000...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Active General Surgeons per 100000 Population ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>CKD_Lower 95% CI</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>CKD_prevalence</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>60-69 y/o % of total pop</td>\n",
       "      <td>3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>80+ y/o % of total pop</td>\n",
       "      <td>3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>70-79 y/o % of total pop</td>\n",
       "      <td>3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>% White-alone</td>\n",
       "      <td>3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fips</td>\n",
       "      <td>3140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Column  Unique_Count\n",
       "107                                   Urban_rural_code             6\n",
       "67   Active Patient Care General Surgeons per 10000...            30\n",
       "66   Active General Surgeons per 100000 Population ...            32\n",
       "104                                   CKD_Lower 95% CI            39\n",
       "103                                     CKD_prevalence            43\n",
       "..                                                 ...           ...\n",
       "15                            60-69 y/o % of total pop          3139\n",
       "19                              80+ y/o % of total pop          3139\n",
       "17                            70-79 y/o % of total pop          3139\n",
       "21                                       % White-alone          3139\n",
       "0                                                 fips          3140\n",
       "\n",
       "[108 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 1) EXPLORE DATAFRAME\n",
    "# -------------------------------\n",
    "print(\"STEP 1) EXPLORE DATAFRAME\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S1 = df_raw.copy()\n",
    "\n",
    "# Print shape\n",
    "print(f\"- ℹ️ Shape of the original DataFrame: {df_S1.shape}\")\n",
    "\n",
    "# Show first rows\n",
    "print(\"- ℹ️ Content of the original DataFrame:\")\n",
    "display(df_S1.head(5))\n",
    "\n",
    "# Show dataframe info\n",
    "print(\"- ℹ️ Info of the original DataFrame (dataType and non-null values):\")\n",
    "df_S1.info(verbose=True, show_counts=True)\n",
    "\n",
    "# Ordered info (fewest non-null first)\n",
    "print(\"\\n- ℹ️ Ordered info by number of non-null values:\")\n",
    "ordered_info = pd.DataFrame({\n",
    "    \"Column\": df_S1.columns,\n",
    "    \"Non-Null Count\": df_S1.notnull().sum(),\n",
    "    \"Null Count\": df_S1.isnull().sum(),\n",
    "    \"Dtype\": df_S1.dtypes.astype(str)\n",
    "}).sort_values(by=\"Non-Null Count\", ascending=True)\n",
    "\n",
    "display(ordered_info)\n",
    "\n",
    "# Count unique attributes (unsorted)\n",
    "df_S1_summary = pd.DataFrame({\n",
    "    \"Column\": df_S1.columns,\n",
    "    \"Unique_Count\": df_S1.nunique().values\n",
    "})\n",
    "print(\"\\n - ℹ️ Final DataFrame unique attributes (unsorted):\")\n",
    "display(df_S1_summary)\n",
    "\n",
    "# Ordered summary (fewest unique values first)\n",
    "print(\"\\n - ℹ️ Ordered unique attributes (fewest unique first):\")\n",
    "df_S1_summary_ordered = df_S1_summary.sort_values(by=\"Unique_Count\", ascending=True)\n",
    "display(df_S1_summary_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1c501",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- there are not non-null values in the data -> nice\n",
    "- charges is going to be the target variable of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "206eefd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fips\n",
      "TOT_POP\n",
      "0-9\n",
      "0-9 y/o % of total pop\n",
      "19-Oct\n",
      "10-19 y/o % of total pop\n",
      "20-29\n",
      "20-29 y/o % of total pop\n",
      "30-39\n",
      "30-39 y/o % of total pop\n",
      "40-49\n",
      "40-49 y/o % of total pop\n",
      "50-59\n",
      "50-59 y/o % of total pop\n",
      "60-69\n",
      "60-69 y/o % of total pop\n",
      "70-79\n",
      "70-79 y/o % of total pop\n",
      "80+\n",
      "80+ y/o % of total pop\n",
      "White-alone pop\n",
      "% White-alone\n",
      "Black-alone pop\n",
      "% Black-alone\n",
      "Native American/American Indian-alone pop\n",
      "% NA/AI-alone\n",
      "Asian-alone pop\n",
      "% Asian-alone\n",
      "Hawaiian/Pacific Islander-alone pop\n",
      "% Hawaiian/PI-alone\n",
      "Two or more races pop\n",
      "% Two or more races\n",
      "POP_ESTIMATE_2018\n",
      "N_POP_CHG_2018\n",
      "GQ_ESTIMATES_2018\n",
      "R_birth_2018\n",
      "R_death_2018\n",
      "R_NATURAL_INC_2018\n",
      "R_INTERNATIONAL_MIG_2018\n",
      "R_DOMESTIC_MIG_2018\n",
      "R_NET_MIG_2018\n",
      "Less than a high school diploma 2014-18\n",
      "High school diploma only 2014-18\n",
      "Some college or associate's degree 2014-18\n",
      "Bachelor's degree or higher 2014-18\n",
      "Percent of adults with less than a high school diploma 2014-18\n",
      "Percent of adults with a high school diploma only 2014-18\n",
      "Percent of adults completing some college or associate's degree 2014-18\n",
      "Percent of adults with a bachelor's degree or higher 2014-18\n",
      "POVALL_2018\n",
      "PCTPOVALL_2018\n",
      "PCTPOV017_2018\n",
      "PCTPOV517_2018\n",
      "MEDHHINC_2018\n",
      "CI90LBINC_2018\n",
      "CI90UBINC_2018\n",
      "Civilian_labor_force_2018\n",
      "Employed_2018\n",
      "Unemployed_2018\n",
      "Unemployment_rate_2018\n",
      "Median_Household_Income_2018\n",
      "Med_HH_Income_Percent_of_State_Total_2018\n",
      "Active Physicians per 100000 Population 2018 (AAMC)\n",
      "Total Active Patient Care Physicians per 100000 Population 2018 (AAMC)\n",
      "Active Primary Care Physicians per 100000 Population 2018 (AAMC)\n",
      "Active Patient Care Primary Care Physicians per 100000 Population 2018 (AAMC)\n",
      "Active General Surgeons per 100000 Population 2018 (AAMC)\n",
      "Active Patient Care General Surgeons per 100000 Population 2018 (AAMC)\n",
      "Total nurse practitioners (2019)\n",
      "Total physician assistants (2019)\n",
      "Total Hospitals (2019)\n",
      "Internal Medicine Primary Care (2019)\n",
      "Family Medicine/General Practice Primary Care (2019)\n",
      "Total Specialist Physicians (2019)\n",
      "ICU Beds_x\n",
      "Total Population\n",
      "Population Aged 60+\n",
      "Percent of Population Aged 60+\n",
      "COUNTY_NAME\n",
      "STATE_NAME\n",
      "STATE_FIPS\n",
      "CNTY_FIPS\n",
      "county_pop2018_18 and older\n",
      "anycondition_prevalence\n",
      "anycondition_Lower 95% CI\n",
      "anycondition_Upper 95% CI\n",
      "anycondition_number\n",
      "Obesity_prevalence\n",
      "Obesity_Lower 95% CI\n",
      "Obesity_Upper 95% CI\n",
      "Obesity_number\n",
      "Heart disease_prevalence\n",
      "Heart disease_Lower 95% CI\n",
      "Heart disease_Upper 95% CI\n",
      "Heart disease_number\n",
      "COPD_prevalence\n",
      "COPD_Lower 95% CI\n",
      "COPD_Upper 95% CI\n",
      "COPD_number\n",
      "diabetes_prevalence\n",
      "diabetes_Lower 95% CI\n",
      "diabetes_Upper 95% CI\n",
      "diabetes_number\n",
      "CKD_prevalence\n",
      "CKD_Lower 95% CI\n",
      "CKD_Upper 95% CI\n",
      "CKD_number\n",
      "Urban_rural_code\n"
     ]
    }
   ],
   "source": [
    "for col in df_S1.columns:\n",
    "    print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c38a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 2) REMOVE DUPLICATES\n",
    "# -------------------------------\n",
    "print(\"STEP 2) REMOVE DUPLICATES\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S2 = df_S1.copy()\n",
    "\n",
    "num_duplicates=df_S2.duplicated().sum()\n",
    "if num_duplicates == 0:\n",
    "    df_S2=df_S2\n",
    "    print(\"- ✅ Original DataFrame does not contain duplicates:\")\n",
    "    print(\" - ℹ️ Dataframe shape: \",df_S2.shape)\n",
    "else:\n",
    "    df_S2_duplicates=df_S2[df_S2.duplicated()] #Works as bool mask\n",
    "    df_S2=df_S2.drop_duplicates()\n",
    "    print(\"- ⚠️ Original DataFrame contained \" + str(num_duplicates) + \" duplicates that have been dropped:\")\n",
    "    print(\" - ℹ️ Original df's shape: \",df_S1.shape)\n",
    "    print(\" - ℹ️ Cleaned df's  shape: \",df_S2.shape)\n",
    "    print(\" - ℹ️ These are the dropped duplicates:\")\n",
    "    display(df_S2_duplicates)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b516165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 3) SELECT RELEVANT ATTRIBUTES\n",
    "# -------------------------------\n",
    "print(\"STEP 3) SELECT RELEVANT ATTRIBUTES\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S3 = df_S2.copy()\n",
    "\n",
    "# Drop non-relevant attributes\n",
    "df_S3=df_S3.drop(labels=[], axis =1)\n",
    "\n",
    "# Print results\n",
    "print(\"- ✅ Non-Relevant attributes have been dropped.\")\n",
    "print(f\" - ℹ️ Original df's columns: {len(df_S2.columns)}\")\n",
    "print(f\" - ℹ️ Cleaned df's  columns: {len(df_S3.columns)}\")\n",
    "print(f\" - ℹ️ Final DataFrame shape: {df_S3.shape}\")\n",
    "display(df_S3.head())\n",
    "\n",
    "# Count attributes\n",
    "df_S3_summary = pd.DataFrame({\n",
    "    \"Column\": df_S3.columns,\n",
    "    \"Unique_Count\": df_S3.nunique().values\n",
    "})\n",
    "print(\" - ℹ️ Final DataFrame unique attributes:\")\n",
    "display(df_S3_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 4) CLASSIFY ATTRIBUTES AND TARGET VARIABLE\n",
    "# -------------------------------\n",
    "print(\"STEP 4) CLASSIFY ATTRIBUTES AND TARGET VARIABLE\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S4 = df_S3.copy()\n",
    "\n",
    "# List of columns\n",
    "columns = df_S4.columns.tolist()\n",
    "\n",
    "# Proposal thresholds\n",
    "var_type_proposal_threshold = 0.25 # [%] Under this percentage of unique values, the attribute would be proposed as CATEGORIC\n",
    "numeric_var_subtype_proposal_threshold = 0.05 # [%] Under this percentage of unique values, NUMERIC var would be proposed as DISCRETE\n",
    "\n",
    "# Validation: check heuristic consistency\n",
    "# Ensure thresholds make logical sense (numeric subtype threshold must be lower than the variable type threshold, because DISCRETE/CONTINUOS is a finer classification inside NUMERIC)\n",
    "if numeric_var_subtype_proposal_threshold >= var_type_proposal_threshold:\n",
    "    raise ValueError(\n",
    "        f\"- ❌ Inconsistent thresholds detected: \"\n",
    "        f\"threshold to define subtype ({numeric_var_subtype_proposal_threshold} %) must be LOWER than threshold for type ({var_type_proposal_threshold} %)\")\n",
    "else:\n",
    "    print(\n",
    "        f\"- ✅ Threshold consistency check passed: \"\n",
    "        f\"threshold to define subtype ({numeric_var_subtype_proposal_threshold} %) is LOWER than threshold for type ({var_type_proposal_threshold} %)\")\n",
    "\n",
    "# Iterate through columns\n",
    "category_var_auto = []\n",
    "numeric_var_auto = []\n",
    "for col in df_S4.columns:\n",
    "    col_data = df_S4[col].dropna()\n",
    "    total_rows = len(df_S4)\n",
    "\n",
    "    # Skip empty columns\n",
    "    if total_rows == 0:\n",
    "        continue\n",
    "    \n",
    "    # Define local variables for each loop\n",
    "    unique_count = col_data.nunique()\n",
    "    unique_ratio = unique_count / total_rows * 100\n",
    "    col_dtype = str(df_S4[col].dtype)\n",
    "\n",
    "    # Case 1: text-based columns\n",
    "    if col_dtype in [\"object\", \"category\"]:\n",
    "        category_var_auto.append(col)\n",
    "        continue\n",
    "    # Case 2: integer columns\n",
    "    if col_dtype.startswith(\"int\"):\n",
    "        if unique_ratio <= var_type_proposal_threshold:\n",
    "            category_var_auto.append(col)\n",
    "        else:\n",
    "            numeric_var_auto.append(col)\n",
    "        continue\n",
    "    # Case 3: float columns\n",
    "    if col_dtype.startswith(\"float\"):\n",
    "        if unique_ratio <= var_type_proposal_threshold:\n",
    "            category_var_auto.append(col)\n",
    "        else:\n",
    "            numeric_var_auto.append(col)\n",
    "        continue\n",
    "\n",
    "# Print proposed Data Types\n",
    "print(\"- ℹ️ Proposed CATEGORY Attributes: \" + str(category_var_auto))\n",
    "print(\"- ℹ️ Proposed NUMERIC Attributes: \" + str(numeric_var_auto))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b20374",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- Proposal of CATEGORY vs NUMERIC values matches perfectly the types given by the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7306f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm target variable\n",
    "y_var = \"charges\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25211d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm categories and target variable\n",
    "category_att = []\n",
    "numeric_att = []\n",
    "for att in category_var_auto:\n",
    "    if att != y_var:\n",
    "        category_att.append(att)\n",
    "for att in numeric_var_auto:\n",
    "    if att != y_var:\n",
    "        numeric_att.append(att)\n",
    "\n",
    "# Checking if y_var is binary/multiclass or discrete/continuous\n",
    "y_var_unique_values = df_S4[y_var].unique()\n",
    "if y_var in category_var_auto:\n",
    "    if len(y_var_unique_values) == 2:\n",
    "        y_var_type = \"CATEGORIC\"\n",
    "        y_var_subtype = \"BINARY\"\n",
    "    elif len(y_var_unique_values) > 2:\n",
    "        y_var_type = \"CATEGORIC\"\n",
    "        y_var_subtype = \"MULTICLASS\"\n",
    "    else:\n",
    "        y_var_type = \"CATEGORIC\"\n",
    "        y_var_subtype = \"CONSTANT\"\n",
    "else:\n",
    "    unique_ratio = len(y_var_unique_values) / len(df_S4[y_var]) * 100\n",
    "    if unique_ratio < numeric_var_subtype_proposal_threshold:\n",
    "        y_var_type = \"NUMERIC\"\n",
    "        y_var_subtype = \"DISCRETE\"\n",
    "    else:\n",
    "        y_var_type = \"NUMERIC\"\n",
    "        y_var_subtype = \"CONTINUOUS\"\n",
    "\n",
    "# Checking if attributes are binary/multiclass or discrete/continuous\n",
    "binary_att = []\n",
    "multiclass_att = []\n",
    "constant_att = []\n",
    "for att in category_att:\n",
    "    att_unique_values = df_S4[att].unique()\n",
    "    if len(att_unique_values) == 2:\n",
    "        binary_att.append(att)\n",
    "    elif len(att_unique_values) > 2:\n",
    "        multiclass_att.append(att)\n",
    "    else:\n",
    "        constant_att.append(att)\n",
    "\n",
    "discrete_att = []\n",
    "continuos_att = []\n",
    "for att in numeric_att:\n",
    "    att_unique_values = df_S4[att].unique()\n",
    "    unique_ratio = len(att_unique_values) / len(df_S4[att]) * 100\n",
    "    if unique_ratio < numeric_var_subtype_proposal_threshold:\n",
    "        discrete_att.append(att)\n",
    "    else:\n",
    "        continuos_att.append(att)\n",
    "\n",
    "# Print results\n",
    "print(\"- ℹ️ Confirmed CATEGORY Attributes:\")\n",
    "print(\"   ↳ BINARY: \" + str(binary_att))\n",
    "print(\"   ↳ MULTICLASS: \" + str(multiclass_att))\n",
    "print(\"   ↳ CONSTANT: \" + str(constant_att))\n",
    "print(\"- ℹ️ Confirmed NUMERIC Attributes: \" + str(numeric_att))\n",
    "print(\"   ↳ DISCRETE: \" + str(discrete_att))\n",
    "print(\"   ↳ CONTINUOUS: \" + str(continuos_att))\n",
    "print(\"- ℹ️ Confirmed TARGET Variable: \" + y_var + \" -> \" + y_var_type + \" and \" + y_var_subtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm setup before plottings\n",
    "make_plots = True \n",
    "figHeight_unit = 8 # Unitary figure height\n",
    "figWidth_unit = 12 # Unitary figure width\n",
    "num_cols = 2 # Number of columns per plot\n",
    "my_palette = \"pastel\"\n",
    "my_font_size = 15\n",
    "num_values_to_plot = 40 # Max number of different values to plot (for CATEGORY_var)\n",
    "num_bins = 100# Num of bins (for NUMERIC_var plots)\n",
    "category_combi_att = \"sex\" # Combination attribute for multivariant analysis (must be a CATEGORIC attribute)\n",
    "y_var_highlighting_color = \"green\"\n",
    "\n",
    " # Validation\n",
    "if category_combi_att in category_att:\n",
    "    print(\"- ✅ Sucessfull verification: combination attribute \" +  category_combi_att + \" is CATEGORIC\")\n",
    "elif category_combi_att in numeric_att:\n",
    "    raise ValueError(\"❌ Combination attribute \" +  category_combi_att + \" for multivariant analysis must be a CATEGORY attribute!\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Combination attribute \" +  category_combi_att + \" does not exist in the DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7250bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 5 - UNIVARIABLE ANALYSIS\n",
    "# -------------------------------\n",
    "print(\"STEP 5 - UNIVARIABLE ANALYSIS\")\n",
    "\n",
    "if not make_plots:\n",
    "    print(\"UNIVARIABLE ANALYSIS is not printed, set make_plots = True\")\n",
    "else:\n",
    "\n",
    "    # Copy of previous DataFrame\n",
    "    df_S5 = df_S4.copy()\n",
    "\n",
    "    # Target highlighting styles\n",
    "    target_box_style = dict(facecolor='none', edgecolor=y_var_highlighting_color, linewidth=5)\n",
    "    target_title_style = dict(color=y_var_highlighting_color, fontweight='bold')\n",
    "\n",
    "    # CATEGORY VARIABLES (including target if categorical)\n",
    "    print(\"🏷️ CATEGORY VARIABLES\")\n",
    "\n",
    "    var_to_plot = category_att.copy()\n",
    "    if y_var_type == \"CATEGORIC\" and y_var not in var_to_plot:\n",
    "        var_to_plot.insert(0, y_var)\n",
    "\n",
    "    num_rows = math.ceil(len(var_to_plot) / num_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=num_rows,\n",
    "        ncols=num_cols,\n",
    "        figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows)\n",
    "    )\n",
    "\n",
    "    axes = axes.flatten()\n",
    "    idx = 0\n",
    "\n",
    "    for col in var_to_plot:\n",
    "        unique_count = df_S5[col].nunique()\n",
    "\n",
    "        if unique_count > num_values_to_plot:\n",
    "            order = df_S5[col].value_counts().head(num_values_to_plot).index\n",
    "        else:\n",
    "            order = df_S5[col].value_counts().index\n",
    "\n",
    "        sns.countplot(\n",
    "            ax=axes[idx],\n",
    "            data=df_S5,\n",
    "            x=col,\n",
    "            hue=col,\n",
    "            palette=my_palette,\n",
    "            order=order,\n",
    "            legend=False\n",
    "        )\n",
    "        axes[idx].tick_params(axis='x', rotation=90, labelsize=my_font_size)\n",
    "\n",
    "        # Highlight target\n",
    "        if col == y_var:\n",
    "            axes[idx].set_title(col, **target_title_style)\n",
    "            axes[idx].add_patch(\n",
    "                plt.Rectangle((0, 0), 1, 1, transform=axes[idx].transAxes, **target_box_style)\n",
    "            )\n",
    "        else:\n",
    "            axes[idx].set_title(col)\n",
    "\n",
    "        # Add truncated info\n",
    "        if unique_count > num_values_to_plot:\n",
    "            msg = f\"There are {unique_count} values,\\nbut only {num_values_to_plot} have been plotted\"\n",
    "            axes[idx].text(\n",
    "                0.5, 0.9, msg,\n",
    "                transform=axes[idx].transAxes,\n",
    "                fontsize=my_font_size,\n",
    "                color=\"red\",\n",
    "                ha=\"center\", va=\"top\",\n",
    "                bbox=dict(facecolor=\"grey\", alpha=0.25, edgecolor=\"red\")\n",
    "            )\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(idx, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # NUMERIC VARIABLES (including target if numeric)\n",
    "    print(\"🔢 NUMERIC VARIABLES\")\n",
    "\n",
    "    var_to_plot = numeric_att.copy()\n",
    "    if y_var_type == \"NUMERIC\" and y_var not in var_to_plot:\n",
    "        var_to_plot.insert(0, y_var)\n",
    "\n",
    "    num_rows = math.ceil(len(var_to_plot) / num_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=num_rows * 2,\n",
    "        ncols=num_cols,\n",
    "        figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows),\n",
    "        gridspec_kw={'height_ratios': [4, 0.5] * num_rows}\n",
    "    )\n",
    "\n",
    "    var_idx = 0\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "\n",
    "            if var_idx >= len(var_to_plot):\n",
    "                axes[row * 2, col].set_visible(False)\n",
    "                axes[row * 2 + 1, col].set_visible(False)\n",
    "                continue\n",
    "\n",
    "            colname = var_to_plot[var_idx]\n",
    "\n",
    "            # Histogram\n",
    "            sns.histplot(\n",
    "                ax=axes[row * 2, col],\n",
    "                data=df_S5,\n",
    "                x=colname,\n",
    "                bins=num_bins\n",
    "            )\n",
    "            axes[row * 2, col].set_xlabel(\"\")\n",
    "\n",
    "            # Boxplot\n",
    "            sns.boxplot(\n",
    "                ax=axes[row * 2 + 1, col],\n",
    "                data=df_S5,\n",
    "                x=colname\n",
    "            )\n",
    "\n",
    "            # Highlight target\n",
    "            if colname == y_var:\n",
    "                axes[row * 2, col].set_title(colname, **target_title_style)\n",
    "                axes[row * 2 + 1, col].set_title(colname, **target_title_style)\n",
    "\n",
    "                axes[row * 2, col].add_patch(\n",
    "                    plt.Rectangle((0, 0), 1, 1, transform=axes[row * 2, col].transAxes, **target_box_style)\n",
    "                )\n",
    "                axes[row * 2 + 1, col].add_patch(\n",
    "                    plt.Rectangle((0, 0), 1, 1, transform=axes[row * 2 + 1, col].transAxes, **target_box_style)\n",
    "                )\n",
    "            else:\n",
    "                axes[row * 2, col].set_title(colname)\n",
    "                axes[row * 2 + 1, col].set_title(colname)\n",
    "\n",
    "            var_idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b2719f",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- data is well divided between male and female\n",
    "- most of the data are non-smokers\n",
    "- data is well divided among the 4 different regions\n",
    "- bmi shows some outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094425ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 6 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS TARGET\n",
    "# -------------------------------\n",
    "print(\"STEP 6 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS TARGET\")\n",
    "\n",
    "if not make_plots:\n",
    "    print(\"MULTIVARIANT ANALYSIS - ATTRIBUTES VS TARGET is not printed, set make_plots = True\")\n",
    "else:\n",
    "    # Copy  of previous DataFrame\n",
    "    df_S6 = df_S4.copy()\n",
    "\n",
    "    print(\"\\n 🔢 NUMERIC Attributes VS 🔢 NUMERIC Target\")\n",
    "\n",
    "    if y_var_type == \"CATEGORIC\":\n",
    "        print(\"   This type of plot is non applicable for this case, because the target variable is CATEGORIC\")\n",
    "    else:\n",
    "        # Set plotting variables\n",
    "        var_to_plot=numeric_att # Variable type for this plot\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
    "\n",
    "        # Create subplots with custom height ratios\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows * 2,\n",
    "            ncols = num_cols,\n",
    "            figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows),\n",
    "            gridspec_kw={'height_ratios': [4, 1] * num_rows})\n",
    "\n",
    "        # Loop through variables\n",
    "        var_idx = 0\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                if var_idx >= len(var_to_plot):\n",
    "                    # Hide unused subplots\n",
    "                    axes[row * 2, col].set_visible(False)\n",
    "                    axes[row * 2 + 1, col].set_visible(False)\n",
    "                    continue\n",
    "\n",
    "                # Regplot (top)\n",
    "                sns.regplot(\n",
    "                    ax = axes[row * 2, col],\n",
    "                    data = df_S6,\n",
    "                    x = var_to_plot[var_idx],\n",
    "                    y = y_var,\n",
    "                    scatter_kws = {'s': my_font_size, 'alpha': 0.6},\n",
    "                    line_kws = {'color': 'red'})\n",
    "\n",
    "                # Boxplot (bottom)\n",
    "                sns.heatmap(\n",
    "                    ax = axes[row * 2 + 1, col],\n",
    "                    data = df_S6[[var_to_plot[var_idx], y_var]].corr(),\n",
    "                    annot = True,\n",
    "                    fmt = \".2f\",\n",
    "                    cbar = False)\n",
    "\n",
    "                var_idx += 1\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n 🏷️ CATEGORY Attributes VS 🔢 NUMERIC Target\")\n",
    "\n",
    "    if y_var_type == \"CATEGORIC\":\n",
    "        print(\"   This type of plot is non applicable for this case, because the target variable is CATEGORIC\")\n",
    "    else:        \n",
    "        # Set plotting variables\n",
    "        var_to_plot=category_att # Variable type for this plot\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
    "\n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows,\n",
    "            ncols = num_cols,\n",
    "            figsize = (figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
    "\n",
    "        # flatten the axes array for easier access\n",
    "        axes = axes.flatten()  \n",
    "\n",
    "        # Loop through variables\n",
    "        idx = 0\n",
    "        for col in var_to_plot:\n",
    "\n",
    "            # Count unique values\n",
    "            unique_count = df_S6[col].nunique()\n",
    "            \n",
    "            # Limit the number of plotted categories if there are more than num_values_to_plot\n",
    "            if unique_count > num_values_to_plot:\n",
    "                order = df_S6[col].value_counts().head(num_values_to_plot).index\n",
    "            else:\n",
    "                order = df_S6[col].value_counts().index\n",
    "            \n",
    "            # Create barplot\n",
    "            sns.barplot(\n",
    "                ax=axes[idx],\n",
    "                data = df_S6,\n",
    "                x = col,\n",
    "                y = y_var,\n",
    "                hue = category_combi_att,\n",
    "                order = order)\n",
    "            axes[idx].tick_params(axis='x', rotation=90, labelsize=10)\n",
    "            \n",
    "            # Add text box if truncated\n",
    "            if unique_count > num_values_to_plot:\n",
    "                msg = \"There are \" + str(unique_count) + \" different values,\\nbut only \" + str(num_values_to_plot) + \" have been plotted\"\n",
    "                axes[idx].text(\n",
    "                    x = 0.5,\n",
    "                    y = 0.9,\n",
    "                    s = msg,\n",
    "                    transform=axes[idx].transAxes,\n",
    "                    fontsize = my_font_size,\n",
    "                    color = 'red',\n",
    "                    ha = 'center',\n",
    "                    va = 'top',\n",
    "                    bbox = dict(facecolor='grey', alpha=0.5, edgecolor='red'))\n",
    "            \n",
    "            idx += 1\n",
    "\n",
    "        # Turn off unused axes if there are any\n",
    "        for j in range(idx, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\n 🏷️ CATEGORY Attributes with 🏷️ Combined CATEGORY Target\")\n",
    "\n",
    "    if y_var_type == \"NUMERIC\":\n",
    "        print(\"   This type of plot is non applicable for this case, because the target variable is NUMERIC\")\n",
    "    else:\n",
    "        # Set plotting variables\n",
    "        hue_order = sorted(df_S6[y_var].dropna().unique().tolist()) # Determine hue order dynamically\n",
    "        var_to_plot=category_att # Variable type for this plot\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
    "\n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows,\n",
    "            ncols = num_cols,\n",
    "            figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
    "            \n",
    "        # Flatten the axes array for easier access\n",
    "        axes = axes.flatten()  \n",
    "\n",
    "        # Loop through variables\n",
    "        idx = 0\n",
    "        for col in var_to_plot:\n",
    "            # Count unique values\n",
    "            unique_count = df_S6[col].nunique()\n",
    "            \n",
    "            # Limit the number of plotted categories if there are more than num_values_to_plot\n",
    "            if unique_count > num_values_to_plot:\n",
    "                order = df_S6[col].value_counts().head(num_values_to_plot).index\n",
    "            else:\n",
    "                order = df_S6[col].value_counts().index\n",
    "            \n",
    "            # Create countplot\n",
    "            sns.countplot(\n",
    "                ax = axes[idx],\n",
    "                data = df_S6,\n",
    "                x = col,\n",
    "                hue = y_var,\n",
    "                hue_order = hue_order,\n",
    "                palette = my_palette,\n",
    "                order = order)\n",
    "            axes[idx].tick_params(axis='x', rotation=90, labelsize=my_font_size)\n",
    "            \n",
    "            # Add text box if truncated\n",
    "            if unique_count > num_values_to_plot:\n",
    "                msg = \"There are \" + str(unique_count) + \" different values,\\nbut only \" + str(num_values_to_plot) + \" have been plotted\"\n",
    "                axes[idx].text(\n",
    "                    x = 0.5,\n",
    "                    y = 0.9,\n",
    "                    s = msg,\n",
    "                    transform=axes[idx].transAxes,\n",
    "                    fontsize=my_font_size,\n",
    "                    color='red',\n",
    "                    ha='center',\n",
    "                    va='top',\n",
    "                    bbox=dict(facecolor='grey', alpha=0.25, edgecolor='red'))\n",
    "            \n",
    "            idx += 1\n",
    "\n",
    "        # Turn off unused axes if there are any\n",
    "        for j in range(idx, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9275c",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- males have slighly more tendecy to smoke than females\n",
    "- northwest is the only region with more females than males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3540b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 7 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS ATTRIBUTES\n",
    "# -------------------------------\n",
    "print(\"STEP 7 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS ATTRIBUTES\")\n",
    "\n",
    "if not make_plots:\n",
    "    print(\"MULTIVARIANT ANALYSIS - ATTRIBUTES VS ATTRIBUTES is not printed, set make_plots = True\")\n",
    "else:\n",
    "    # Copy of previous DataFrame\n",
    "    df_S7 = df_S4.copy()\n",
    "\n",
    "    print(\"\\n 🔢 NUMERIC Attributes VS 🔢 NUMERIC Attributes\")\n",
    "\n",
    "    var_to_plot = numeric_att\n",
    "    num_rows = len(var_to_plot) - 1  # Number of rows (one less than number of variables)\n",
    "\n",
    "    # Create subplots with two stacked plots (regplot + heatmap) per variable pair\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows = num_rows * 2,\n",
    "        ncols = len(var_to_plot) - 1,\n",
    "        figsize=(figWidth_unit * (len(var_to_plot) - 1), figHeight_unit * num_rows),\n",
    "        gridspec_kw={'height_ratios': [4, 1] * num_rows})\n",
    "\n",
    "    # Flatten axes for easy handling\n",
    "    axes = np.array(axes)\n",
    "\n",
    "    # Track subplot usage\n",
    "    for row in range(num_rows):\n",
    "        n_cols = len(var_to_plot) - row - 1  # Decreasing number of columns each row\n",
    "        for col in range(n_cols):\n",
    "\n",
    "            # Top: regplot\n",
    "            sns.regplot(\n",
    "                ax = axes[row * 2, col],\n",
    "                data = df_S7,\n",
    "                x = var_to_plot[row + col + 1],\n",
    "                y = var_to_plot[row],\n",
    "                scatter_kws = {'s': my_font_size, 'alpha': 0.6},\n",
    "                line_kws = {'color': 'red'})\n",
    "            axes[row * 2, col].set_xlabel(var_to_plot[row + col + 1], fontsize=20)\n",
    "\n",
    "            # Show Y label only for first plot in row\n",
    "            if col == 0:\n",
    "                axes[row * 2, col].set_ylabel(var_to_plot[row], fontsize=my_font_size)\n",
    "            else:\n",
    "                axes[row * 2, col].set_ylabel(\"\")\n",
    "\n",
    "            # Bottom: heatmap (correlation)\n",
    "            sns.heatmap(\n",
    "                ax = axes[row * 2 + 1, col],\n",
    "                data = df_S7[[var_to_plot[row + col + 1], var_to_plot[row]]].corr(),\n",
    "                annot = True,\n",
    "                fmt = \".2f\",\n",
    "                cbar = False,\n",
    "                annot_kws = {\"size\": 20})\n",
    "\n",
    "        # Hide unused subplots on the right for this row\n",
    "        for col in range(n_cols, len(var_to_plot) - 1):\n",
    "            axes[row * 2, col].set_visible(False)\n",
    "            axes[row * 2 + 1, col].set_visible(False)\n",
    "\n",
    "    # Adjust layout and show\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n 🏷️🔢 ALL Attributes VS 🏷️🔢 ALL Attributes\")\n",
    "\n",
    "    # Encode categorical variables using the Series.factorize() method\n",
    "    for col in category_att:\n",
    "        codes, uniques = df_S7[col].factorize()\n",
    "        df_S7[col] = codes  # replace column with integer codes\n",
    "\n",
    "    # CATEGORIC ATTRIBUTES HEATMAP\n",
    "    if len(category_att) > 1:\n",
    "        corr_cat = df_S7[category_att].corr()\n",
    "        fig = plt.figure(figsize=(figWidth_unit, figHeight_unit))\n",
    "        plt.title(\"ONLY CATEGORIC ATTRIBUTES\", fontsize=my_font_size + 2, fontweight=\"bold\")\n",
    "        sns.heatmap(\n",
    "            data=corr_cat,\n",
    "            annot=True,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            fmt=\".2f\",\n",
    "            annot_kws={\"size\": my_font_size}\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ Not enough categorical attributes for a correlation matrix.\")\n",
    "\n",
    "    # NUMERIC ATTRIBUTES HEATMAP\n",
    "    if len(numeric_att) > 1:\n",
    "        corr_num = df_S7[numeric_att].corr()\n",
    "        fig = plt.figure(figsize=(figWidth_unit, figHeight_unit))\n",
    "        plt.title(\"ONLY NUMERIC ATTRIBUTES\", fontsize=my_font_size + 2, fontweight=\"bold\")\n",
    "        sns.heatmap(\n",
    "            data=corr_num,\n",
    "            annot=True,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            fmt=\".2f\",\n",
    "            annot_kws={\"size\": my_font_size}\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ Not enough numeric attributes for a correlation matrix.\")\n",
    "\n",
    "    # ALL VARIABLES HEATMAP\n",
    "    corr_matrix = df_S7[numeric_att + category_att].corr()\n",
    "    corr_order = corr_matrix.mean().sort_values(ascending=False).index\n",
    "    corr_matrix = corr_matrix.loc[corr_order, corr_order]\n",
    "\n",
    "    fig = plt.figure(figsize=(2 * figWidth_unit, 2 * figHeight_unit))\n",
    "    plt.title(\"CATEGORIC AND NUMERIC ATTRIBUTES\", fontsize=my_font_size + 2, fontweight=\"bold\")\n",
    "    sns.heatmap(\n",
    "        data=corr_matrix,\n",
    "        annot=True,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        fmt=\".2f\",\n",
    "        annot_kws={\"size\": my_font_size}\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # PAIRPLOT (sorted by correlation order)\n",
    "    fig = plt.figure(figsize=(figWidth_unit, figHeight_unit))\n",
    "    sns.pairplot(data=df_S7[corr_order])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd8dc0f",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- attributes do not keep a hight correlation between them -> no noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 8) REMOVE NOISY ATTRIBUTES\n",
    "# -------------------------------\n",
    "print(\"STEP 8) REMOVE NOISY ATTRIBUTES\")\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "df_S8 = df_S4.copy()\n",
    "\n",
    "# Correlation level considered as \"too high\"\n",
    "corr_threshold = 0.9  \n",
    "\n",
    "#  NUMERIC ATTRIBUTES (Pearson correlation)\n",
    "corr_matrix = df_S8[numeric_att].corr().abs()\n",
    "to_drop = set()\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if corr_matrix.iloc[i, j] > corr_threshold:\n",
    "            col_i = corr_matrix.columns[i]\n",
    "            col_j = corr_matrix.columns[j]\n",
    "            if col_i not in to_drop:\n",
    "                to_drop.add(col_i)\n",
    "\n",
    "if to_drop:\n",
    "    df_S8 = df_S8.drop(columns=list(to_drop), axis=1)\n",
    "    print(f\"- ⚠️ High NUMERIC attributes correlation detected (Pearson Corr. > {corr_threshold}). Dropped:\")\n",
    "    for col in to_drop:\n",
    "        print(f\"   • {col}\")\n",
    "else:\n",
    "    print(f\"- ✅ No NUMERIC attributes exceeded {corr_threshold} Pearson Correlation\")\n",
    "\n",
    "#  CATEGORICAL ATTRIBUTES (Cramér's V)\n",
    "def cramers_v(x, y): \n",
    "    # Step 1: confusion matrix\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    # Step 2: chi-square statistic\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    # Step 3: phi-squared\n",
    "    total_samples = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / total_samples\n",
    "    # Shape of confusion matrix\n",
    "    r, k = confusion_matrix.shape\n",
    "    num_rows = confusion_matrix.shape[0]\n",
    "    num_cols = confusion_matrix.shape[1]\n",
    "    # Step 4: bias correction (recommended formula)\n",
    "    correction = ((num_cols - 1) * (num_rows - 1)) / (total_samples - 1)\n",
    "    phi2_corrected = max(0, phi2 - correction)\n",
    "    # Corrected dimensions\n",
    "    rows_corrected = num_rows - ((num_rows - 1) ** 2) / (total_samples - 1)\n",
    "    cols_corrected = num_cols - ((num_cols - 1) ** 2) / (total_samples - 1)\n",
    "    # Step 5: compute Cramér's V\n",
    "    denominator = min(rows_corrected - 1, cols_corrected - 1)\n",
    "    if denominator <= 0:\n",
    "        return 0  # avoid division by zero for degenerate tables\n",
    "    cramers_v_value = np.sqrt(phi2_corrected / denominator)\n",
    "    return cramers_v_value\n",
    "\n",
    "to_drop_cat = set()\n",
    "\n",
    "if len(category_att) > 1:\n",
    "    for i in range(len(category_att)):\n",
    "        for j in range(i):\n",
    "            v = cramers_v(df_S8[category_att[i]], df_S8[category_att[j]])\n",
    "            if v > corr_threshold:\n",
    "                col_i = category_att[i]\n",
    "                col_j = category_att[j]\n",
    "                if col_i not in to_drop_cat:\n",
    "                    to_drop_cat.add(col_i)\n",
    "\n",
    "if to_drop_cat:\n",
    "    df_S8 = df_S8.drop(columns=list(to_drop_cat), axis=1)\n",
    "    print(f\"- ⚠️ High CATEGORICAL attributes association detected (Cramer’s V > {corr_threshold}). Dropped:\")\n",
    "    for col in to_drop_cat:\n",
    "        print(f\"   • {col}\")\n",
    "else:\n",
    "    print(f\"- ✅ No CATEGORICAL attributes exceeded {corr_threshold} Cramer’s V\")\n",
    "\n",
    "#  Print results\n",
    "print(f\"- ℹ️ Previous df's columns: {len(df_S4.columns)}\")\n",
    "print(f\"- ℹ️ Cleaned df's  columns: {len(df_S8.columns)}\")\n",
    "print(f\"- ℹ️ Final DataFrame shape: {df_S8.shape}\")\n",
    "\n",
    "display(df_S8.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec9d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 9) CLEAN OUTLIERS\n",
    "# -------------------------------\n",
    "print(\"STEP 9) CLEAN OUTLIERS\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S9=df_S8.copy()\n",
    "\n",
    "low_outliers_threshold = 1.0   # [%] Max percentage of lower outliers allowed to remove\n",
    "up_outliers_threshold = 1.0  # [% ]Max percentage of upper outliers allowed to remove\n",
    "removal_type = \"NORMAL OUTLIERS\" # Removal logic type (NORMAL or EXTREME outliers)\n",
    "\n",
    "# Print info\n",
    "display(df_S9.describe())\n",
    "\n",
    "# Outliers detection is applied over numeric attributes\n",
    "cols = []\n",
    "for col in numeric_att:\n",
    "    if col in df_S9.columns.to_list():\n",
    "        cols.append(col)\n",
    "\n",
    "# Initialize containers\n",
    "lower_limits = []\n",
    "upper_limits = []\n",
    "n_outliers_lower = []\n",
    "n_outliers_upper = []\n",
    "pct_outliers_lower = []\n",
    "pct_outliers_upper = []\n",
    "extreme_lower_limits = []\n",
    "extreme_upper_limits = []\n",
    "n_extreme_outliers_lower = []\n",
    "n_extreme_outliers_upper = []\n",
    "pct_extreme_outliers_lower = []\n",
    "pct_extreme_outliers_upper = []\n",
    "\n",
    "len(df_S9.index)\n",
    "\n",
    "for col in cols:\n",
    "    Q1 = df_S9[col].quantile(0.25)\n",
    "    Q3 = df_S9[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Calulate limits\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    extreme_lower = Q1 - 3 * IQR\n",
    "    extreme_upper = Q3 + 3 * IQR\n",
    "\n",
    "    # Count num of outliers\n",
    "    n_low = (df_S9[col] < lower).sum()\n",
    "    n_high = (df_S9[col] > upper).sum()\n",
    "    n_extreme_low = (df_S9[col] < extreme_lower).sum()\n",
    "    n_extreme_high = (df_S9[col] > extreme_upper).sum()\n",
    "\n",
    "    # Percentages of outliers\n",
    "    pct_low = (n_low / len(df_S9.index)) * 100\n",
    "    pct_high = (n_high / len(df_S9.index)) * 100\n",
    "    pct_extreme_low = (n_extreme_low / len(df_S9.index)) * 100\n",
    "    pct_extreme_high = (n_extreme_high / len(df_S9.index)) * 100\n",
    "\n",
    "    # Save limits\n",
    "    lower_limits.append(lower)\n",
    "    upper_limits.append(upper)\n",
    "    extreme_lower_limits.append(extreme_lower)\n",
    "    extreme_upper_limits.append(extreme_upper)\n",
    "\n",
    "    # Save num of outliers\n",
    "    n_outliers_lower.append(n_low)\n",
    "    n_outliers_upper.append(n_high)\n",
    "    n_extreme_outliers_lower.append(n_extreme_low)\n",
    "    n_extreme_outliers_upper.append(n_extreme_high)\n",
    "\n",
    "    # Save percentages of outliers\n",
    "    pct_outliers_lower.append(pct_low)\n",
    "    pct_outliers_upper.append(pct_high)\n",
    "    pct_extreme_outliers_lower.append(pct_extreme_low)\n",
    "    pct_extreme_outliers_upper.append(pct_extreme_high)\n",
    "\n",
    "\n",
    "# Build DataFrame with all results\n",
    "df_limits = pd.DataFrame(\n",
    "    [\n",
    "        lower_limits,\n",
    "        upper_limits,\n",
    "        n_outliers_lower,\n",
    "        n_outliers_upper,\n",
    "        pct_outliers_lower,\n",
    "        pct_outliers_upper\n",
    "    ],\n",
    "    columns=cols,\n",
    "    index=[\n",
    "        \"lower_limit\",\n",
    "        \"upper_limit\",\n",
    "        \"n_outliers_lower\",\n",
    "        \"n_outliers_upper\",\n",
    "        \"pct_outliers_lower\",\n",
    "        \"pct_outliers_upper\"\n",
    "    ]\n",
    ")\n",
    "df_extreme_limits = pd.DataFrame(\n",
    "    [\n",
    "        extreme_lower_limits,\n",
    "        extreme_upper_limits,\n",
    "        n_extreme_outliers_lower,\n",
    "        n_extreme_outliers_upper,\n",
    "        pct_extreme_outliers_lower,\n",
    "        pct_extreme_outliers_upper\n",
    "    ],\n",
    "    columns=cols,\n",
    "    index=[\n",
    "        \"extreme_lower_limit\",\n",
    "        \"extreme_upper_limit\",\n",
    "        \"n_extreme_outliers_lower\",\n",
    "        \"n_extreme_outliers_upper\",\n",
    "        \"pct_extreme_outliers_lower\",\n",
    "        \"pct_extreme_outliers_upper\"\n",
    "    ]\n",
    ")\n",
    "# Display results\n",
    "display(df_limits)\n",
    "display(df_extreme_limits)\n",
    "\n",
    "# Outliers detection is applied over numeric attributes\n",
    "cols = []\n",
    "for col in numeric_att:\n",
    "    if col in df_S9.columns.to_list():\n",
    "        cols.append(col)\n",
    "\n",
    "if removal_type == \"NORMAL OUTLIERS\":\n",
    "    # Loop through each column and apply filtering rules\n",
    "    for col in cols:\n",
    "        low_limit = df_limits.loc[\"lower_limit\", col]\n",
    "        high_limit = df_limits.loc[\"upper_limit\", col]\n",
    "        pct_low = df_limits.loc[\"pct_outliers_lower\", col]\n",
    "        pct_high = df_limits.loc[\"pct_outliers_upper\", col]\n",
    "        \n",
    "        # Remove low outliers if below threshold\n",
    "        print(f\"- ATTRIBUTE {col}:\")\n",
    "        if pct_low == 0:\n",
    "            print(f\" - ℹ️ None lower outliers detected\")\n",
    "        elif pct_low <= low_outliers_threshold:\n",
    "            df_S9 = df_S9[df_S9[col] >= low_limit]\n",
    "            print(f\" - ⚠️ REMOVED lower outliers ({pct_low:.2f}% <= {low_outliers_threshold}%)\")\n",
    "        else:\n",
    "            print(f\" - ✅ KEPT lower outliers ({pct_low:.2f}% > {low_outliers_threshold}%)\")\n",
    "        \n",
    "        # Remove high outliers if below threshold\n",
    "        if pct_high == 0:\n",
    "            print(f\" - ℹ️ None upper outliers detected\")\n",
    "        elif pct_high <= up_outliers_threshold:\n",
    "            df_S9 = df_S9[df_S9[col] <= high_limit]\n",
    "            print(f\" - ⚠️ REMOVED upper outliers ({pct_high:.2f}% <= {up_outliers_threshold}%)\")\n",
    "        else:\n",
    "            print(f\" - ✅ KEPT upper outliers ({pct_high:.2f}% > {up_outliers_threshold}%)\")\n",
    "        print(\"\\n\")\n",
    "elif removal_type == \"EXTREME OUTLIERS\":\n",
    "    # Loop through each column and apply filtering rules\n",
    "    for col in cols:\n",
    "        low_limit = df_extreme_limits.loc[\"extreme_lower_limit\", col]\n",
    "        high_limit = df_extreme_limits.loc[\"extreme_upper_limit\", col]\n",
    "        pct_low = df_extreme_limits.loc[\"pct_extreme_outliers_lower\", col]\n",
    "        pct_high = df_extreme_limits.loc[\"pct_extreme_outliers_upper\", col]\n",
    "        \n",
    "        # Remove low outliers if below threshold\n",
    "        print(f\"- ATTRIBUTE {col}:\")\n",
    "        if pct_low == 0:\n",
    "            print(f\" - ℹ️ None extreme lower outliers detected\")\n",
    "        elif pct_low <= low_outliers_threshold:\n",
    "            df_S9 = df_S9[df_S9[col] >= low_limit]\n",
    "            print(f\" - ⚠️ REMOVED extreme lower outliers ({pct_low:.2f}% <= {low_outliers_threshold}%)\")\n",
    "        else:\n",
    "            print(f\" - ✅ KEPT extreme lower outliers ({pct_low:.2f}% > {low_outliers_threshold}%)\")\n",
    "        \n",
    "        # Remove high outliers if below threshold\n",
    "        if pct_high == 0:\n",
    "            print(f\" - ℹ️ None extreme upper outliers detected\")\n",
    "        elif pct_high <= up_outliers_threshold:\n",
    "            df_S9 = df_S9[df_S9[col] <= high_limit]\n",
    "            print(f\" - ⚠️ REMOVED extreme upper outliers ({pct_high:.2f}% <= {up_outliers_threshold}%)\")\n",
    "        else:\n",
    "            print(f\" - ✅ KEPT extreme upper outliers ({pct_high:.2f}% > {up_outliers_threshold}%)\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(\"- ✅Outliers cleaning completed.\")\n",
    "print(f\" - ℹ️ Original df's rows: {len(df_S8)}\")\n",
    "print(f\" - ℹ️ Cleaned df's  rows: {len(df_S9)}\")\n",
    "display(df_S9.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5119c",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- I goint to stablish a \"cut\" based on normal upper outliers, it is enough\n",
    "- The \"cut\" would be for a maximum of 1 %, higher that than that, normal outliers will not be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48119caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 10) MISSING VALUES\n",
    "# -------------------------------\n",
    "print(\"STEP 10) MISSING VALUES\")\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "df_S10 = df_S9.copy()\n",
    "\n",
    "# [%] If missing perc > filling_threshold → fill values, otherwise drop rows\n",
    "filling_threshold = 5.0   \n",
    "\n",
    "# TARGET VARIABLE\n",
    "missing_y = df_S10[y_var].isnull().sum()\n",
    "\n",
    "if missing_y > 0:\n",
    "    print(f\"- ⚠️ Target variable '{y_var}' contains {missing_y} missing values → rows will be dropped.\")\n",
    "    df_S10 = df_S10.dropna(subset=[y_var])\n",
    "else:\n",
    "    print(f\"- ✅ Target variable '{y_var}' has no missing values.\")\n",
    "\n",
    "# Identify categorical variables suitable for grouping (<6 unique values)\n",
    "group_vars = []\n",
    "for col in category_att:\n",
    "    if df_S10[col].nunique() < 6:\n",
    "        group_vars.append(col)\n",
    "\n",
    "# Calculate missing percentages per column\n",
    "missing_pct = (df_S10.isnull().sum() / len(df_S10)) * 100\n",
    "missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_pct) == 0:\n",
    "    print(\"- ✅ DataFrame has no missing values at all (excluding target variable already handled)\")\n",
    "else:\n",
    "    # Process each column with missing values\n",
    "    for col in missing_pct.index:\n",
    "        pct = missing_pct[col]\n",
    "        print(f\"- ⚠️ Column: {col} → {pct:.2f}% missing\")\n",
    "\n",
    "        # CASE 1: NUMERIC ATTRIBUTE → grouped median\n",
    "        if col in numeric_att:\n",
    "\n",
    "            if pct > filling_threshold and len(group_vars) > 0:\n",
    "                print(f\" - ⚠️ FILLED missing numeric values in {col} using grouped median by {group_vars}...\\n\")\n",
    "\n",
    "                medians = df_S10.groupby(group_vars)[col].median().reset_index()\n",
    "                medians = medians.rename(columns={col: f\"median_{col}\"})\n",
    "\n",
    "                df_S10 = pd.merge(df_S10, medians, on=group_vars, how=\"left\")\n",
    "                df_S10[col] = df_S10[col].fillna(df_S10[f\"median_{col}\"])\n",
    "                df_S10 = df_S10.drop(columns=[f\"median_{col}\"])\n",
    "\n",
    "            elif pct <= filling_threshold:\n",
    "                print(f\" - ⚠️ DROPPED rows with missing values in {col} ({pct:.2f}% ≤ {filling_threshold}%)...\\n\")\n",
    "                df_S10 = df_S10.dropna(subset=[col])\n",
    "\n",
    "            else:\n",
    "                print(f\" - ℹ️ No suitable grouping columns found — skipping fill for {col}.\\n\")\n",
    "\n",
    "        # CASE 2: CATEGORICAL ATTRIBUTE → mode imputation\n",
    "        elif col in category_att:\n",
    "\n",
    "            print(f\" - ⚠️ FILLED missing categorical values in {col} using mode (most frequent value)...\\n\")\n",
    "            mode_value = df_S10[col].mode().iloc[0]\n",
    "            df_S10[col] = df_S10[col].fillna(mode_value)\n",
    "\n",
    "        # CASE 3: Other types (edge cases)\n",
    "        else:\n",
    "            print(f\" - ℹ️ Column {col} has unsupported type for imputation — dropping rows.\\n\")\n",
    "            df_S10 = df_S10.dropna(subset=[col])\n",
    "\n",
    "\n",
    "# Summary\n",
    "print(f\"- ℹ️ Remaining missing values per column:\\n{df_S10.isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8495fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 11) SPLIT\n",
    "# -------------------------------\n",
    "print(\"STEP 11) SPLIT\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_SPLIT = df_S10.copy()\n",
    "\n",
    "my_test_size = 0.2\n",
    "random_seed = 42\n",
    "\n",
    "X = df_SPLIT.drop(\n",
    "    labels = y_var,\n",
    "    axis = 1\n",
    ")\n",
    "y = df_SPLIT[y_var]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = my_test_size, random_state = random_seed)\n",
    "\n",
    "print(\"- ℹ️ Shape of DataFrames after SPLIT:\")\n",
    "print(\" - X_train:\",X_train.shape)\n",
    "print(\" - X_test:\",X_test.shape)\n",
    "print(\" - y_train:\",y_train.shape)\n",
    "print(\" - y_test:\",y_test.shape)\n",
    "\n",
    "print(\"\\n- ℹ️ Content of DataFrames after SPLIT:\")\n",
    "print(\" - X_train:\\n\",X_train.head(5))\n",
    "print(\" - X_test:\\n\",X_test.head(5))\n",
    "print(\" - y_train:\\n\",y_train.head(5))\n",
    "print(\" - y_test:\\n\",y_test.head(5))\n",
    "\n",
    "print(\"\\n- ℹ️ Info of DataFrames after SPLIT:\")\n",
    "print(\" - X_train:\",X_train.info())\n",
    "print(\" - X_test:\",X_test.info())\n",
    "print(\" - y_train:\",y_train.info())\n",
    "print(\" - y_test:\",y_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 12) SCALLING\n",
    "# -------------------------------\n",
    "print(\"STEP 12) SCALLING\")\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "X_train_es = X_train.copy()\n",
    "X_test_es = X_test.copy()\n",
    "\n",
    "# List of columns\n",
    "columns = X_train_es.columns.tolist()\n",
    "\n",
    "# Instance scaler for each catenumeric attribute\n",
    "scaler_dic = {\n",
    "    \"age\": StandardScaler(),\n",
    "    \"bmi\": StandardScaler(),\n",
    "    \"children\": StandardScaler()\n",
    "}\n",
    "print(\"- ✅ All Scalers have been instanced successfully\")\n",
    "\n",
    "# Train scaler with ONLY train data\n",
    "for i in range(len(columns)):\n",
    "    if columns[i] in numeric_att:\n",
    "        scaler_dic[columns[i]].fit(X_train_es[[columns[i]]])\n",
    "print(\"- ✅ All Scalers have been trained successfully\")\n",
    "\n",
    "# Apply scaler to BOTH train + test data\n",
    "for i in range(len(columns)):\n",
    "    if columns[i] in numeric_att:\n",
    "        # Transform both sets\n",
    "        X_train_es_aux = scaler_dic[columns[i]].transform(X_train_es[[columns[i]]])\n",
    "        X_test_es_aux = scaler_dic[columns[i]].transform(X_test_es[[columns[i]]])\n",
    "        # Determine column_name based on scaler type\n",
    "        new_col_name = columns[i] +\"_SS\"\n",
    "        # Convert to DataFrame\n",
    "        X_train_es_aux = pd.DataFrame(X_train_es_aux, index=X_train_es.index, columns=[new_col_name])\n",
    "        X_test_es_aux = pd.DataFrame(X_test_es_aux, index=X_test_es.index, columns=[new_col_name])\n",
    "         # Concatenate back\n",
    "        X_train_es = pd.concat([X_train_es, X_train_es_aux], axis=1)\n",
    "        X_test_es = pd.concat([X_test_es, X_test_es_aux], axis=1)\n",
    "        print(\"- ✅ Train and Test data have been scaled for: \" + columns[i])\n",
    "\n",
    "# Keep only scaled columms\n",
    "scaled_cols = []\n",
    "for i in range(len(columns)):\n",
    "    if columns[i] in numeric_att:\n",
    "        scaled_cols.append(columns[i] + \"_SS\")\n",
    "X_train_es = X_train_es[scaled_cols]\n",
    "X_test_es = X_test_es[scaled_cols]\n",
    "\n",
    "display(X_train_es.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866305f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 13) ENCODING\n",
    "# -------------------------------\n",
    "print(\"STEP 13) ENCODING\")\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "X_train_en = X_train.copy()\n",
    "X_test_en = X_test.copy()\n",
    "\n",
    "# List of columns\n",
    "columns = X_train_en.columns.tolist()\n",
    "\n",
    "# Dictionary of encoders for each categorical attribute\n",
    "encoder_dic = {}\n",
    "\n",
    "# Create encoder instance for each categorical attribute\n",
    "for col in category_att:\n",
    "\n",
    "    if col in binary_att:\n",
    "        encoder_dic[col] = LabelEncoder()\n",
    "        print(f\"- Encoder instanced for {col}: LabelEncoder()\")\n",
    "\n",
    "    elif col in multiclass_att:\n",
    "        encoder_dic[col] = OneHotEncoder(sparse_output=False)\n",
    "        print(f\"- Encoder instanced for {col}: OneHotEncoder()\")\n",
    "\n",
    "print(\"- ✅ All Encoders have been instanced successfully\")\n",
    "\n",
    "# Train encoders with TRAIN data only\n",
    "for col in category_att:\n",
    "\n",
    "    encoder = encoder_dic[col]\n",
    "\n",
    "    if isinstance(encoder, LabelEncoder):\n",
    "        encoder.fit(X_train_en[col])        # LabelEncoder needs 1D\n",
    "\n",
    "    elif isinstance(encoder, OneHotEncoder):\n",
    "        encoder.fit(X_train_en[[col]])      # OHE needs 2D\n",
    "\n",
    "print(\"- ✅ All Encoders have been trained successfully\")\n",
    "\n",
    "# Apply encoders to TRAIN + TEST\n",
    "for col in category_att:\n",
    "\n",
    "    encoder = encoder_dic[col]\n",
    "\n",
    "    if isinstance(encoder, LabelEncoder):\n",
    "\n",
    "        X_train_en[col + \"_LE\"] = encoder.transform(X_train_en[col])\n",
    "        X_test_en[col + \"_LE\"] = encoder.transform(X_test_en[col])\n",
    "\n",
    "        print(f\"- ✅ {col} encoded with LabelEncoder()\")\n",
    "\n",
    "    elif isinstance(encoder, OneHotEncoder):\n",
    "\n",
    "        # Transform train and test\n",
    "        train_encoded = encoder.transform(X_train_en[[col]])\n",
    "        test_encoded = encoder.transform(X_test_en[[col]])\n",
    "\n",
    "        # New names\n",
    "        ohe_colnames = encoder.get_feature_names_out([col])\n",
    "        ohe_colnames = [name + \"_OHE\" for name in ohe_colnames]\n",
    "\n",
    "        # Convert to DataFrames\n",
    "        train_ohe_df = pd.DataFrame(train_encoded, index=X_train_en.index, columns=ohe_colnames)\n",
    "        test_ohe_df = pd.DataFrame(test_encoded, index=X_test_en.index, columns=ohe_colnames)\n",
    "\n",
    "        # Concatenate new cols\n",
    "        X_train_en = pd.concat([X_train_en, train_ohe_df], axis=1)\n",
    "        X_test_en = pd.concat([X_test_en, test_ohe_df], axis=1)\n",
    "\n",
    "        print(f\"- ✅ {col} encoded with OneHotEncoder()\")\n",
    "\n",
    "# Keep only encoded columns\n",
    "encoded_cols = []\n",
    "for col in category_att:\n",
    "    encoder = encoder_dic[col]\n",
    "    if isinstance(encoder, LabelEncoder):\n",
    "        encoded_cols.append(col + \"_LE\")\n",
    "    elif isinstance(encoder, OneHotEncoder):\n",
    "        ohe_colnames = encoder.get_feature_names_out([col])\n",
    "        for name in ohe_colnames:\n",
    "            encoded_cols.append(name + \"_OHE\")\n",
    "\n",
    "X_train_en = X_train_en[encoded_cols]\n",
    "X_test_en = X_test_en[encoded_cols]\n",
    "\n",
    "print(\"- ✅ Final encoded datasets created successfully\")\n",
    "\n",
    "X_train_en.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 14) FEATURE SELECTION\n",
    "# -------------------------------\n",
    "print(\"STEP 14) FEATURE SELECTION\")\n",
    "\n",
    "# Concatenate NUMERIC_var_scaled with CATEGORY_var_encoded\n",
    "X_train_assembled = pd.concat([X_train_es, X_train_en], axis=1)\n",
    "X_test_assembled = pd.concat([X_test_es, X_test_en], axis=1)\n",
    "\n",
    "feature_keeping_threshold = 100 # [%] Percentaje of features to keep (SelectKBest) \n",
    "\n",
    "# Instance selector\n",
    "num_features_to_keep = round(feature_keeping_threshold/100 * len(X_train_assembled.columns))\n",
    "selection_model = SelectKBest(score_func = f_classif, k = num_features_to_keep)\n",
    "print(\"- ✅ Selector have been instanced successfully to keep \" + str(num_features_to_keep) + \" features\")\n",
    "\n",
    "# Train selector with ONLY train data (y_train must be included because this is SUPERVISED selector)\n",
    "selection_model.fit(X_train_assembled, y_train)\n",
    "print(\"- ✅ Selector have been trained successfully\")\n",
    "\n",
    "# Drop non-selected features\n",
    "keeping_mask = selection_model.get_support()\n",
    "X_train_assembled = pd.DataFrame(selection_model.transform(X_train_assembled), columns = X_train_assembled.columns.values[keeping_mask])\n",
    "X_test_assembled = pd.DataFrame(selection_model.transform(X_test_assembled), columns = X_test_assembled.columns.values[keeping_mask])\n",
    "\n",
    "# Print results\n",
    "print(\"\\n 🧮 X_train_assembled\", X_train_assembled.shape)\n",
    "display(X_train_assembled.head())\n",
    "print(\"\\n 🧮 X_test_assembled\", X_test_assembled.shape)\n",
    "display(X_test_assembled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ae6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 15) SAVE PROCESSED DATA\n",
    "# -------------------------------\n",
    "print(\"STEP 15) SAVE PROCESSED DATA\")\n",
    "\n",
    "# Set from previous DataFrame\n",
    "X_train_final = X_train_assembled\n",
    "X_test_final = X_test_assembled\n",
    "y_train_final = y_train\n",
    "y_test_final = y_test\n",
    "output_path = \"../data/processed/\"\n",
    "\n",
    "# Get revision number - Returns the next free integer revision based on existing files\n",
    "import os\n",
    "def get_revision_number(base_path, base_name):\n",
    "    rev = 0\n",
    "    while True:\n",
    "        full_path = os.path.join(base_path, base_name + \"_\" + str(rev) + \".csv\")\n",
    "        if not os.path.exists(full_path):\n",
    "            return rev\n",
    "        rev += 1\n",
    "\n",
    "# Build filenames WITH revision number\n",
    "rev_number = get_revision_number(output_path, \"X_train_final\")\n",
    "suffix = \"_\" + str(rev_number)\n",
    "\n",
    "output_path_X_train = output_path + \"X_train_final\" + suffix + \".csv\"\n",
    "output_path_X_test  = output_path + \"X_test_final\"  + suffix + \".csv\"\n",
    "output_path_y_train = output_path + \"y_train_final\" + suffix + \".csv\"\n",
    "output_path_y_test  = output_path + \"y_test_final\"  + suffix + \".csv\"\n",
    "\n",
    "# Save all datasets\n",
    "X_train_final.to_csv(output_path_X_train, index=False)\n",
    "X_test_final.to_csv(output_path_X_test, index=False)\n",
    "y_train_final.to_csv(output_path_y_train, index=False)\n",
    "y_test_final.to_csv(output_path_y_test, index=False)\n",
    "\n",
    "print(\"- ✅ Files saved with revision number:\", rev_number)\n",
    "print(\"- 💡 Reminder: data/processed folder is ignored in .gitignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 16) PREDICTION MODEL\n",
    "# -------------------------------\n",
    "print(\"STEP 16) PREDICTION MODEL\")\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "X_train_model = X_train_final.copy()\n",
    "X_test_model = X_test_final.copy()\n",
    "y_train_model = y_train_final.copy()\n",
    "y_test_model = y_test_final.copy()\n",
    "\n",
    "# Model selection\n",
    "classification_models_selection = {\n",
    "    \"LogisticRegression\": False,\n",
    "    \"RandomForestClassifier\": False,\n",
    "    \"DecisionTreeClassifier\": False\n",
    "}\n",
    "regression_models_selection = {\n",
    "    \"LinearRegression\": True,\n",
    "    \"DecisionTreeRegressor\": False,\n",
    "    \"RandomForestRegressor\": False\n",
    "}\n",
    "\n",
    "# Available classification models\n",
    "classification_available = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=random_seed),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(random_state=random_seed),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=random_seed)\n",
    "}\n",
    "classification_grids = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [None, 5, 10, 20]\n",
    "    },\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        \"max_depth\": [None, 5, 10, 20],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Available regression models\n",
    "regression_available = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Lasso\": Lasso(random_state=random_seed),\n",
    "    \"Ridge\": Ridge(random_state=random_seed),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=random_seed),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=random_seed)\n",
    "}\n",
    "\n",
    "regression_grids = {\n",
    "    \"LinearRegression\": {},  # no optimization\n",
    "\n",
    "    \"Lasso\": {\n",
    "        \"alpha\": [0.001, 0.01, 0.1, 1, 10],\n",
    "        \"max_iter\": [1000, 5000, 10000]\n",
    "    },\n",
    "\n",
    "    \"Ridge\": {\n",
    "        \"alpha\": [0.001, 0.01, 0.1, 1, 10],\n",
    "        \"max_iter\": [1000, 5000, 10000]\n",
    "    },\n",
    "\n",
    "    \"DecisionTreeRegressor\": {\n",
    "        \"max_depth\": [None, 5, 10, 20],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "\n",
    "    \"RandomForestRegressor\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [None, 5, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Auxiliary functions\n",
    "def compute_classification_metrics(y_true, y_pred, avg, pos_label):\n",
    "    metrics = {}\n",
    "    metrics[\"Accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"Precision\"] = precision_score(y_true, y_pred, average=avg, pos_label=pos_label)\n",
    "    metrics[\"Recall\"] = recall_score(y_true, y_pred, average=avg, pos_label=pos_label)\n",
    "    metrics[\"F1_score\"] = f1_score(y_true, y_pred, average=avg, pos_label=pos_label)\n",
    "    return metrics\n",
    "\n",
    "def compute_regression_metrics(y_true, y_pred):\n",
    "    metrics = {}\n",
    "    metrics[\"MAE (Mean Absolute Error)\"] = mean_absolute_error(y_true, y_pred)\n",
    "    metrics[\"MSE (Mean Squared Error)\"] = mean_squared_error(y_true, y_pred)\n",
    "    metrics[\"RMSE (Root Mean Squared Error)\"] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    metrics[\"R2 (Coefficient of Determination)\"] = r2_score(y_true, y_pred)\n",
    "    return metrics\n",
    "\n",
    "def set_average_proposal(y):\n",
    "    unique_count = y.nunique()\n",
    "    if unique_count == 2:\n",
    "        freq = y.value_counts()\n",
    "        pos_label = freq.index[-1]\n",
    "        return \"binary\", pos_label, \"f1\"\n",
    "    freq_norm = y.value_counts(normalize=True)\n",
    "    imbalance_ratio = freq_norm.max() / freq_norm.min()\n",
    "\n",
    "    if imbalance_ratio <= 1.2:\n",
    "        return \"micro\", None, \"f1_micro\"\n",
    "    if imbalance_ratio <= 1.5:\n",
    "        return \"macro\", None, \"f1_macro\"\n",
    "    return \"weighted\", None, \"f1_weighted\"\n",
    "\n",
    "# Determine problem type\n",
    "if y_var_type == \"CATEGORIC\":\n",
    "    problem_type = \"classification\"\n",
    "    selected_models = classification_models_selection\n",
    "    available_models = classification_available\n",
    "    model_grids = classification_grids\n",
    "    icon = \"⚖️\"\n",
    "    proposed_avg, proposed_pos, proposed_score = set_average_proposal(y_train_model)\n",
    "\n",
    "else:\n",
    "    problem_type = \"regression\"\n",
    "    selected_models = regression_models_selection\n",
    "    available_models = regression_available\n",
    "    model_grids = regression_grids\n",
    "    icon = \"📈\"\n",
    "\n",
    "# Gather selected models\n",
    "selected_models_list = []\n",
    "for model_name, active in selected_models.items():\n",
    "    if active:\n",
    "        selected_models_list.append(model_name)\n",
    "\n",
    "# Auto-add Lasso and Ridge if LinearRegression is selected\n",
    "if problem_type == \"regression\":\n",
    "    if \"LinearRegression\" in selected_models_list:\n",
    "        selected_models_list.append(\"Lasso\")\n",
    "        selected_models_list.append(\"Ridge\")\n",
    "\n",
    "print(f\"\\n - Selected {problem_type} models:\", selected_models_list)\n",
    "\n",
    "# Loop through selected models\n",
    "default_results = {}\n",
    "optimized_results = {}\n",
    "\n",
    "for model_name in selected_models_list:\n",
    "\n",
    "    print(f\"\\n==================== {icon} {model_name} ====================\")\n",
    "\n",
    "    #-----------------------\n",
    "    # DEFAULT MODEL\n",
    "    #-----------------------\n",
    "    # Instance DEFAULT model\n",
    "    default_model = available_models[model_name]\n",
    "\n",
    "    # Train DEFAULT model\n",
    "    default_model.fit(X_train_model, y_train_model)\n",
    "\n",
    "    # Predict DEFAULT model\n",
    "    y_pred_train = default_model.predict(X_train_model)\n",
    "    y_pred_test = default_model.predict(X_test_model)\n",
    "\n",
    "    # Compute metrics\n",
    "    if problem_type == \"classification\":\n",
    "        default_metrics_train = compute_classification_metrics(y_train_model, y_pred_train, proposed_avg, proposed_pos)\n",
    "        default_metrics_test = compute_classification_metrics(y_test_model, y_pred_test, proposed_avg, proposed_pos)\n",
    "    else:\n",
    "        default_metrics_train = compute_regression_metrics(y_train_model, y_pred_train)\n",
    "        default_metrics_test = compute_regression_metrics(y_test_model, y_pred_test)\n",
    "\n",
    "    # Print DEFAULT MODEL results\n",
    "    print(\"\\n DEFAULT MODEL\")\n",
    "    print(\" 🏋️ TRAIN:\")\n",
    "    for metric_name, metric_value in default_metrics_train.items():\n",
    "        print(f\"    - {metric_name}: {metric_value}\")\n",
    "    print(\" 🧪 TEST:\")\n",
    "    for metric_name, metric_value in default_metrics_test.items():\n",
    "        print(f\"    - {metric_name}: {metric_value}\")\n",
    "\n",
    "    # Tag for regularized versions\n",
    "    name_tag = model_name\n",
    "    if model_name in [\"Lasso\", \"Ridge\"]:\n",
    "        name_tag = model_name + \" (Regularized)\"\n",
    "\n",
    "    # Store default results\n",
    "    default_results[name_tag + \" - 🏋️ TRAIN\"] = default_metrics_train\n",
    "    default_results[name_tag + \" - 🧪 TEST\"] = default_metrics_test\n",
    "\n",
    "    #-----------------------\n",
    "    # GRIDSEARCH FOR OPTIMIZATION\n",
    "    #-----------------------\n",
    "    grid_params = model_grids[model_name]\n",
    "\n",
    "    if len(grid_params) > 0:\n",
    "\n",
    "        if problem_type == \"classification\":\n",
    "            scoring_metric = proposed_score\n",
    "        else:\n",
    "            scoring_metric = \"r2\"\n",
    "\n",
    "        # Instance GRIDSEARCH\n",
    "        grid = GridSearchCV(\n",
    "            estimator=available_models[model_name],\n",
    "            param_grid=grid_params,\n",
    "            scoring=scoring_metric,\n",
    "            cv=5\n",
    "        )\n",
    "\n",
    "        # Train GRIDSEARCH\n",
    "        grid.fit(X_train_model, y_train_model)\n",
    "\n",
    "        # Get best estimator (clean and simple)\n",
    "        optimized_model = grid.best_estimator_\n",
    "        best_params = grid.best_params_\n",
    "\n",
    "        # Predict with optimized model\n",
    "        y_opt_train = optimized_model.predict(X_train_model)\n",
    "        y_opt_test = optimized_model.predict(X_test_model)\n",
    "\n",
    "        # Compute metrics\n",
    "        if problem_type == \"classification\":\n",
    "            optimized_metrics_train = compute_classification_metrics(y_train_model, y_opt_train, proposed_avg, proposed_pos)\n",
    "            optimized_metrics_test = compute_classification_metrics(y_test_model, y_opt_test, proposed_avg, proposed_pos)\n",
    "        else:\n",
    "            optimized_metrics_train = compute_regression_metrics(y_train_model, y_opt_train)\n",
    "            optimized_metrics_test = compute_regression_metrics(y_test_model, y_opt_test)\n",
    "\n",
    "        # Print OPTIMIZED MODEL results\n",
    "        print(\"\\n OPTIMIZED MODEL\")\n",
    "        print(\" 🏋️ TRAIN:\")\n",
    "        for metric_name, metric_value in optimized_metrics_train.items():\n",
    "            print(f\"    - {metric_name}: {metric_value}\")\n",
    "        print(\" 🧪 TEST:\")\n",
    "        for metric_name, metric_value in optimized_metrics_test.items():\n",
    "            print(f\"    - {metric_name}: {metric_value}\")\n",
    "\n",
    "        # Store optimized results\n",
    "        optimized_results[name_tag + \" - 🏋️ TRAIN\"] = {**optimized_metrics_train, \"Best Parameters\": best_params}\n",
    "        optimized_results[name_tag + \" - 🧪 TEST\"] = {**optimized_metrics_test, \"Best Parameters\": best_params}\n",
    "\n",
    "    else:\n",
    "        # Add LinearRegression to optimized table too\n",
    "        optimized_results[name_tag + \" - 🏋️ TRAIN\"] = {**default_metrics_train, \"Best Parameters\": \"N/A\"}\n",
    "        optimized_results[name_tag + \" - 🧪 TEST\"] = {**default_metrics_test, \"Best Parameters\": \"N/A\"}\n",
    "\n",
    "# --------------------------\n",
    "# FINAL COMPARISON TABLES\n",
    "# --------------------------\n",
    "print(\"\\n==================== FINAL DEFAULT MODELS TABLE ====================\")\n",
    "default_results_table = pd.DataFrame(default_results).T\n",
    "display(default_results_table)\n",
    "\n",
    "print(\"\\n==================== FINAL OPTIMIZED MODELS TABLE ====================\")\n",
    "optimized_results_table = pd.DataFrame(optimized_results).T\n",
    "display(optimized_results_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
