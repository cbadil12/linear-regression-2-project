{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c311b1",
   "metadata": {},
   "source": [
    "# EXPLORED EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e95d00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# IMPORTS\n",
    "# -------------------------------\n",
    "\n",
    "# Main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Checking correlation between attributes\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Optimize models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Plot models\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Save models\n",
    "from pickle import dump\n",
    "\n",
    "# CLASSIFICATION MODELS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# REGRESSION MODELS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "402175fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0) LOAD RAW DATAFRAME\n",
      "- ✅ DataFrame loaded sucessfully!\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 0) LOAD RAW DATAFRAME\n",
    "# -------------------------------\n",
    "print(\"STEP 0) LOAD RAW DATAFRAME\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "data_separator = \",\"\n",
    "input_path = \"../data/raw/internal-link.csv\"\n",
    "\n",
    "# Read DataFrame\n",
    "df_raw=pd.read_csv(input_path, sep = data_separator)\n",
    "\n",
    "print(\"- ✅ DataFrame loaded sucessfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30e7ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1) EXPLORE DATAFRAME\n",
      "- ℹ️ Shape of the original DataFrame: (3140, 108)\n",
      "- ℹ️ Content of the original DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>TOT_POP</th>\n",
       "      <th>0-9</th>\n",
       "      <th>0-9 y/o % of total pop</th>\n",
       "      <th>19-Oct</th>\n",
       "      <th>10-19 y/o % of total pop</th>\n",
       "      <th>20-29</th>\n",
       "      <th>20-29 y/o % of total pop</th>\n",
       "      <th>30-39</th>\n",
       "      <th>30-39 y/o % of total pop</th>\n",
       "      <th>...</th>\n",
       "      <th>COPD_number</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>diabetes_Lower 95% CI</th>\n",
       "      <th>diabetes_Upper 95% CI</th>\n",
       "      <th>diabetes_number</th>\n",
       "      <th>CKD_prevalence</th>\n",
       "      <th>CKD_Lower 95% CI</th>\n",
       "      <th>CKD_Upper 95% CI</th>\n",
       "      <th>CKD_number</th>\n",
       "      <th>Urban_rural_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>55601</td>\n",
       "      <td>6787</td>\n",
       "      <td>12.206615</td>\n",
       "      <td>7637</td>\n",
       "      <td>13.735364</td>\n",
       "      <td>6878</td>\n",
       "      <td>12.370281</td>\n",
       "      <td>7089</td>\n",
       "      <td>12.749771</td>\n",
       "      <td>...</td>\n",
       "      <td>3644</td>\n",
       "      <td>12.9</td>\n",
       "      <td>11.9</td>\n",
       "      <td>13.8</td>\n",
       "      <td>5462</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1326</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>218022</td>\n",
       "      <td>24757</td>\n",
       "      <td>11.355276</td>\n",
       "      <td>26913</td>\n",
       "      <td>12.344167</td>\n",
       "      <td>23579</td>\n",
       "      <td>10.814964</td>\n",
       "      <td>25213</td>\n",
       "      <td>11.564429</td>\n",
       "      <td>...</td>\n",
       "      <td>14692</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>20520</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5479</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>24881</td>\n",
       "      <td>2732</td>\n",
       "      <td>10.980266</td>\n",
       "      <td>2960</td>\n",
       "      <td>11.896628</td>\n",
       "      <td>3268</td>\n",
       "      <td>13.134520</td>\n",
       "      <td>3201</td>\n",
       "      <td>12.865239</td>\n",
       "      <td>...</td>\n",
       "      <td>2373</td>\n",
       "      <td>19.7</td>\n",
       "      <td>18.6</td>\n",
       "      <td>20.6</td>\n",
       "      <td>3870</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>887</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>22400</td>\n",
       "      <td>2456</td>\n",
       "      <td>10.964286</td>\n",
       "      <td>2596</td>\n",
       "      <td>11.589286</td>\n",
       "      <td>3029</td>\n",
       "      <td>13.522321</td>\n",
       "      <td>3113</td>\n",
       "      <td>13.897321</td>\n",
       "      <td>...</td>\n",
       "      <td>1789</td>\n",
       "      <td>14.1</td>\n",
       "      <td>13.2</td>\n",
       "      <td>14.9</td>\n",
       "      <td>2511</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>595</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>57840</td>\n",
       "      <td>7095</td>\n",
       "      <td>12.266598</td>\n",
       "      <td>7570</td>\n",
       "      <td>13.087828</td>\n",
       "      <td>6742</td>\n",
       "      <td>11.656293</td>\n",
       "      <td>6884</td>\n",
       "      <td>11.901798</td>\n",
       "      <td>...</td>\n",
       "      <td>4661</td>\n",
       "      <td>13.5</td>\n",
       "      <td>12.6</td>\n",
       "      <td>14.5</td>\n",
       "      <td>6017</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1507</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fips  TOT_POP    0-9  0-9 y/o % of total pop  19-Oct  \\\n",
       "0  1001    55601   6787               12.206615    7637   \n",
       "1  1003   218022  24757               11.355276   26913   \n",
       "2  1005    24881   2732               10.980266    2960   \n",
       "3  1007    22400   2456               10.964286    2596   \n",
       "4  1009    57840   7095               12.266598    7570   \n",
       "\n",
       "   10-19 y/o % of total pop  20-29  20-29 y/o % of total pop  30-39  \\\n",
       "0                 13.735364   6878                 12.370281   7089   \n",
       "1                 12.344167  23579                 10.814964  25213   \n",
       "2                 11.896628   3268                 13.134520   3201   \n",
       "3                 11.589286   3029                 13.522321   3113   \n",
       "4                 13.087828   6742                 11.656293   6884   \n",
       "\n",
       "   30-39 y/o % of total pop  ...  COPD_number  diabetes_prevalence  \\\n",
       "0                 12.749771  ...         3644                 12.9   \n",
       "1                 11.564429  ...        14692                 12.0   \n",
       "2                 12.865239  ...         2373                 19.7   \n",
       "3                 13.897321  ...         1789                 14.1   \n",
       "4                 11.901798  ...         4661                 13.5   \n",
       "\n",
       "   diabetes_Lower 95% CI  diabetes_Upper 95% CI  diabetes_number  \\\n",
       "0                   11.9                   13.8             5462   \n",
       "1                   11.0                   13.1            20520   \n",
       "2                   18.6                   20.6             3870   \n",
       "3                   13.2                   14.9             2511   \n",
       "4                   12.6                   14.5             6017   \n",
       "\n",
       "   CKD_prevalence  CKD_Lower 95% CI  CKD_Upper 95% CI  CKD_number  \\\n",
       "0             3.1               2.9               3.3        1326   \n",
       "1             3.2               3.0               3.5        5479   \n",
       "2             4.5               4.2               4.8         887   \n",
       "3             3.3               3.1               3.6         595   \n",
       "4             3.4               3.2               3.7        1507   \n",
       "\n",
       "   Urban_rural_code  \n",
       "0                 3  \n",
       "1                 4  \n",
       "2                 6  \n",
       "3                 2  \n",
       "4                 2  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ℹ️ Info of the original DataFrame (dataType and non-null values):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3140 entries, 0 to 3139\n",
      "Data columns (total 108 columns):\n",
      " #    Column                                                                         Non-Null Count  Dtype  \n",
      "---   ------                                                                         --------------  -----  \n",
      " 0    fips                                                                           3140 non-null   int64  \n",
      " 1    TOT_POP                                                                        3140 non-null   int64  \n",
      " 2    0-9                                                                            3140 non-null   int64  \n",
      " 3    0-9 y/o % of total pop                                                         3140 non-null   float64\n",
      " 4    19-Oct                                                                         3140 non-null   int64  \n",
      " 5    10-19 y/o % of total pop                                                       3140 non-null   float64\n",
      " 6    20-29                                                                          3140 non-null   int64  \n",
      " 7    20-29 y/o % of total pop                                                       3140 non-null   float64\n",
      " 8    30-39                                                                          3140 non-null   int64  \n",
      " 9    30-39 y/o % of total pop                                                       3140 non-null   float64\n",
      " 10   40-49                                                                          3140 non-null   int64  \n",
      " 11   40-49 y/o % of total pop                                                       3140 non-null   float64\n",
      " 12   50-59                                                                          3140 non-null   int64  \n",
      " 13   50-59 y/o % of total pop                                                       3140 non-null   float64\n",
      " 14   60-69                                                                          3140 non-null   int64  \n",
      " 15   60-69 y/o % of total pop                                                       3140 non-null   float64\n",
      " 16   70-79                                                                          3140 non-null   int64  \n",
      " 17   70-79 y/o % of total pop                                                       3140 non-null   float64\n",
      " 18   80+                                                                            3140 non-null   int64  \n",
      " 19   80+ y/o % of total pop                                                         3140 non-null   float64\n",
      " 20   White-alone pop                                                                3140 non-null   int64  \n",
      " 21   % White-alone                                                                  3140 non-null   float64\n",
      " 22   Black-alone pop                                                                3140 non-null   int64  \n",
      " 23   % Black-alone                                                                  3140 non-null   float64\n",
      " 24   Native American/American Indian-alone pop                                      3140 non-null   int64  \n",
      " 25   % NA/AI-alone                                                                  3140 non-null   float64\n",
      " 26   Asian-alone pop                                                                3140 non-null   int64  \n",
      " 27   % Asian-alone                                                                  3140 non-null   float64\n",
      " 28   Hawaiian/Pacific Islander-alone pop                                            3140 non-null   int64  \n",
      " 29   % Hawaiian/PI-alone                                                            3140 non-null   float64\n",
      " 30   Two or more races pop                                                          3140 non-null   int64  \n",
      " 31   % Two or more races                                                            3140 non-null   float64\n",
      " 32   POP_ESTIMATE_2018                                                              3140 non-null   int64  \n",
      " 33   N_POP_CHG_2018                                                                 3140 non-null   int64  \n",
      " 34   GQ_ESTIMATES_2018                                                              3140 non-null   int64  \n",
      " 35   R_birth_2018                                                                   3140 non-null   float64\n",
      " 36   R_death_2018                                                                   3140 non-null   float64\n",
      " 37   R_NATURAL_INC_2018                                                             3140 non-null   float64\n",
      " 38   R_INTERNATIONAL_MIG_2018                                                       3140 non-null   float64\n",
      " 39   R_DOMESTIC_MIG_2018                                                            3140 non-null   float64\n",
      " 40   R_NET_MIG_2018                                                                 3140 non-null   float64\n",
      " 41   Less than a high school diploma 2014-18                                        3140 non-null   int64  \n",
      " 42   High school diploma only 2014-18                                               3140 non-null   int64  \n",
      " 43   Some college or associate's degree 2014-18                                     3140 non-null   int64  \n",
      " 44   Bachelor's degree or higher 2014-18                                            3140 non-null   int64  \n",
      " 45   Percent of adults with less than a high school diploma 2014-18                 3140 non-null   float64\n",
      " 46   Percent of adults with a high school diploma only 2014-18                      3140 non-null   float64\n",
      " 47   Percent of adults completing some college or associate's degree 2014-18        3140 non-null   float64\n",
      " 48   Percent of adults with a bachelor's degree or higher 2014-18                   3140 non-null   float64\n",
      " 49   POVALL_2018                                                                    3140 non-null   int64  \n",
      " 50   PCTPOVALL_2018                                                                 3140 non-null   float64\n",
      " 51   PCTPOV017_2018                                                                 3140 non-null   float64\n",
      " 52   PCTPOV517_2018                                                                 3140 non-null   float64\n",
      " 53   MEDHHINC_2018                                                                  3140 non-null   int64  \n",
      " 54   CI90LBINC_2018                                                                 3140 non-null   int64  \n",
      " 55   CI90UBINC_2018                                                                 3140 non-null   int64  \n",
      " 56   Civilian_labor_force_2018                                                      3140 non-null   int64  \n",
      " 57   Employed_2018                                                                  3140 non-null   int64  \n",
      " 58   Unemployed_2018                                                                3140 non-null   int64  \n",
      " 59   Unemployment_rate_2018                                                         3140 non-null   float64\n",
      " 60   Median_Household_Income_2018                                                   3140 non-null   int64  \n",
      " 61   Med_HH_Income_Percent_of_State_Total_2018                                      3140 non-null   float64\n",
      " 62   Active Physicians per 100000 Population 2018 (AAMC)                            3140 non-null   float64\n",
      " 63   Total Active Patient Care Physicians per 100000 Population 2018 (AAMC)         3140 non-null   float64\n",
      " 64   Active Primary Care Physicians per 100000 Population 2018 (AAMC)               3140 non-null   float64\n",
      " 65   Active Patient Care Primary Care Physicians per 100000 Population 2018 (AAMC)  3140 non-null   float64\n",
      " 66   Active General Surgeons per 100000 Population 2018 (AAMC)                      3140 non-null   float64\n",
      " 67   Active Patient Care General Surgeons per 100000 Population 2018 (AAMC)         3140 non-null   float64\n",
      " 68   Total nurse practitioners (2019)                                               3140 non-null   float64\n",
      " 69   Total physician assistants (2019)                                              3140 non-null   float64\n",
      " 70   Total Hospitals (2019)                                                         3140 non-null   float64\n",
      " 71   Internal Medicine Primary Care (2019)                                          3140 non-null   float64\n",
      " 72   Family Medicine/General Practice Primary Care (2019)                           3140 non-null   float64\n",
      " 73   Total Specialist Physicians (2019)                                             3140 non-null   float64\n",
      " 74   ICU Beds_x                                                                     3140 non-null   int64  \n",
      " 75   Total Population                                                               3140 non-null   int64  \n",
      " 76   Population Aged 60+                                                            3140 non-null   int64  \n",
      " 77   Percent of Population Aged 60+                                                 3140 non-null   float64\n",
      " 78   COUNTY_NAME                                                                    3140 non-null   object \n",
      " 79   STATE_NAME                                                                     3140 non-null   object \n",
      " 80   STATE_FIPS                                                                     3140 non-null   int64  \n",
      " 81   CNTY_FIPS                                                                      3140 non-null   int64  \n",
      " 82   county_pop2018_18 and older                                                    3140 non-null   int64  \n",
      " 83   anycondition_prevalence                                                        3140 non-null   float64\n",
      " 84   anycondition_Lower 95% CI                                                      3140 non-null   float64\n",
      " 85   anycondition_Upper 95% CI                                                      3140 non-null   float64\n",
      " 86   anycondition_number                                                            3140 non-null   int64  \n",
      " 87   Obesity_prevalence                                                             3140 non-null   float64\n",
      " 88   Obesity_Lower 95% CI                                                           3140 non-null   float64\n",
      " 89   Obesity_Upper 95% CI                                                           3140 non-null   float64\n",
      " 90   Obesity_number                                                                 3140 non-null   int64  \n",
      " 91   Heart disease_prevalence                                                       3140 non-null   float64\n",
      " 92   Heart disease_Lower 95% CI                                                     3140 non-null   float64\n",
      " 93   Heart disease_Upper 95% CI                                                     3140 non-null   float64\n",
      " 94   Heart disease_number                                                           3140 non-null   int64  \n",
      " 95   COPD_prevalence                                                                3140 non-null   float64\n",
      " 96   COPD_Lower 95% CI                                                              3140 non-null   float64\n",
      " 97   COPD_Upper 95% CI                                                              3140 non-null   float64\n",
      " 98   COPD_number                                                                    3140 non-null   int64  \n",
      " 99   diabetes_prevalence                                                            3140 non-null   float64\n",
      " 100  diabetes_Lower 95% CI                                                          3140 non-null   float64\n",
      " 101  diabetes_Upper 95% CI                                                          3140 non-null   float64\n",
      " 102  diabetes_number                                                                3140 non-null   int64  \n",
      " 103  CKD_prevalence                                                                 3140 non-null   float64\n",
      " 104  CKD_Lower 95% CI                                                               3140 non-null   float64\n",
      " 105  CKD_Upper 95% CI                                                               3140 non-null   float64\n",
      " 106  CKD_number                                                                     3140 non-null   int64  \n",
      " 107  Urban_rural_code                                                               3140 non-null   int64  \n",
      "dtypes: float64(61), int64(45), object(2)\n",
      "memory usage: 2.6+ MB\n",
      "\n",
      "- ℹ️ Ordered info by number of non-null values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Non-Null Count</th>\n",
       "      <th>Null Count</th>\n",
       "      <th>Dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fips</th>\n",
       "      <td>fips</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOT_POP</th>\n",
       "      <td>TOT_POP</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-9</th>\n",
       "      <td>0-9</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-9 y/o % of total pop</th>\n",
       "      <td>0-9 y/o % of total pop</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19-Oct</th>\n",
       "      <td>19-Oct</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CKD_prevalence</th>\n",
       "      <td>CKD_prevalence</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CKD_Lower 95% CI</th>\n",
       "      <td>CKD_Lower 95% CI</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CKD_Upper 95% CI</th>\n",
       "      <td>CKD_Upper 95% CI</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CKD_number</th>\n",
       "      <td>CKD_number</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urban_rural_code</th>\n",
       "      <td>Urban_rural_code</td>\n",
       "      <td>3140</td>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Column  Non-Null Count  Null Count  \\\n",
       "fips                                      fips            3140           0   \n",
       "TOT_POP                                TOT_POP            3140           0   \n",
       "0-9                                        0-9            3140           0   \n",
       "0-9 y/o % of total pop  0-9 y/o % of total pop            3140           0   \n",
       "19-Oct                                  19-Oct            3140           0   \n",
       "...                                        ...             ...         ...   \n",
       "CKD_prevalence                  CKD_prevalence            3140           0   \n",
       "CKD_Lower 95% CI              CKD_Lower 95% CI            3140           0   \n",
       "CKD_Upper 95% CI              CKD_Upper 95% CI            3140           0   \n",
       "CKD_number                          CKD_number            3140           0   \n",
       "Urban_rural_code              Urban_rural_code            3140           0   \n",
       "\n",
       "                          Dtype  \n",
       "fips                      int64  \n",
       "TOT_POP                   int64  \n",
       "0-9                       int64  \n",
       "0-9 y/o % of total pop  float64  \n",
       "19-Oct                    int64  \n",
       "...                         ...  \n",
       "CKD_prevalence          float64  \n",
       "CKD_Lower 95% CI        float64  \n",
       "CKD_Upper 95% CI        float64  \n",
       "CKD_number                int64  \n",
       "Urban_rural_code          int64  \n",
       "\n",
       "[108 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - ℹ️ Final DataFrame unique attributes (unsorted):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Unique_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fips</td>\n",
       "      <td>3140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TOT_POP</td>\n",
       "      <td>3074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-9</td>\n",
       "      <td>2723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-9 y/o % of total pop</td>\n",
       "      <td>3136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19-Oct</td>\n",
       "      <td>2743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>CKD_prevalence</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>CKD_Lower 95% CI</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>CKD_Upper 95% CI</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>CKD_number</td>\n",
       "      <td>1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Urban_rural_code</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Column  Unique_Count\n",
       "0                      fips          3140\n",
       "1                   TOT_POP          3074\n",
       "2                       0-9          2723\n",
       "3    0-9 y/o % of total pop          3136\n",
       "4                    19-Oct          2743\n",
       "..                      ...           ...\n",
       "103          CKD_prevalence            43\n",
       "104        CKD_Lower 95% CI            39\n",
       "105        CKD_Upper 95% CI            46\n",
       "106              CKD_number          1894\n",
       "107        Urban_rural_code             6\n",
       "\n",
       "[108 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - ℹ️ Ordered unique attributes (fewest unique first):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Unique_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Urban_rural_code</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Active Patient Care General Surgeons per 10000...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Active General Surgeons per 100000 Population ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>CKD_Lower 95% CI</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>CKD_prevalence</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>60-69 y/o % of total pop</td>\n",
       "      <td>3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>80+ y/o % of total pop</td>\n",
       "      <td>3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>70-79 y/o % of total pop</td>\n",
       "      <td>3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>% White-alone</td>\n",
       "      <td>3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fips</td>\n",
       "      <td>3140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Column  Unique_Count\n",
       "107                                   Urban_rural_code             6\n",
       "67   Active Patient Care General Surgeons per 10000...            30\n",
       "66   Active General Surgeons per 100000 Population ...            32\n",
       "104                                   CKD_Lower 95% CI            39\n",
       "103                                     CKD_prevalence            43\n",
       "..                                                 ...           ...\n",
       "15                            60-69 y/o % of total pop          3139\n",
       "19                              80+ y/o % of total pop          3139\n",
       "17                            70-79 y/o % of total pop          3139\n",
       "21                                       % White-alone          3139\n",
       "0                                                 fips          3140\n",
       "\n",
       "[108 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - ⚠️ Consider dropping the following columns for having UNIQUE values for EVERY row:\n",
      "   - fips\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 1) EXPLORE DATAFRAME\n",
    "# -------------------------------\n",
    "print(\"STEP 1) EXPLORE DATAFRAME\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S1 = df_raw.copy()\n",
    "\n",
    "# Print shape\n",
    "print(f\"- ℹ️ Shape of the original DataFrame: {df_S1.shape}\")\n",
    "\n",
    "# Show first rows\n",
    "print(\"- ℹ️ Content of the original DataFrame:\")\n",
    "display(df_S1.head(5))\n",
    "\n",
    "# Show dataframe info\n",
    "print(\"- ℹ️ Info of the original DataFrame (dataType and non-null values):\")\n",
    "df_S1.info(verbose=True, show_counts=True)\n",
    "\n",
    "# Ordered info (fewest non-null first)\n",
    "print(\"\\n- ℹ️ Ordered info by number of non-null values:\")\n",
    "ordered_info = pd.DataFrame({\n",
    "    \"Column\": df_S1.columns,\n",
    "    \"Non-Null Count\": df_S1.notnull().sum(),\n",
    "    \"Null Count\": df_S1.isnull().sum(),\n",
    "    \"Dtype\": df_S1.dtypes.astype(str)\n",
    "}).sort_values(by=\"Non-Null Count\", ascending=True)\n",
    "\n",
    "display(ordered_info)\n",
    "\n",
    "# Count unique attributes (unsorted)\n",
    "df_S1_summary = pd.DataFrame({\n",
    "    \"Column\": df_S1.columns,\n",
    "    \"Unique_Count\": df_S1.nunique().values\n",
    "})\n",
    "print(\"\\n - ℹ️ Final DataFrame unique attributes (unsorted):\")\n",
    "display(df_S1_summary)\n",
    "\n",
    "# Ordered summary (fewest unique values first)\n",
    "print(\"\\n - ℹ️ Ordered unique attributes (fewest unique first):\")\n",
    "df_S1_summary_ordered = df_S1_summary.sort_values(by=\"Unique_Count\", ascending=True)\n",
    "display(df_S1_summary_ordered)\n",
    "\n",
    "# Automatic Warning for high-uniqueness columns\n",
    "unique_counts = df_S1.nunique()\n",
    "high_unique_cols = unique_counts[unique_counts == len(df_S1)].index.tolist()\n",
    "if len(high_unique_cols) > 0:\n",
    "    print(\"\\n - ⚠️ Consider dropping the following columns for having UNIQUE values for EVERY row:\")\n",
    "    for col in high_unique_cols:\n",
    "        print(f\"   - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1c501",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- fips is considered non relevant because is unique of each ocurrence (used to designate a specific county in the United States)\n",
    "- there are not non-null values in the data -> nice\n",
    "- Heart disease_prevalence is going to be the target variable of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa8cc127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2) SELECT RELEVANT ATTRIBUTES\n",
      "- ✅ Non-Relevant attributes have been dropped.\n",
      " - ℹ️ Previous df's columns: 108\n",
      " - ℹ️ Current df's columns: 103\n",
      " - ℹ️ Final DataFrame shape: (3140, 103)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOT_POP</th>\n",
       "      <th>0-9</th>\n",
       "      <th>0-9 y/o % of total pop</th>\n",
       "      <th>19-Oct</th>\n",
       "      <th>10-19 y/o % of total pop</th>\n",
       "      <th>20-29</th>\n",
       "      <th>20-29 y/o % of total pop</th>\n",
       "      <th>30-39</th>\n",
       "      <th>30-39 y/o % of total pop</th>\n",
       "      <th>40-49</th>\n",
       "      <th>...</th>\n",
       "      <th>COPD_number</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>diabetes_Lower 95% CI</th>\n",
       "      <th>diabetes_Upper 95% CI</th>\n",
       "      <th>diabetes_number</th>\n",
       "      <th>CKD_prevalence</th>\n",
       "      <th>CKD_Lower 95% CI</th>\n",
       "      <th>CKD_Upper 95% CI</th>\n",
       "      <th>CKD_number</th>\n",
       "      <th>Urban_rural_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55601</td>\n",
       "      <td>6787</td>\n",
       "      <td>12.206615</td>\n",
       "      <td>7637</td>\n",
       "      <td>13.735364</td>\n",
       "      <td>6878</td>\n",
       "      <td>12.370281</td>\n",
       "      <td>7089</td>\n",
       "      <td>12.749771</td>\n",
       "      <td>7582</td>\n",
       "      <td>...</td>\n",
       "      <td>3644</td>\n",
       "      <td>12.9</td>\n",
       "      <td>11.9</td>\n",
       "      <td>13.8</td>\n",
       "      <td>5462</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1326</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218022</td>\n",
       "      <td>24757</td>\n",
       "      <td>11.355276</td>\n",
       "      <td>26913</td>\n",
       "      <td>12.344167</td>\n",
       "      <td>23579</td>\n",
       "      <td>10.814964</td>\n",
       "      <td>25213</td>\n",
       "      <td>11.564429</td>\n",
       "      <td>27338</td>\n",
       "      <td>...</td>\n",
       "      <td>14692</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>20520</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5479</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24881</td>\n",
       "      <td>2732</td>\n",
       "      <td>10.980266</td>\n",
       "      <td>2960</td>\n",
       "      <td>11.896628</td>\n",
       "      <td>3268</td>\n",
       "      <td>13.134520</td>\n",
       "      <td>3201</td>\n",
       "      <td>12.865239</td>\n",
       "      <td>3074</td>\n",
       "      <td>...</td>\n",
       "      <td>2373</td>\n",
       "      <td>19.7</td>\n",
       "      <td>18.6</td>\n",
       "      <td>20.6</td>\n",
       "      <td>3870</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>887</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22400</td>\n",
       "      <td>2456</td>\n",
       "      <td>10.964286</td>\n",
       "      <td>2596</td>\n",
       "      <td>11.589286</td>\n",
       "      <td>3029</td>\n",
       "      <td>13.522321</td>\n",
       "      <td>3113</td>\n",
       "      <td>13.897321</td>\n",
       "      <td>3038</td>\n",
       "      <td>...</td>\n",
       "      <td>1789</td>\n",
       "      <td>14.1</td>\n",
       "      <td>13.2</td>\n",
       "      <td>14.9</td>\n",
       "      <td>2511</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>595</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57840</td>\n",
       "      <td>7095</td>\n",
       "      <td>12.266598</td>\n",
       "      <td>7570</td>\n",
       "      <td>13.087828</td>\n",
       "      <td>6742</td>\n",
       "      <td>11.656293</td>\n",
       "      <td>6884</td>\n",
       "      <td>11.901798</td>\n",
       "      <td>7474</td>\n",
       "      <td>...</td>\n",
       "      <td>4661</td>\n",
       "      <td>13.5</td>\n",
       "      <td>12.6</td>\n",
       "      <td>14.5</td>\n",
       "      <td>6017</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1507</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TOT_POP    0-9  0-9 y/o % of total pop  19-Oct  10-19 y/o % of total pop  \\\n",
       "0    55601   6787               12.206615    7637                 13.735364   \n",
       "1   218022  24757               11.355276   26913                 12.344167   \n",
       "2    24881   2732               10.980266    2960                 11.896628   \n",
       "3    22400   2456               10.964286    2596                 11.589286   \n",
       "4    57840   7095               12.266598    7570                 13.087828   \n",
       "\n",
       "   20-29  20-29 y/o % of total pop  30-39  30-39 y/o % of total pop  40-49  \\\n",
       "0   6878                 12.370281   7089                 12.749771   7582   \n",
       "1  23579                 10.814964  25213                 11.564429  27338   \n",
       "2   3268                 13.134520   3201                 12.865239   3074   \n",
       "3   3029                 13.522321   3113                 13.897321   3038   \n",
       "4   6742                 11.656293   6884                 11.901798   7474   \n",
       "\n",
       "   ...  COPD_number  diabetes_prevalence  diabetes_Lower 95% CI  \\\n",
       "0  ...         3644                 12.9                   11.9   \n",
       "1  ...        14692                 12.0                   11.0   \n",
       "2  ...         2373                 19.7                   18.6   \n",
       "3  ...         1789                 14.1                   13.2   \n",
       "4  ...         4661                 13.5                   12.6   \n",
       "\n",
       "   diabetes_Upper 95% CI  diabetes_number  CKD_prevalence  CKD_Lower 95% CI  \\\n",
       "0                   13.8             5462             3.1               2.9   \n",
       "1                   13.1            20520             3.2               3.0   \n",
       "2                   20.6             3870             4.5               4.2   \n",
       "3                   14.9             2511             3.3               3.1   \n",
       "4                   14.5             6017             3.4               3.2   \n",
       "\n",
       "   CKD_Upper 95% CI  CKD_number  Urban_rural_code  \n",
       "0               3.3        1326                 3  \n",
       "1               3.5        5479                 4  \n",
       "2               4.8         887                 6  \n",
       "3               3.6         595                 2  \n",
       "4               3.7        1507                 2  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - ℹ️ Final DataFrame unique attributes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Unique_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TOT_POP</td>\n",
       "      <td>3074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-9</td>\n",
       "      <td>2723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-9 y/o % of total pop</td>\n",
       "      <td>3136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19-Oct</td>\n",
       "      <td>2743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-19 y/o % of total pop</td>\n",
       "      <td>3137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>CKD_prevalence</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>CKD_Lower 95% CI</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>CKD_Upper 95% CI</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>CKD_number</td>\n",
       "      <td>1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Urban_rural_code</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Column  Unique_Count\n",
       "0                     TOT_POP          3074\n",
       "1                         0-9          2723\n",
       "2      0-9 y/o % of total pop          3136\n",
       "3                      19-Oct          2743\n",
       "4    10-19 y/o % of total pop          3137\n",
       "..                        ...           ...\n",
       "98             CKD_prevalence            43\n",
       "99           CKD_Lower 95% CI            39\n",
       "100          CKD_Upper 95% CI            46\n",
       "101                CKD_number          1894\n",
       "102          Urban_rural_code             6\n",
       "\n",
       "[103 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 2) SELECT RELEVANT ATTRIBUTES\n",
    "# -------------------------------\n",
    "print(\"STEP 2) SELECT RELEVANT ATTRIBUTES\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S2 = df_S1.copy()\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "df_S2=df_S2.drop(labels=[\"fips\", \"STATE_FIPS\", \"CNTY_FIPS\", \"COUNTY_NAME\", \"STATE_NAME\"], axis =1) # Drop non-relevant attributes\n",
    "\n",
    "# Print results\n",
    "print(\"- ✅ Non-Relevant attributes have been dropped.\")\n",
    "print(f\" - ℹ️ Previous df's columns: {len(df_S1.columns)}\")\n",
    "print(f\" - ℹ️ Current df's columns: {len(df_S2.columns)}\")\n",
    "print(f\" - ℹ️ Final DataFrame shape: {df_S2.shape}\")\n",
    "display(df_S2.head())\n",
    "\n",
    "# Count attributes\n",
    "df_S2_summary = pd.DataFrame({\n",
    "    \"Column\": df_S2.columns,\n",
    "    \"Unique_Count\": df_S2.nunique().values\n",
    "})\n",
    "print(\" - ℹ️ Final DataFrame unique attributes:\")\n",
    "display(df_S2_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c38a8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3) REMOVE DUPLICATES\n",
      "- ✅ Previous DataFrame does not contain duplicates:\n",
      " - ℹ️ Previous df's shape:  (3140, 103)\n",
      " - ℹ️ Current df's  shape:  (3140, 103)\n",
      " - ℹ️ These are the dropped duplicates:\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 3) REMOVE DUPLICATES\n",
    "# -------------------------------\n",
    "print(\"STEP 3) REMOVE DUPLICATES\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S3 = df_S2.copy()\n",
    "\n",
    "num_duplicates=df_S3.duplicated().sum()\n",
    "if num_duplicates == 0:\n",
    "    df_S3=df_S3\n",
    "    print(\"- ✅ Previous DataFrame does not contain duplicates:\")\n",
    "    print(\" - ℹ️ Previous df's shape: \",df_S2.shape)\n",
    "    print(\" - ℹ️ Current df's  shape: \",df_S3.shape)\n",
    "    print(\" - ℹ️ These are the dropped duplicates:\")\n",
    "else:\n",
    "    df_S3_duplicates=df_S3[df_S3.duplicated()] #Works as bool mask\n",
    "    df_S3=df_S3.drop_duplicates()\n",
    "    print(\"- ⚠️ Previous DataFrame contained \" + str(num_duplicates) + \" duplicates that have been dropped:\")\n",
    "    print(\" - ℹ️ Previous df's shape: \",df_S2.shape)\n",
    "    print(\" - ℹ️ Current df's  shape: \",df_S3.shape)\n",
    "    print(\" - ℹ️ These are the dropped duplicates:\")\n",
    "    display(df_S3_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38f2266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4) CLASSIFY ATTRIBUTES AND TARGET VARIABLE\n",
      "- ℹ️ Proposed CATEGORY Attributes: ['Urban_rural_code']\n",
      "- ℹ️ Proposed NUMERIC Attributes: ['TOT_POP', '0-9', '0-9 y/o % of total pop', '19-Oct', '10-19 y/o % of total pop', '20-29', '20-29 y/o % of total pop', '30-39', '30-39 y/o % of total pop', '40-49', '40-49 y/o % of total pop', '50-59', '50-59 y/o % of total pop', '60-69', '60-69 y/o % of total pop', '70-79', '70-79 y/o % of total pop', '80+', '80+ y/o % of total pop', 'White-alone pop', '% White-alone', 'Black-alone pop', '% Black-alone', 'Native American/American Indian-alone pop', '% NA/AI-alone', 'Asian-alone pop', '% Asian-alone', 'Hawaiian/Pacific Islander-alone pop', '% Hawaiian/PI-alone', 'Two or more races pop', '% Two or more races', 'POP_ESTIMATE_2018', 'N_POP_CHG_2018', 'GQ_ESTIMATES_2018', 'R_birth_2018', 'R_death_2018', 'R_NATURAL_INC_2018', 'R_INTERNATIONAL_MIG_2018', 'R_DOMESTIC_MIG_2018', 'R_NET_MIG_2018', 'Less than a high school diploma 2014-18', 'High school diploma only 2014-18', \"Some college or associate's degree 2014-18\", \"Bachelor's degree or higher 2014-18\", 'Percent of adults with less than a high school diploma 2014-18', 'Percent of adults with a high school diploma only 2014-18', \"Percent of adults completing some college or associate's degree 2014-18\", \"Percent of adults with a bachelor's degree or higher 2014-18\", 'POVALL_2018', 'PCTPOVALL_2018', 'PCTPOV017_2018', 'PCTPOV517_2018', 'MEDHHINC_2018', 'CI90LBINC_2018', 'CI90UBINC_2018', 'Civilian_labor_force_2018', 'Employed_2018', 'Unemployed_2018', 'Unemployment_rate_2018', 'Median_Household_Income_2018', 'Med_HH_Income_Percent_of_State_Total_2018', 'Active Physicians per 100000 Population 2018 (AAMC)', 'Total Active Patient Care Physicians per 100000 Population 2018 (AAMC)', 'Active Primary Care Physicians per 100000 Population 2018 (AAMC)', 'Active Patient Care Primary Care Physicians per 100000 Population 2018 (AAMC)', 'Active General Surgeons per 100000 Population 2018 (AAMC)', 'Active Patient Care General Surgeons per 100000 Population 2018 (AAMC)', 'Total nurse practitioners (2019)', 'Total physician assistants (2019)', 'Total Hospitals (2019)', 'Internal Medicine Primary Care (2019)', 'Family Medicine/General Practice Primary Care (2019)', 'Total Specialist Physicians (2019)', 'ICU Beds_x', 'Total Population', 'Population Aged 60+', 'Percent of Population Aged 60+', 'county_pop2018_18 and older', 'anycondition_prevalence', 'anycondition_Lower 95% CI', 'anycondition_Upper 95% CI', 'anycondition_number', 'Obesity_prevalence', 'Obesity_Lower 95% CI', 'Obesity_Upper 95% CI', 'Obesity_number', 'Heart disease_prevalence', 'Heart disease_Lower 95% CI', 'Heart disease_Upper 95% CI', 'Heart disease_number', 'COPD_prevalence', 'COPD_Lower 95% CI', 'COPD_Upper 95% CI', 'COPD_number', 'diabetes_prevalence', 'diabetes_Lower 95% CI', 'diabetes_Upper 95% CI', 'diabetes_number', 'CKD_prevalence', 'CKD_Lower 95% CI', 'CKD_Upper 95% CI', 'CKD_number']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 4) CLASSIFY ATTRIBUTES AND TARGET VARIABLE\n",
    "# -------------------------------\n",
    "print(\"STEP 4) CLASSIFY ATTRIBUTES AND TARGET VARIABLE\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S4 = df_S3.copy()\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "var_type_proposal_threshold = 0.50  # [%] Under this percentage of unique values, the attribute is proposed as CATEGORIC\n",
    "float_discrete_threshold = min(30, round(0.02 * len(df_S4))) # Dynamic threshold for FLOAT to be considered DISCRETE\n",
    "\n",
    "# List of columns\n",
    "columns = df_S4.columns.tolist()\n",
    "\n",
    "# Iterate through columns\n",
    "category_var_auto = []\n",
    "numeric_var_auto = []\n",
    "for col in df_S4.columns:\n",
    "    col_data = df_S4[col].dropna()\n",
    "    total_rows = len(df_S4)\n",
    "\n",
    "    # Skip empty columns\n",
    "    if total_rows == 0:\n",
    "        continue\n",
    "\n",
    "    unique_count = col_data.nunique()\n",
    "    unique_ratio = unique_count / total_rows * 100\n",
    "    col_dtype = str(df_S4[col].dtype)\n",
    "\n",
    "    # Case 1: text-based columns\n",
    "    if col_dtype in [\"object\", \"category\"]:\n",
    "        category_var_auto.append(col)\n",
    "        continue\n",
    "\n",
    "    # Case 2: integer columns\n",
    "    if col_dtype.startswith(\"int\"):\n",
    "        if unique_ratio <= var_type_proposal_threshold:\n",
    "            category_var_auto.append(col)\n",
    "        else:\n",
    "            numeric_var_auto.append(col)\n",
    "        continue\n",
    "\n",
    "    # Case 3: float columns\n",
    "    if col_dtype.startswith(\"float\"):\n",
    "        if unique_ratio <= var_type_proposal_threshold:\n",
    "            category_var_auto.append(col)\n",
    "        else:\n",
    "            numeric_var_auto.append(col)\n",
    "        continue\n",
    "\n",
    "# Print proposed Data Types\n",
    "print(\"- ℹ️ Proposed CATEGORY Attributes: \" + str(category_var_auto))\n",
    "print(\"- ℹ️ Proposed NUMERIC Attributes: \" + str(numeric_var_auto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c7306f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "y_var = \"Heart disease_prevalence\" # Confirm target variable\n",
    "if_target_is_binary_treat_as_categoric = False # Confirm treatment for target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c25211d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ℹ️ Confirmed CATEGORY Attributes:\n",
      "   ↳ BINARY: []\n",
      "   ↳ MULTICLASS: ['Urban_rural_code']\n",
      "   ↳ CONSTANT: []\n",
      "- ℹ️ Confirmed NUMERIC Attributes: ['TOT_POP', '0-9', '0-9 y/o % of total pop', '19-Oct', '10-19 y/o % of total pop', '20-29', '20-29 y/o % of total pop', '30-39', '30-39 y/o % of total pop', '40-49', '40-49 y/o % of total pop', '50-59', '50-59 y/o % of total pop', '60-69', '60-69 y/o % of total pop', '70-79', '70-79 y/o % of total pop', '80+', '80+ y/o % of total pop', 'White-alone pop', '% White-alone', 'Black-alone pop', '% Black-alone', 'Native American/American Indian-alone pop', '% NA/AI-alone', 'Asian-alone pop', '% Asian-alone', 'Hawaiian/Pacific Islander-alone pop', '% Hawaiian/PI-alone', 'Two or more races pop', '% Two or more races', 'POP_ESTIMATE_2018', 'N_POP_CHG_2018', 'GQ_ESTIMATES_2018', 'R_birth_2018', 'R_death_2018', 'R_NATURAL_INC_2018', 'R_INTERNATIONAL_MIG_2018', 'R_DOMESTIC_MIG_2018', 'R_NET_MIG_2018', 'Less than a high school diploma 2014-18', 'High school diploma only 2014-18', \"Some college or associate's degree 2014-18\", \"Bachelor's degree or higher 2014-18\", 'Percent of adults with less than a high school diploma 2014-18', 'Percent of adults with a high school diploma only 2014-18', \"Percent of adults completing some college or associate's degree 2014-18\", \"Percent of adults with a bachelor's degree or higher 2014-18\", 'POVALL_2018', 'PCTPOVALL_2018', 'PCTPOV017_2018', 'PCTPOV517_2018', 'MEDHHINC_2018', 'CI90LBINC_2018', 'CI90UBINC_2018', 'Civilian_labor_force_2018', 'Employed_2018', 'Unemployed_2018', 'Unemployment_rate_2018', 'Median_Household_Income_2018', 'Med_HH_Income_Percent_of_State_Total_2018', 'Active Physicians per 100000 Population 2018 (AAMC)', 'Total Active Patient Care Physicians per 100000 Population 2018 (AAMC)', 'Active Primary Care Physicians per 100000 Population 2018 (AAMC)', 'Active Patient Care Primary Care Physicians per 100000 Population 2018 (AAMC)', 'Active General Surgeons per 100000 Population 2018 (AAMC)', 'Active Patient Care General Surgeons per 100000 Population 2018 (AAMC)', 'Total nurse practitioners (2019)', 'Total physician assistants (2019)', 'Total Hospitals (2019)', 'Internal Medicine Primary Care (2019)', 'Family Medicine/General Practice Primary Care (2019)', 'Total Specialist Physicians (2019)', 'ICU Beds_x', 'Total Population', 'Population Aged 60+', 'Percent of Population Aged 60+', 'county_pop2018_18 and older', 'anycondition_prevalence', 'anycondition_Lower 95% CI', 'anycondition_Upper 95% CI', 'anycondition_number', 'Obesity_prevalence', 'Obesity_Lower 95% CI', 'Obesity_Upper 95% CI', 'Obesity_number', 'Heart disease_Lower 95% CI', 'Heart disease_Upper 95% CI', 'Heart disease_number', 'COPD_prevalence', 'COPD_Lower 95% CI', 'COPD_Upper 95% CI', 'COPD_number', 'diabetes_prevalence', 'diabetes_Lower 95% CI', 'diabetes_Upper 95% CI', 'diabetes_number', 'CKD_prevalence', 'CKD_Lower 95% CI', 'CKD_Upper 95% CI', 'CKD_number']\n",
      "   ↳ DISCRETE: ['TOT_POP', '0-9', '19-Oct', '20-29', '30-39', '40-49', '50-59', '60-69', '70-79', '80+', 'White-alone pop', 'Black-alone pop', 'Native American/American Indian-alone pop', 'Asian-alone pop', 'Hawaiian/Pacific Islander-alone pop', 'Two or more races pop', 'POP_ESTIMATE_2018', 'N_POP_CHG_2018', 'GQ_ESTIMATES_2018', 'Less than a high school diploma 2014-18', 'High school diploma only 2014-18', \"Some college or associate's degree 2014-18\", \"Bachelor's degree or higher 2014-18\", 'POVALL_2018', 'MEDHHINC_2018', 'CI90LBINC_2018', 'CI90UBINC_2018', 'Civilian_labor_force_2018', 'Employed_2018', 'Unemployed_2018', 'Median_Household_Income_2018', 'ICU Beds_x', 'Total Population', 'Population Aged 60+', 'county_pop2018_18 and older', 'anycondition_number', 'Obesity_number', 'Heart disease_number', 'COPD_number', 'diabetes_number', 'CKD_number']\n",
      "   ↳ CONTINUOUS: ['0-9 y/o % of total pop', '10-19 y/o % of total pop', '20-29 y/o % of total pop', '30-39 y/o % of total pop', '40-49 y/o % of total pop', '50-59 y/o % of total pop', '60-69 y/o % of total pop', '70-79 y/o % of total pop', '80+ y/o % of total pop', '% White-alone', '% Black-alone', '% NA/AI-alone', '% Asian-alone', '% Hawaiian/PI-alone', '% Two or more races', 'R_birth_2018', 'R_death_2018', 'R_NATURAL_INC_2018', 'R_INTERNATIONAL_MIG_2018', 'R_DOMESTIC_MIG_2018', 'R_NET_MIG_2018', 'Percent of adults with less than a high school diploma 2014-18', 'Percent of adults with a high school diploma only 2014-18', \"Percent of adults completing some college or associate's degree 2014-18\", \"Percent of adults with a bachelor's degree or higher 2014-18\", 'PCTPOVALL_2018', 'PCTPOV017_2018', 'PCTPOV517_2018', 'Unemployment_rate_2018', 'Med_HH_Income_Percent_of_State_Total_2018', 'Active Physicians per 100000 Population 2018 (AAMC)', 'Total Active Patient Care Physicians per 100000 Population 2018 (AAMC)', 'Active Primary Care Physicians per 100000 Population 2018 (AAMC)', 'Active Patient Care Primary Care Physicians per 100000 Population 2018 (AAMC)', 'Active General Surgeons per 100000 Population 2018 (AAMC)', 'Active Patient Care General Surgeons per 100000 Population 2018 (AAMC)', 'Total nurse practitioners (2019)', 'Total physician assistants (2019)', 'Total Hospitals (2019)', 'Internal Medicine Primary Care (2019)', 'Family Medicine/General Practice Primary Care (2019)', 'Total Specialist Physicians (2019)', 'Percent of Population Aged 60+', 'anycondition_prevalence', 'anycondition_Lower 95% CI', 'anycondition_Upper 95% CI', 'Obesity_prevalence', 'Obesity_Lower 95% CI', 'Obesity_Upper 95% CI', 'Heart disease_Lower 95% CI', 'Heart disease_Upper 95% CI', 'COPD_prevalence', 'COPD_Lower 95% CI', 'COPD_Upper 95% CI', 'diabetes_prevalence', 'diabetes_Lower 95% CI', 'diabetes_Upper 95% CI', 'CKD_prevalence', 'CKD_Lower 95% CI', 'CKD_Upper 95% CI']\n",
      "- ℹ️ Confirmed TARGET Variable: Heart disease_prevalence -> NUMERIC and CONTINUOUS\n"
     ]
    }
   ],
   "source": [
    "# Confirm categories and target variable\n",
    "category_att = []\n",
    "numeric_att = []\n",
    "for att in category_var_auto:\n",
    "    if att != y_var:\n",
    "        category_att.append(att)\n",
    "for att in numeric_var_auto:\n",
    "    if att != y_var:\n",
    "        numeric_att.append(att)\n",
    "\n",
    "# Checking CATEGORY attributes\n",
    "binary_att = []\n",
    "multiclass_att = []\n",
    "constant_att = []\n",
    "for att in category_att:\n",
    "    att_unique_values = df_S4[att].nunique()\n",
    "\n",
    "    if att_unique_values == 2:\n",
    "        binary_att.append(att)\n",
    "    elif att_unique_values > 2:\n",
    "        multiclass_att.append(att)\n",
    "    else:\n",
    "        constant_att.append(att)\n",
    "\n",
    "# Checking NUMERIC attributes\n",
    "discrete_att = []\n",
    "continuos_att = []\n",
    "for att in numeric_att:\n",
    "    att_dtype = df_S4[att].dtype.kind\n",
    "    unique_count = df_S4[att].nunique()\n",
    "\n",
    "    if att_dtype in ['i', 'u']:\n",
    "        discrete_att.append(att)\n",
    "    elif att_dtype == 'f' and unique_count < float_discrete_threshold:\n",
    "        discrete_att.append(att)\n",
    "    else:\n",
    "        continuos_att.append(att)\n",
    "\n",
    "# Checking TARGET variable\n",
    "y_unique_values = df_S4[y_var].nunique()\n",
    "y_dtype = df_S4[y_var].dtype.kind\n",
    "\n",
    "if y_var in category_var_auto:\n",
    "    if y_unique_values == 2:\n",
    "        y_var_type = \"CATEGORIC\"\n",
    "        y_var_subtype = \"BINARY\"\n",
    "    elif y_unique_values > 2:\n",
    "        y_var_type = \"CATEGORIC\"\n",
    "        y_var_subtype = \"MULTICLASS\"\n",
    "    else:\n",
    "        y_var_type = \"CATEGORIC\"\n",
    "        y_var_subtype = \"CONSTANT\"\n",
    "else:\n",
    "    if y_unique_values == 2 and if_target_is_binary_treat_as_categoric:\n",
    "        y_var_type = \"CATEGORIC\"\n",
    "        y_var_subtype = \"BINARY\"\n",
    "    elif y_dtype in ['i', 'u']:\n",
    "        y_var_type = \"NUMERIC\"\n",
    "        y_var_subtype = \"DISCRETE\"\n",
    "    elif y_dtype == 'f' and y_unique_values < float_discrete_threshold:\n",
    "        y_var_type = \"NUMERIC\"\n",
    "        y_var_subtype = \"DISCRETE\"\n",
    "    else:\n",
    "        y_var_type = \"NUMERIC\"\n",
    "        y_var_subtype = \"CONTINUOUS\"\n",
    "\n",
    "# Print results\n",
    "print(\"- ℹ️ Confirmed CATEGORY Attributes:\")\n",
    "print(\"   ↳ BINARY: \" + str(binary_att))\n",
    "print(\"   ↳ MULTICLASS: \" + str(multiclass_att))\n",
    "print(\"   ↳ CONSTANT: \" + str(constant_att))\n",
    "print(\"- ℹ️ Confirmed NUMERIC Attributes: \" + str(numeric_att))\n",
    "print(\"   ↳ DISCRETE: \" + str(discrete_att))\n",
    "print(\"   ↳ CONTINUOUS: \" + str(continuos_att))\n",
    "print(\"- ℹ️ Confirmed TARGET Variable: \" + y_var + \" -> \" + y_var_type + \" and \" + y_var_subtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "744f2344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ✅ Sucessfull verification: combination attribute Urban_rural_code is CATEGORIC\n"
     ]
    }
   ],
   "source": [
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "make_plots = False                       # Draw plots?\n",
    "figHeight_unit = 8                      # Unitary figure height\n",
    "figWidth_unit = 12                      # Unitary figure width\n",
    "num_cols = 2                            # Number of columns per plot\n",
    "my_palette = \"pastel\"                   # Palette\n",
    "my_font_size = 15                       # Font size\n",
    "num_values_to_plot = 40                 # Max number of different values to plot (for CATEGORY_var)\n",
    "num_bins = 100                          # Num of bins (for NUMERIC_var plots)\n",
    "category_combi_att = \"Urban_rural_code\" # Combination attribute for multivariant analysis (must be a CATEGORIC attribute)\n",
    "y_var_highlighting_color = \"green\"      # Color to highlight target variable\n",
    "\n",
    " # Validation\n",
    "if not category_att:\n",
    "    print(\"- ℹ️ There are no CATEGORIC attributes in the DataFrame\")\n",
    "elif category_combi_att in category_att:\n",
    "    print(\"- ✅ Sucessfull verification: combination attribute \" +  category_combi_att + \" is CATEGORIC\")\n",
    "elif category_combi_att in numeric_att:\n",
    "    raise ValueError(\"❌ Combination attribute \" +  category_combi_att + \" for multivariant analysis must be a CATEGORY attribute!\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Combination attribute \" +  category_combi_att + \" does not exist in the DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a7250bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5 - UNIVARIABLE ANALYSIS\n",
      "⚠️ UNIVARIABLE ANALYSIS is not printed, set make_plots = True\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 5 - UNIVARIABLE ANALYSIS\n",
    "# -------------------------------\n",
    "print(\"STEP 5 - UNIVARIABLE ANALYSIS\")\n",
    "\n",
    "if not make_plots:\n",
    "    print(\"⚠️ UNIVARIABLE ANALYSIS is not printed, set make_plots = True\")\n",
    "else:\n",
    "\n",
    "    # Copy of previous DataFrame\n",
    "    df_S5 = df_S4.copy()\n",
    "\n",
    "    # Target highlighting styles\n",
    "    target_box_style = dict(facecolor='none', edgecolor=y_var_highlighting_color, linewidth=5)\n",
    "    target_title_style = dict(fontsize= my_font_size, color=y_var_highlighting_color, fontweight='bold')\n",
    "\n",
    "    # CATEGORY VARIABLES (including target if categorical)\n",
    "    print(\"🏷️ CATEGORY VARIABLES\")\n",
    "\n",
    "    if not category_att and y_var_type == \"NUMERIC\":\n",
    "        print(\"  ⚠️ This type of plot is non applicable for this case, because there are not CATEGORIC variables in the DataFrame\")\n",
    "    else:    \n",
    "        var_to_plot = category_att.copy()\n",
    "        if y_var_type == \"CATEGORIC\" and y_var not in var_to_plot:\n",
    "            var_to_plot.insert(0, y_var)\n",
    "\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols)\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=num_rows,\n",
    "            ncols=num_cols,\n",
    "            figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows)\n",
    "        )\n",
    "\n",
    "        axes = axes.flatten()\n",
    "        idx = 0\n",
    "\n",
    "        for col in var_to_plot:\n",
    "            unique_count = df_S5[col].nunique()\n",
    "\n",
    "            if unique_count > num_values_to_plot:\n",
    "                order = df_S5[col].value_counts().head(num_values_to_plot).index\n",
    "            else:\n",
    "                order = df_S5[col].value_counts().index\n",
    "\n",
    "            sns.countplot(\n",
    "                ax=axes[idx],\n",
    "                data=df_S5,\n",
    "                x=col,\n",
    "                hue=col,\n",
    "                palette=my_palette,\n",
    "                order=order,\n",
    "                legend=False\n",
    "            )\n",
    "            axes[idx].tick_params(axis='x', rotation=90, labelsize=my_font_size)\n",
    "            axes[idx].set_xlabel(\"\")\n",
    "\n",
    "            # Highlight target\n",
    "            if col == y_var:\n",
    "                axes[idx].set_title(col, **target_title_style)\n",
    "                axes[idx].add_patch(\n",
    "                    plt.Rectangle((0, 0), 1, 1, transform=axes[idx].transAxes, **target_box_style)\n",
    "                )\n",
    "            else:\n",
    "                axes[idx].set_title(col, fontdict = {\"fontsize\": my_font_size})\n",
    "\n",
    "            # Add truncated info\n",
    "            if unique_count > num_values_to_plot:\n",
    "                msg = f\"There are {unique_count} values,\\nbut only {num_values_to_plot} have been plotted\"\n",
    "                axes[idx].text(\n",
    "                    0.5, 0.9, msg,\n",
    "                    transform=axes[idx].transAxes,\n",
    "                    fontsize=my_font_size,\n",
    "                    color=\"red\",\n",
    "                    ha=\"center\", va=\"top\",\n",
    "                    bbox=dict(facecolor=\"grey\", alpha=0.25, edgecolor=\"red\")\n",
    "                )\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "        # Hide unused axes\n",
    "        for j in range(idx, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # NUMERIC VARIABLES (including target if numeric)\n",
    "    print(\"🔢 NUMERIC VARIABLES\")\n",
    "\n",
    "    if not numeric_att and y_var_type == \"CATEGORIC\":\n",
    "        print(\"  ⚠️ This type of plot is non applicable for this case, because there are not NUMERIC variables in the DataFrame\")\n",
    "    else: \n",
    "        var_to_plot = numeric_att.copy()\n",
    "        if y_var_type == \"NUMERIC\" and y_var not in var_to_plot:\n",
    "            var_to_plot.insert(0, y_var)\n",
    "\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols)\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=num_rows * 2,\n",
    "            ncols=num_cols,\n",
    "            figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows),\n",
    "            gridspec_kw={'height_ratios': [4, 0.5] * num_rows}\n",
    "        )\n",
    "\n",
    "        var_idx = 0\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "\n",
    "                if var_idx >= len(var_to_plot):\n",
    "                    axes[row * 2, col].set_visible(False)\n",
    "                    axes[row * 2 + 1, col].set_visible(False)\n",
    "                    continue\n",
    "\n",
    "                colname = var_to_plot[var_idx]\n",
    "\n",
    "                # Histogram\n",
    "                sns.histplot(\n",
    "                    ax=axes[row * 2, col],\n",
    "                    data=df_S5,\n",
    "                    x=colname,\n",
    "                    bins=num_bins\n",
    "                )\n",
    "                axes[row * 2, col].set_xlabel(\"\")\n",
    "\n",
    "                # Boxplot\n",
    "                sns.boxplot(\n",
    "                    ax=axes[row * 2 + 1, col],\n",
    "                    data=df_S5,\n",
    "                    x=colname\n",
    "                )\n",
    "                axes[row * 2 + 1, col].set_xlabel(\"\")\n",
    "\n",
    "                # Highlight target\n",
    "                if colname == y_var:\n",
    "                    axes[row * 2, col].set_title(colname, **target_title_style)\n",
    "                    axes[row * 2 + 1, col].set_title(colname, **target_title_style)\n",
    "\n",
    "                    axes[row * 2, col].add_patch(\n",
    "                        plt.Rectangle((0, 0), 1, 1, transform=axes[row * 2, col].transAxes, **target_box_style)\n",
    "                    )\n",
    "                    axes[row * 2 + 1, col].add_patch(\n",
    "                        plt.Rectangle((0, 0), 1, 1, transform=axes[row * 2 + 1, col].transAxes, **target_box_style)\n",
    "                    )\n",
    "                else:\n",
    "                    axes[row * 2, col].set_title(colname, fontdict = {\"fontsize\": my_font_size})\n",
    "\n",
    "                var_idx += 1\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "094425ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS TARGET\n",
      "⚠️ MULTIVARIANT ANALYSIS - ATTRIBUTES VS TARGET is not printed, set make_plots = True\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 6 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS TARGET\n",
    "# -------------------------------\n",
    "print(\"STEP 6 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS TARGET\")\n",
    "\n",
    "if not make_plots:\n",
    "    print(\"⚠️ MULTIVARIANT ANALYSIS - ATTRIBUTES VS TARGET is not printed, set make_plots = True\")\n",
    "else:\n",
    "    # Copy  of previous DataFrame\n",
    "    df_S6 = df_S4.copy()\n",
    "    print(\"\\n 🔢 NUMERIC Attributes VS 🏷️ CATEGORY Target\")\n",
    "\n",
    "    if not numeric_att:\n",
    "        print(\"  ⚠️ This type of plot is non applicable for this case, because there are not NUMERIC attributes in the DataFrame\")\n",
    "    elif y_var_type == \"NUMERIC\":\n",
    "        print(\"  ⚠️ This type of plot is non applicable for this case, because the target variable is NUMERIC\")\n",
    "    else:\n",
    "        # Set plotting variables\n",
    "        var_to_plot=numeric_att # Variable type for this plot\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
    "\n",
    "        # Create subplots with custom height ratios\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows,\n",
    "            ncols = num_cols * 2,\n",
    "            figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows),\n",
    "            gridspec_kw={'width_ratios': [3, 1] * num_cols})\n",
    "\n",
    "        # Loop through variables\n",
    "        var_idx = 0\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                if var_idx >= len(var_to_plot):\n",
    "                    # Hide unused subplots\n",
    "                    axes[row, col * 2].set_visible(False)\n",
    "                    axes[row , col* 2 + 1].set_visible(False)\n",
    "                    continue\n",
    "                \n",
    "                sns.stripplot(\n",
    "                    ax = axes[row, col * 2],\n",
    "                    data = df_S6,\n",
    "                    x = y_var,\n",
    "                    y = var_to_plot[var_idx],\n",
    "                    hue = y_var,\n",
    "                    alpha = 0.3,\n",
    "                    legend = False)\n",
    "                axes[row, col * 2].set_ylabel(var_to_plot[var_idx],fontdict = {\"fontsize\": my_font_size})\n",
    "                axes[row, col * 2].grid(True)\n",
    "\n",
    "                sns.boxplot(\n",
    "                    ax = axes[row, col * 2 + 1],\n",
    "                    data = df_S6,\n",
    "                    x = y_var,\n",
    "                    y = var_to_plot[var_idx],\n",
    "                    hue = y_var,\n",
    "                    palette = my_palette,\n",
    "                    legend = False)\n",
    "                axes[row, col * 2 + 1].set_ylabel(\"\")\n",
    "                axes[row, col * 2 + 1].grid(True)\n",
    "                axes[row, col * 2 + 1].set_yticklabels([])\n",
    "                \n",
    "                var_idx += 1\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    print(\"\\n 🔢 NUMERIC Attributes VS 🔢 NUMERIC Target\")\n",
    "\n",
    "    if not numeric_att:\n",
    "        print(\"  ⚠️ This type of plot is non applicable for this case, because there are not NUMERIC attributes in the DataFrame\")\n",
    "    elif y_var_type == \"CATEGORIC\":\n",
    "        print(\"  ⚠️ This type of plot is non applicable for this case, because the target variable is CATEGORIC\")\n",
    "    else:\n",
    "        # Set plotting variables\n",
    "        var_to_plot=numeric_att # Variable type for this plot\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
    "\n",
    "        # Create subplots with custom height ratios\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows * 2,\n",
    "            ncols = num_cols,\n",
    "            figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows),\n",
    "            gridspec_kw={'height_ratios': [4, 1] * num_rows})\n",
    "\n",
    "        # Loop through variables\n",
    "        var_idx = 0\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                if var_idx >= len(var_to_plot):\n",
    "                    # Hide unused subplots\n",
    "                    axes[row * 2, col].set_visible(False)\n",
    "                    axes[row * 2 + 1, col].set_visible(False)\n",
    "                    continue\n",
    "\n",
    "                # Regplot (top)\n",
    "                sns.regplot(\n",
    "                    ax = axes[row * 2, col],\n",
    "                    data = df_S6,\n",
    "                    x = var_to_plot[var_idx],\n",
    "                    y = y_var,\n",
    "                    scatter_kws = {'s': my_font_size, 'alpha': 0.6},\n",
    "                    line_kws = {'color': 'red'})\n",
    "\n",
    "                # Boxplot (bottom)\n",
    "                sns.heatmap(\n",
    "                    ax = axes[row * 2 + 1, col],\n",
    "                    data = df_S6[[var_to_plot[var_idx], y_var]].corr(),\n",
    "                    annot = True,\n",
    "                    fmt = \".2f\",\n",
    "                    cbar = False)\n",
    "\n",
    "                var_idx += 1\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n 🏷️ CATEGORY Attributes VS 🔢 NUMERIC Target\")\n",
    "\n",
    "    if not category_att:\n",
    "        print(\"  ⚠️ This type of plot is non applicable for this case, because there are not CATEGORIC attributes in the DataFrame\")\n",
    "    elif y_var_type == \"CATEGORIC\":\n",
    "        print(\"  ⚠️ This type of plot is non applicable for this case, because the target variable is CATEGORIC\")\n",
    "    else:        \n",
    "        # Set plotting variables\n",
    "        var_to_plot=category_att # Variable type for this plot\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
    "\n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows,\n",
    "            ncols = num_cols,\n",
    "            figsize = (figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
    "\n",
    "        # flatten the axes array for easier access\n",
    "        axes = axes.flatten()  \n",
    "\n",
    "        # Loop through variables\n",
    "        idx = 0\n",
    "        for col in var_to_plot:\n",
    "\n",
    "            # Count unique values\n",
    "            unique_count = df_S6[col].nunique()\n",
    "            \n",
    "            # Limit the number of plotted categories if there are more than num_values_to_plot\n",
    "            if unique_count > num_values_to_plot:\n",
    "                order = df_S6[col].value_counts().head(num_values_to_plot).index\n",
    "            else:\n",
    "                order = df_S6[col].value_counts().index\n",
    "            \n",
    "            # Create barplot\n",
    "            sns.barplot(\n",
    "                ax=axes[idx],\n",
    "                data = df_S6,\n",
    "                x = col,\n",
    "                y = y_var,\n",
    "                hue = category_combi_att,\n",
    "                order = order)\n",
    "            axes[idx].tick_params(axis='x', rotation=90, labelsize=10)\n",
    "            \n",
    "            # Add text box if truncated\n",
    "            if unique_count > num_values_to_plot:\n",
    "                msg = \"There are \" + str(unique_count) + \" different values,\\nbut only \" + str(num_values_to_plot) + \" have been plotted\"\n",
    "                axes[idx].text(\n",
    "                    x = 0.5,\n",
    "                    y = 0.9,\n",
    "                    s = msg,\n",
    "                    transform=axes[idx].transAxes,\n",
    "                    fontsize = my_font_size,\n",
    "                    color = 'red',\n",
    "                    ha = 'center',\n",
    "                    va = 'top',\n",
    "                    bbox = dict(facecolor='grey', alpha=0.5, edgecolor='red'))\n",
    "            \n",
    "            idx += 1\n",
    "\n",
    "        # Turn off unused axes if there are any\n",
    "        for j in range(idx, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\n 🏷️ CATEGORY Attributes with 🏷️ Combined CATEGORY Target\")\n",
    "\n",
    "    if not category_att:\n",
    "        print(\"  ⚠️ This type of plot is non applicable for this case, because there are not CATEGORIC attributes in the DataFrame\")\n",
    "    elif y_var_type == \"NUMERIC\":\n",
    "        print(\"  ⚠️ This type of plot is non applicable for this case, because the target variable is NUMERIC\")\n",
    "    else:\n",
    "        # Set plotting variables\n",
    "        hue_order = sorted(df_S6[y_var].dropna().unique().tolist()) # Determine hue order dynamically\n",
    "        var_to_plot=category_att # Variable type for this plot\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
    "\n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows,\n",
    "            ncols = num_cols,\n",
    "            figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
    "            \n",
    "        # Flatten the axes array for easier access\n",
    "        axes = axes.flatten()  \n",
    "\n",
    "        # Loop through variables\n",
    "        idx = 0\n",
    "        for col in var_to_plot:\n",
    "            # Count unique values\n",
    "            unique_count = df_S6[col].nunique()\n",
    "            \n",
    "            # Limit the number of plotted categories if there are more than num_values_to_plot\n",
    "            if unique_count > num_values_to_plot:\n",
    "                order = df_S6[col].value_counts().head(num_values_to_plot).index\n",
    "            else:\n",
    "                order = df_S6[col].value_counts().index\n",
    "            \n",
    "            # Create countplot\n",
    "            sns.countplot(\n",
    "                ax = axes[idx],\n",
    "                data = df_S6,\n",
    "                x = col,\n",
    "                hue = y_var,\n",
    "                hue_order = hue_order,\n",
    "                palette = my_palette,\n",
    "                order = order)\n",
    "            axes[idx].tick_params(axis='x', rotation=90, labelsize=my_font_size)\n",
    "            \n",
    "            # Add text box if truncated\n",
    "            if unique_count > num_values_to_plot:\n",
    "                msg = \"There are \" + str(unique_count) + \" different values,\\nbut only \" + str(num_values_to_plot) + \" have been plotted\"\n",
    "                axes[idx].text(\n",
    "                    x = 0.5,\n",
    "                    y = 0.9,\n",
    "                    s = msg,\n",
    "                    transform=axes[idx].transAxes,\n",
    "                    fontsize=my_font_size,\n",
    "                    color='red',\n",
    "                    ha='center',\n",
    "                    va='top',\n",
    "                    bbox=dict(facecolor='grey', alpha=0.25, edgecolor='red'))\n",
    "            \n",
    "            idx += 1\n",
    "\n",
    "        # Turn off unused axes if there are any\n",
    "        for j in range(idx, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd3540b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS ATTRIBUTES\n",
      "⚠️ MULTIVARIANT ANALYSIS - ATTRIBUTES VS ATTRIBUTES is not printed, set make_plots = True\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 7 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS ATTRIBUTES\n",
    "# -------------------------------\n",
    "print(\"STEP 7 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS ATTRIBUTES\")\n",
    "\n",
    "if not make_plots:\n",
    "    print(\"⚠️ MULTIVARIANT ANALYSIS - ATTRIBUTES VS ATTRIBUTES is not printed, set make_plots = True\")\n",
    "else:\n",
    "    # Copy of previous DataFrame\n",
    "    df_S7 = df_S4.copy()\n",
    "\n",
    "    print(\"\\n 🔢 NUMERIC Attributes VS 🔢 NUMERIC Attributes\")\n",
    "\n",
    "    var_to_plot = numeric_att\n",
    "    num_rows = len(var_to_plot) - 1  # Number of rows (one less than number of variables)\n",
    "\n",
    "    # Create subplots with two stacked plots (regplot + heatmap) per variable pair\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows = num_rows * 2,\n",
    "        ncols = len(var_to_plot) - 1,\n",
    "        figsize=(figWidth_unit * (len(var_to_plot) - 1), figHeight_unit * num_rows),\n",
    "        gridspec_kw={'height_ratios': [4, 1] * num_rows})\n",
    "\n",
    "    # Flatten axes for easy handling\n",
    "    axes = np.array(axes)\n",
    "\n",
    "    # Track subplot usage\n",
    "    for row in range(num_rows):\n",
    "        n_cols = len(var_to_plot) - row - 1  # Decreasing number of columns each row\n",
    "        for col in range(n_cols):\n",
    "\n",
    "            # Top: regplot\n",
    "            sns.regplot(\n",
    "                ax = axes[row * 2, col],\n",
    "                data = df_S7,\n",
    "                x = var_to_plot[row + col + 1],\n",
    "                y = var_to_plot[row],\n",
    "                scatter_kws = {'s': my_font_size, 'alpha': 0.6},\n",
    "                line_kws = {'color': 'red'})\n",
    "            axes[row * 2, col].set_xlabel(var_to_plot[row + col + 1], fontsize=20)\n",
    "            axes[row * 2, col].set_ylabel(var_to_plot[row], fontsize=20)\n",
    "\n",
    "            # Show Y label only for first plot in row\n",
    "            if col == 0:\n",
    "                axes[row * 2, col].set_ylabel(var_to_plot[row], fontsize=my_font_size)\n",
    "            else:\n",
    "                axes[row * 2, col].set_ylabel(\"\")\n",
    "\n",
    "            # Bottom: heatmap (correlation)\n",
    "            sns.heatmap(\n",
    "                ax = axes[row * 2 + 1, col],\n",
    "                data = df_S7[[var_to_plot[row + col + 1], var_to_plot[row]]].corr(),\n",
    "                annot = True,\n",
    "                fmt = \".2f\",\n",
    "                cbar = False,\n",
    "                annot_kws = {\"size\": 20})\n",
    "\n",
    "        # Hide unused subplots on the right for this row\n",
    "        for col in range(n_cols, len(var_to_plot) - 1):\n",
    "            axes[row * 2, col].set_visible(False)\n",
    "            axes[row * 2 + 1, col].set_visible(False)\n",
    "\n",
    "    # Adjust layout and show\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n 🏷️🔢 ALL Attributes VS 🏷️🔢 ALL Attributes\")\n",
    "\n",
    "    # Encode categorical variables using the Series.factorize() method\n",
    "    for col in category_att:\n",
    "        codes, uniques = df_S7[col].factorize()\n",
    "        df_S7[col] = codes  # replace column with integer codes\n",
    "\n",
    "    # CATEGORIC ATTRIBUTES HEATMAP\n",
    "    if len(category_att) > 1:\n",
    "        corr_cat = df_S7[category_att].corr()\n",
    "        fig = plt.figure(figsize=(figWidth_unit, figHeight_unit))\n",
    "        plt.title(\"ONLY CATEGORIC ATTRIBUTES\", fontsize=my_font_size + 2, fontweight=\"bold\")\n",
    "        sns.heatmap(\n",
    "            data=corr_cat,\n",
    "            annot=True,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            fmt=\".2f\",\n",
    "            annot_kws={\"size\": my_font_size}\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ Not enough categorical attributes for a correlation matrix.\")\n",
    "\n",
    "    # NUMERIC ATTRIBUTES HEATMAP\n",
    "    if len(numeric_att) > 1:\n",
    "        corr_num = df_S7[numeric_att].corr()\n",
    "        fig = plt.figure(figsize=(figWidth_unit, figHeight_unit))\n",
    "        plt.title(\"ONLY NUMERIC ATTRIBUTES\", fontsize=my_font_size + 2, fontweight=\"bold\")\n",
    "        sns.heatmap(\n",
    "            data=corr_num,\n",
    "            annot=True,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            fmt=\".2f\",\n",
    "            annot_kws={\"size\": my_font_size}\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ Not enough numeric attributes for a correlation matrix.\")\n",
    "\n",
    "    # ALL VARIABLES HEATMAP\n",
    "    corr_matrix = df_S7[numeric_att + category_att].corr()\n",
    "    corr_order = corr_matrix.mean().sort_values(ascending=False).index\n",
    "    corr_matrix = corr_matrix.loc[corr_order, corr_order]\n",
    "\n",
    "    fig = plt.figure(figsize=(2 * figWidth_unit, 2 * figHeight_unit))\n",
    "    plt.title(\"CATEGORIC AND NUMERIC ATTRIBUTES\", fontsize=my_font_size + 2, fontweight=\"bold\")\n",
    "    sns.heatmap(\n",
    "        data=corr_matrix,\n",
    "        annot=True,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        fmt=\".2f\",\n",
    "        annot_kws={\"size\": my_font_size}\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # PAIRPLOT (sorted by correlation order)\n",
    "    fig = plt.figure(figsize=(figWidth_unit, figHeight_unit))\n",
    "    sns.pairplot(data=df_S7[corr_order])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 8) MISSING VALUES\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "zero_to_nan = []                    # List of attributes where zero should be considered missing\n",
    "filling_threshold = 5.0             # [%] If missing perc > filling_threshold → fill values, otherwise drop rows\n",
    "grouping_max_unique = 6             # Max number of unique values for a categorical attribute to be usable as keys for grouped median\n",
    "make_missing_values_plots = True    # Make plots?\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "df_S8 = df_S4.copy()\n",
    "\n",
    "# Replace zeros by NaN for selected columns\n",
    "for col in zero_to_nan:\n",
    "    if col in df_S8.columns:\n",
    "        df_S8[col] = df_S8[col].replace(0, np.nan)\n",
    "        print(f\"- ⚠️ Values equal to 0 in '{col}' have been replaced by NaN\")\n",
    "\n",
    "# TARGET VARIABLE\n",
    "missing_y = df_S8[y_var].isnull().sum()\n",
    "\n",
    "if missing_y > 0:\n",
    "    print(f\"- ⚠️ Target variable '{y_var}' contains {missing_y} missing values → rows will be dropped.\")\n",
    "    df_S8 = df_S8.dropna(subset=[y_var])\n",
    "else:\n",
    "    print(f\"- ✅ Target variable '{y_var}' has no missing values.\")\n",
    "\n",
    "# Identify categorical variables usable as grouping keys for numeric imputation\n",
    "group_vars = []\n",
    "\n",
    "# Normal categorical attributes\n",
    "for col in category_att:\n",
    "    if df_S8[col].nunique() <= grouping_max_unique:\n",
    "        group_vars.append(col)\n",
    "\n",
    "# Add target as grouping variable if it is CATEGORICAL and has few unique values\n",
    "if y_var_type == \"CATEGORIC\":\n",
    "    if df_S8[y_var].nunique() <= grouping_max_unique:\n",
    "        group_vars.append(y_var)\n",
    "        print(f\"- ℹ️ Target variable '{y_var}' added to grouping keys for numeric imputation\")\n",
    "\n",
    "# Calculate missing percentages per column\n",
    "missing_pct = (df_S8.isnull().sum() / len(df_S8)) * 100\n",
    "missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_pct) == 0:\n",
    "    print(\"- ✅ DataFrame has no missing values at all (excluding target variable already handled)\")\n",
    "else:\n",
    "    # Process each column with missing values\n",
    "    for col in missing_pct.index:\n",
    "        pct = missing_pct[col]\n",
    "        print(f\"- ⚠️ Column: {col} → {pct:.2f}% missing\")\n",
    "\n",
    "        # CASE 1: NUMERIC ATTRIBUTE\n",
    "        if col in numeric_att:\n",
    "\n",
    "            # CASE 1A: grouped median\n",
    "            if pct > filling_threshold and len(group_vars) > 0:\n",
    "                print(f\" - ⚠️ FILLED missing numeric values in {col} using grouped median by {group_vars}...\\n\")\n",
    "\n",
    "                medians = df_S8.groupby(group_vars)[col].median().reset_index()\n",
    "                medians = medians.rename(columns={col: f\"median_{col}\"})\n",
    "\n",
    "                df_S8 = pd.merge(df_S8, medians, on=group_vars, how=\"left\")\n",
    "                df_S8[col] = df_S8[col].fillna(df_S8[f\"median_{col}\"])\n",
    "                df_S8 = df_S8.drop(columns=[f\"median_{col}\"])\n",
    "\n",
    "            # CASE 1B: global median\n",
    "            elif pct > filling_threshold and len(group_vars) == 0:\n",
    "                print(f\" - ⚠️ FILLED missing numeric values in {col} using global median (no grouping columns)...\\n\")\n",
    "                df_S8[col] = df_S8[col].fillna(df_S8[col].median())\n",
    "\n",
    "            # CASE 1C: drop rows\n",
    "            elif pct <= filling_threshold:\n",
    "                print(f\" - ⚠️ DROPPED rows with missing values in {col} ({pct:.2f}% ≤ {filling_threshold}%)...\\n\")\n",
    "                df_S8 = df_S8.dropna(subset=[col])\n",
    "\n",
    "        # CASE 2: CATEGORICAL ATTRIBUTE → mode imputation\n",
    "        elif col in category_att:\n",
    "\n",
    "            print(f\" - ⚠️ FILLED missing categorical values in {col} using mode (most frequent value)...\\n\")\n",
    "            mode_value = df_S8[col].mode().iloc[0]\n",
    "            df_S8[col] = df_S8[col].fillna(mode_value)\n",
    "\n",
    "        # CASE 3: unsupported\n",
    "        else:\n",
    "            print(f\" - ℹ️ Column {col} has unsupported type for imputation — dropping rows.\\n\")\n",
    "            df_S8 = df_S8.dropna(subset=[col])\n",
    "\n",
    "# Print results\n",
    "print(\"- ✅ Missing values have been handled successfully!\")\n",
    "print(f\" - ℹ️ Previous df's rows: {len(df_S4)}\")\n",
    "print(f\" - ℹ️ Current df's rows: {len(df_S8)}\")\n",
    "print(f\" - ℹ️ Current DataFrame shape: {df_S8.shape}\")\n",
    "print(f\" - ℹ️ Remaining missing values per column:\\n{df_S8.isnull().sum()}\")\n",
    "\n",
    "if make_missing_values_plots:\n",
    "    # BEFORE vs AFTER missing values handling\n",
    "    print(\"\\n📊 VISUAL CHECK - BEFORE vs AFTER missing values handling\")\n",
    "\n",
    "    df_S8_before = df_S4.copy()   # Before missing-value handling\n",
    "    df_S8_after = df_S8.copy()    # After missing-value handling\n",
    "\n",
    "    if not numeric_att:\n",
    "        print(\"   This type of plot is non applicable for this case, because there are not NUMERIC variables in the DataFrame\")\n",
    "\n",
    "    else:\n",
    "        var_to_plot = numeric_att.copy()\n",
    "        if y_var_type == \"NUMERIC\" and y_var not in var_to_plot:\n",
    "            var_to_plot.insert(0, y_var)\n",
    "\n",
    "        num_rows = len(var_to_plot)\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows * 2,\n",
    "            ncols = 2,\n",
    "            figsize = (figWidth_unit * 2, figHeight_unit * num_rows),\n",
    "            gridspec_kw={'height_ratios': [4, 0.5] * num_rows})\n",
    "\n",
    "        for i, colname in enumerate(var_to_plot):\n",
    "\n",
    "            # Row indices for histogram and boxplot of this variable\n",
    "            hist_row  = i * 2\n",
    "            box_row   = i * 2 + 1\n",
    "\n",
    "            # Common bins (syncronize BEFORE and AFTER)\n",
    "            xmin = min(df_S8_before[colname].min(), df_S8_after[colname].min())\n",
    "            xmax = max(df_S8_before[colname].max(), df_S8_after[colname].max())\n",
    "            common_bins = np.linspace(xmin, xmax, num_bins + 1)\n",
    "\n",
    "            # ================\n",
    "            # BEFORE PLOTS\n",
    "            # ================\n",
    "            before_hist_ax = axes[hist_row, 0]\n",
    "            before_box_ax  = axes[box_row, 0]\n",
    "\n",
    "            sns.histplot(\n",
    "                ax = before_hist_ax,\n",
    "                data = df_S8_before,\n",
    "                x = colname,\n",
    "                bins = num_bins,\n",
    "                color = \"gray\",\n",
    "                alpha = 0.35)\n",
    "            before_hist_ax.set_title(colname + \" - BEFORE\")\n",
    "            before_hist_ax.set_xlabel(\"\")\n",
    "\n",
    "            sns.boxplot(\n",
    "                ax = before_box_ax,\n",
    "                data = df_S8_before,\n",
    "                x = colname,\n",
    "                color = \"lightgray\")\n",
    "            before_box_ax.set_xlabel(\"\")\n",
    "\n",
    "            # Store BEFORE axis limits\n",
    "            xlim_hist_before = before_hist_ax.get_xlim()\n",
    "            ylim_hist_before = before_hist_ax.get_ylim()\n",
    "            xlim_box_before  = before_box_ax.get_xlim()\n",
    "\n",
    "            # ================\n",
    "            # AFTER PLOTS\n",
    "            # ================\n",
    "            after_hist_ax = axes[hist_row, 1]\n",
    "            after_box_ax  = axes[box_row, 1]\n",
    "\n",
    "            sns.histplot(\n",
    "                ax = after_hist_ax,\n",
    "                data = df_S8_after,\n",
    "                x = colname,\n",
    "                bins = common_bins)\n",
    "            after_hist_ax.set_title(colname + \" - AFTER\")\n",
    "            after_hist_ax.set_xlabel(\"\")\n",
    "\n",
    "            sns.boxplot(\n",
    "                ax = after_box_ax,\n",
    "                data = df_S8_after,\n",
    "                x = colname)\n",
    "            after_box_ax.set_xlabel(\"\")\n",
    "\n",
    "            # Syncronize axes limits\n",
    "            after_hist_ax.set_xlim(xlim_hist_before)\n",
    "            after_hist_ax.set_ylim(ylim_hist_before)\n",
    "            after_box_ax.set_xlim(xlim_box_before)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 9) CLEAN OUTLIERS\n",
    "# -------------------------------\n",
    "print(\"STEP 9) CLEAN OUTLIERS\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "low_outliers_threshold = 1.0   # [%] Max percentage of lower outliers allowed to remove\n",
    "up_outliers_threshold = 1.0  # [% ]Max percentage of upper outliers allowed to remove\n",
    "removal_type = \"EXTREME OUTLIERS\" # Removal logic type (NORMAL or EXTREME outliers)\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S9=df_S8.copy()\n",
    "\n",
    "# Print info\n",
    "display(df_S9.describe())\n",
    "\n",
    "# Outliers detection is applied over numeric attributes\n",
    "cols = []\n",
    "for col in numeric_att:\n",
    "    if col in df_S9.columns.to_list():\n",
    "        cols.append(col)\n",
    "\n",
    "# Initialize containers\n",
    "lower_limits = []\n",
    "upper_limits = []\n",
    "n_outliers_lower = []\n",
    "n_outliers_upper = []\n",
    "pct_outliers_lower = []\n",
    "pct_outliers_upper = []\n",
    "extreme_lower_limits = []\n",
    "extreme_upper_limits = []\n",
    "n_extreme_outliers_lower = []\n",
    "n_extreme_outliers_upper = []\n",
    "pct_extreme_outliers_lower = []\n",
    "pct_extreme_outliers_upper = []\n",
    "\n",
    "len(df_S9.index)\n",
    "\n",
    "for col in cols:\n",
    "    Q1 = df_S9[col].quantile(0.25)\n",
    "    Q3 = df_S9[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Calulate limits\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    extreme_lower = Q1 - 3 * IQR\n",
    "    extreme_upper = Q3 + 3 * IQR\n",
    "\n",
    "    # Count num of outliers\n",
    "    n_low = (df_S9[col] < lower).sum()\n",
    "    n_high = (df_S9[col] > upper).sum()\n",
    "    n_extreme_low = (df_S9[col] < extreme_lower).sum()\n",
    "    n_extreme_high = (df_S9[col] > extreme_upper).sum()\n",
    "\n",
    "    # Percentages of outliers\n",
    "    pct_low = (n_low / len(df_S9.index)) * 100\n",
    "    pct_high = (n_high / len(df_S9.index)) * 100\n",
    "    pct_extreme_low = (n_extreme_low / len(df_S9.index)) * 100\n",
    "    pct_extreme_high = (n_extreme_high / len(df_S9.index)) * 100\n",
    "\n",
    "    # Save limits\n",
    "    lower_limits.append(lower)\n",
    "    upper_limits.append(upper)\n",
    "    extreme_lower_limits.append(extreme_lower)\n",
    "    extreme_upper_limits.append(extreme_upper)\n",
    "\n",
    "    # Save num of outliers\n",
    "    n_outliers_lower.append(n_low)\n",
    "    n_outliers_upper.append(n_high)\n",
    "    n_extreme_outliers_lower.append(n_extreme_low)\n",
    "    n_extreme_outliers_upper.append(n_extreme_high)\n",
    "\n",
    "    # Save percentages of outliers\n",
    "    pct_outliers_lower.append(pct_low)\n",
    "    pct_outliers_upper.append(pct_high)\n",
    "    pct_extreme_outliers_lower.append(pct_extreme_low)\n",
    "    pct_extreme_outliers_upper.append(pct_extreme_high)\n",
    "\n",
    "\n",
    "# Build DataFrame with all results\n",
    "df_limits = pd.DataFrame(\n",
    "    [\n",
    "        lower_limits,\n",
    "        upper_limits,\n",
    "        n_outliers_lower,\n",
    "        n_outliers_upper,\n",
    "        pct_outliers_lower,\n",
    "        pct_outliers_upper\n",
    "    ],\n",
    "    columns=cols,\n",
    "    index=[\n",
    "        \"lower_limit\",\n",
    "        \"upper_limit\",\n",
    "        \"n_outliers_lower\",\n",
    "        \"n_outliers_upper\",\n",
    "        \"pct_outliers_lower\",\n",
    "        \"pct_outliers_upper\"\n",
    "    ]\n",
    ")\n",
    "df_extreme_limits = pd.DataFrame(\n",
    "    [\n",
    "        extreme_lower_limits,\n",
    "        extreme_upper_limits,\n",
    "        n_extreme_outliers_lower,\n",
    "        n_extreme_outliers_upper,\n",
    "        pct_extreme_outliers_lower,\n",
    "        pct_extreme_outliers_upper\n",
    "    ],\n",
    "    columns=cols,\n",
    "    index=[\n",
    "        \"extreme_lower_limit\",\n",
    "        \"extreme_upper_limit\",\n",
    "        \"n_extreme_outliers_lower\",\n",
    "        \"n_extreme_outliers_upper\",\n",
    "        \"pct_extreme_outliers_lower\",\n",
    "        \"pct_extreme_outliers_upper\"\n",
    "    ]\n",
    ")\n",
    "# Display results\n",
    "display(df_limits)\n",
    "display(df_extreme_limits)\n",
    "\n",
    "# Outliers detection is applied over numeric attributes\n",
    "cols = []\n",
    "for col in numeric_att:\n",
    "    if col in df_S9.columns.to_list():\n",
    "        cols.append(col)\n",
    "\n",
    "if removal_type == \"NORMAL OUTLIERS\":\n",
    "    # Loop through each column and apply filtering rules\n",
    "    for col in cols:\n",
    "        low_limit = df_limits.loc[\"lower_limit\", col]\n",
    "        high_limit = df_limits.loc[\"upper_limit\", col]\n",
    "        pct_low = df_limits.loc[\"pct_outliers_lower\", col]\n",
    "        pct_high = df_limits.loc[\"pct_outliers_upper\", col]\n",
    "        \n",
    "        # Remove low outliers if below threshold\n",
    "        print(f\"\\n- ATTRIBUTE {col}:\")\n",
    "        if pct_low == 0:\n",
    "            print(f\" - ℹ️ None lower outliers detected\")\n",
    "        elif pct_low <= low_outliers_threshold:\n",
    "            df_S9 = df_S9[df_S9[col] >= low_limit]\n",
    "            print(f\" - ⚠️ REMOVED lower outliers ({pct_low:.2f}% <= {low_outliers_threshold}%)\")\n",
    "        else:\n",
    "            print(f\" - ✅ KEPT lower outliers ({pct_low:.2f}% > {low_outliers_threshold}%)\")\n",
    "        \n",
    "        # Remove high outliers if below threshold\n",
    "        if pct_high == 0:\n",
    "            print(f\" - ℹ️ None upper outliers detected\")\n",
    "        elif pct_high <= up_outliers_threshold:\n",
    "            df_S9 = df_S9[df_S9[col] <= high_limit]\n",
    "            print(f\" - ⚠️ REMOVED upper outliers ({pct_high:.2f}% <= {up_outliers_threshold}%)\")\n",
    "        else:\n",
    "            print(f\" - ✅ KEPT upper outliers ({pct_high:.2f}% > {up_outliers_threshold}%)\")\n",
    "\n",
    "elif removal_type == \"EXTREME OUTLIERS\":\n",
    "    # Loop through each column and apply filtering rules\n",
    "    for col in cols:\n",
    "        low_limit = df_extreme_limits.loc[\"extreme_lower_limit\", col]\n",
    "        high_limit = df_extreme_limits.loc[\"extreme_upper_limit\", col]\n",
    "        pct_low = df_extreme_limits.loc[\"pct_extreme_outliers_lower\", col]\n",
    "        pct_high = df_extreme_limits.loc[\"pct_extreme_outliers_upper\", col]\n",
    "        \n",
    "        # Remove low outliers if below threshold\n",
    "        print(f\"\\n- ATTRIBUTE {col}:\")\n",
    "        if pct_low == 0:\n",
    "            print(f\" - ℹ️ None extreme lower outliers detected\")\n",
    "        elif pct_low <= low_outliers_threshold:\n",
    "            df_S9 = df_S9[df_S9[col] >= low_limit]\n",
    "            print(f\" - ⚠️ REMOVED extreme lower outliers ({pct_low:.2f}% <= {low_outliers_threshold}%)\")\n",
    "        else:\n",
    "            print(f\" - ✅ KEPT extreme lower outliers ({pct_low:.2f}% > {low_outliers_threshold}%)\")\n",
    "        \n",
    "        # Remove high outliers if below threshold\n",
    "        if pct_high == 0:\n",
    "            print(f\" - ℹ️ None extreme upper outliers detected\")\n",
    "        elif pct_high <= up_outliers_threshold:\n",
    "            df_S9 = df_S9[df_S9[col] <= high_limit]\n",
    "            print(f\" - ⚠️ REMOVED extreme upper outliers ({pct_high:.2f}% <= {up_outliers_threshold}%)\")\n",
    "        else:\n",
    "            print(f\" - ✅ KEPT extreme upper outliers ({pct_high:.2f}% > {up_outliers_threshold}%)\")\n",
    "\n",
    "# Print results\n",
    "print(\"- ✅ ✅Outliers have been handled successfully!\")\n",
    "print(f\" - ℹ️ Previous df's rows: {len(df_S8)}\")\n",
    "print(f\" - ℹ️ Current df's rows: {len(df_S9)}\")\n",
    "print(f\" - ℹ️ Current DataFrame shape: {df_S9.shape}\")\n",
    "display(df_S9.describe())\n",
    "\n",
    "# BEFORE vs AFTER Outliers handling\n",
    "print(\"\\n📊 VISUAL CHECK - BEFORE vs AFTER outliers handling\")\n",
    "\n",
    "df_S9_before = df_S8.copy()   # Before missing-value handling\n",
    "df_S9_after = df_S9.copy()    # After missing-value handling\n",
    "\n",
    "if not numeric_att:\n",
    "    print(\"   This type of plot is non applicable for this case, because there are not NUMERIC variables in the DataFrame\")\n",
    "\n",
    "else:\n",
    "    var_to_plot = numeric_att\n",
    "    num_rows = len(var_to_plot)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows = num_rows * 2,\n",
    "        ncols = 2,\n",
    "        figsize = (figWidth_unit * 2, figHeight_unit * num_rows),\n",
    "        gridspec_kw={'height_ratios': [4, 0.5] * num_rows})\n",
    "\n",
    "    for i, colname in enumerate(var_to_plot):\n",
    "\n",
    "        # Row indices for histogram and boxplot of this variable\n",
    "        hist_row  = i * 2\n",
    "        box_row   = i * 2 + 1\n",
    "\n",
    "        # Set common bins (syncronize BEFORE and AFTER)\n",
    "        xmin = min(df_S9_before[colname].min(), df_S9_after[colname].min())\n",
    "        xmax = max(df_S9_before[colname].max(), df_S9_after[colname].max())\n",
    "        common_bins = np.linspace(xmin, xmax, num_bins + 1)\n",
    "\n",
    "        # Set colored area limits\n",
    "        normal_low = df_limits.loc[\"lower_limit\", colname]\n",
    "        normal_up  = df_limits.loc[\"upper_limit\", colname]\n",
    "        extreme_low = df_extreme_limits.loc[\"extreme_lower_limit\", colname]\n",
    "        extreme_up  = df_extreme_limits.loc[\"extreme_upper_limit\", colname]\n",
    "\n",
    "        # ================\n",
    "        # BEFORE PLOTS\n",
    "        # ================\n",
    "        before_hist_ax = axes[hist_row, 0]\n",
    "        before_box_ax  = axes[box_row, 0]\n",
    "\n",
    "        sns.histplot(\n",
    "            ax = before_hist_ax,\n",
    "            data = df_S9_before,\n",
    "            x = colname,\n",
    "            bins = num_bins,\n",
    "            color = \"gray\",\n",
    "            alpha = 0.35)\n",
    "        before_hist_ax.set_title(colname + \" - BEFORE\")\n",
    "        before_hist_ax.set_xlabel(\"\")\n",
    "\n",
    "        sns.boxplot(\n",
    "            ax = before_box_ax,\n",
    "            data = df_S9_before,\n",
    "            x = colname,\n",
    "            color = \"lightgray\")\n",
    "        before_box_ax.set_xlabel(\"\")\n",
    "\n",
    "        # Outlier count\n",
    "        pct_low_normal  = df_limits.loc[\"pct_outliers_lower\", colname]\n",
    "        pct_high_normal = df_limits.loc[\"pct_outliers_upper\", colname]\n",
    "        pct_low_extreme  = df_extreme_limits.loc[\"pct_extreme_outliers_lower\", colname]\n",
    "        pct_high_extreme = df_extreme_limits.loc[\"pct_extreme_outliers_upper\", colname]\n",
    "\n",
    "        # NORMAL Outliers\n",
    "        if pct_low_normal > 0:\n",
    "            before_hist_ax.axvspan(normal_low, extreme_low, color=\"orange\", alpha=0.22)\n",
    "            before_box_ax.axvspan(normal_low, extreme_low, color=\"orange\", alpha=0.22)\n",
    "        if pct_high_normal > 0:\n",
    "            before_hist_ax.axvspan(extreme_up, normal_up, color=\"orange\", alpha=0.22)\n",
    "            before_box_ax.axvspan(extreme_up, normal_up, color=\"orange\", alpha=0.22)\n",
    "\n",
    "        # EXTREME Outliers\n",
    "        if pct_low_extreme > 0:\n",
    "            before_hist_ax.axvspan(xmin, extreme_low, color=\"red\", alpha=0.22)\n",
    "            before_box_ax.axvspan(xmin, extreme_low, color=\"red\", alpha=0.22)\n",
    "        if pct_high_extreme > 0:\n",
    "            before_hist_ax.axvspan(extreme_up, xmax, color=\"red\", alpha=0.22)\n",
    "            before_box_ax.axvspan(extreme_up, xmax, color=\"red\", alpha=0.22)\n",
    "\n",
    "        # Store BEFORE limits\n",
    "        xlim_hist_before = before_hist_ax.get_xlim()\n",
    "        ylim_hist_before = before_hist_ax.get_ylim()\n",
    "        xlim_box_before  = before_box_ax.get_xlim()\n",
    "\n",
    "        # ================\n",
    "        # AFTER PLOTS\n",
    "        # ================\n",
    "        after_hist_ax = axes[hist_row, 1]\n",
    "        after_box_ax  = axes[box_row, 1]\n",
    "\n",
    "        sns.histplot(\n",
    "            ax = after_hist_ax,\n",
    "            data = df_S9_after,\n",
    "            x = colname,\n",
    "            bins = common_bins)\n",
    "        after_hist_ax.set_title(colname + \" - AFTER\")\n",
    "        after_hist_ax.set_xlabel(\"\")\n",
    "\n",
    "        sns.boxplot(\n",
    "            ax = after_box_ax,\n",
    "            data = df_S9_after,\n",
    "            x = colname)\n",
    "        after_box_ax.set_xlabel(\"\")\n",
    "\n",
    "        # Check if outliers are still present in AFTER\n",
    "        normal_low_present  = (df_S9_after[colname] < normal_low).any()\n",
    "        normal_up_present   = (df_S9_after[colname] > normal_up).any()\n",
    "        extreme_low_present = (df_S9_after[colname] < extreme_low).any()\n",
    "        extreme_up_present  = (df_S9_after[colname] > extreme_up).any()\n",
    "\n",
    "        # NORMAL Outliers\n",
    "        if normal_low_present:\n",
    "            after_hist_ax.axvspan(normal_low, extreme_low, color=\"orange\", alpha=0.22)\n",
    "            after_box_ax.axvspan(normal_low, extreme_low, color=\"orange\", alpha=0.22)\n",
    "        if normal_up_present:\n",
    "            after_hist_ax.axvspan(extreme_up, normal_up, color=\"orange\", alpha=0.22)\n",
    "            after_box_ax.axvspan(extreme_up, normal_up, color=\"orange\", alpha=0.22)\n",
    "\n",
    "        # EXTREME Outliers\n",
    "        if extreme_low_present:\n",
    "            after_hist_ax.axvspan(xmin, extreme_low, color=\"red\", alpha=0.22)\n",
    "            after_box_ax.axvspan(xmin, extreme_low, color=\"red\", alpha=0.22)\n",
    "        if extreme_up_present:\n",
    "            after_hist_ax.axvspan(extreme_up, xmax, color=\"red\", alpha=0.22)\n",
    "            after_box_ax.axvspan(extreme_up, xmax, color=\"red\", alpha=0.22)\n",
    "\n",
    "        # Legends\n",
    "        before_hist_ax.legend(\n",
    "            handles=[\n",
    "                plt.Rectangle((0,0),1,1, facecolor='orange', alpha=0.45, label=\"NORMAL OUTLIERS\"),\n",
    "                plt.Rectangle((0,0),1,1, facecolor='red', alpha=0.45, label=\"EXTREME OUTLIERS\")],\n",
    "            loc=\"upper right\")\n",
    "        after_hist_ax.legend(\n",
    "            handles=[\n",
    "                plt.Rectangle((0,0),1,1, facecolor='orange', alpha=0.45, label=\"NORMAL OUTLIERS\"),\n",
    "                plt.Rectangle((0,0),1,1, facecolor='red', alpha=0.45, label=\"EXTREME OUTLIERS\")],\n",
    "            loc=\"upper right\")\n",
    "\n",
    "        # Syncronize axes limits\n",
    "        after_hist_ax.set_xlim(xlim_hist_before)\n",
    "        after_hist_ax.set_ylim(ylim_hist_before)\n",
    "        after_box_ax.set_xlim(xlim_box_before)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5119c",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- I goint to stablish a \"cut\" based on EXTREME outliers\n",
    "- The \"cut\" would be for a maximum of 1 %, higher that than that, EXTREME outliers will not be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48119caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 10) REMOVE NOISY ATTRIBUTES\n",
    "# -------------------------------\n",
    "print(\"STEP 10) REMOVE NOISY ATTRIBUTES\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "corr_threshold = 0.9 # Correlation level considered as \"too high\"\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "df_S10 = df_S9.copy()\n",
    "\n",
    "#  NUMERIC ATTRIBUTES (Pearson correlation)\n",
    "corr_matrix = df_S10[numeric_att].corr().abs()\n",
    "to_drop = set()\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if corr_matrix.iloc[i, j] > corr_threshold:\n",
    "            col_i = corr_matrix.columns[i]\n",
    "            col_j = corr_matrix.columns[j]\n",
    "            if col_i not in to_drop:\n",
    "                to_drop.add(col_i)\n",
    "\n",
    "if to_drop:\n",
    "    df_S10 = df_S10.drop(columns=list(to_drop), axis=1)\n",
    "    print(f\"- ⚠️ High NUMERIC attributes correlation detected (Pearson Corr. > {corr_threshold}). Dropped:\")\n",
    "    for col in to_drop:\n",
    "        print(f\"   • {col}\")\n",
    "else:\n",
    "    print(f\"- ✅ No NUMERIC attributes exceeded {corr_threshold} Pearson Correlation\")\n",
    "\n",
    "#  CATEGORICAL ATTRIBUTES (Cramér's V)\n",
    "def cramers_v(x, y): \n",
    "    # Step 1: confusion matrix\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    # Step 2: chi-square statistic\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    # Step 3: phi-squared\n",
    "    total_samples = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / total_samples\n",
    "    # Shape of confusion matrix\n",
    "    r, k = confusion_matrix.shape\n",
    "    num_rows = confusion_matrix.shape[0]\n",
    "    num_cols = confusion_matrix.shape[1]\n",
    "    # Step 4: bias correction (recommended formula)\n",
    "    correction = ((num_cols - 1) * (num_rows - 1)) / (total_samples - 1)\n",
    "    phi2_corrected = max(0, phi2 - correction)\n",
    "    # Corrected dimensions\n",
    "    rows_corrected = num_rows - ((num_rows - 1) ** 2) / (total_samples - 1)\n",
    "    cols_corrected = num_cols - ((num_cols - 1) ** 2) / (total_samples - 1)\n",
    "    # Step 5: compute Cramér's V\n",
    "    denominator = min(rows_corrected - 1, cols_corrected - 1)\n",
    "    if denominator <= 0:\n",
    "        return 0  # avoid division by zero for degenerate tables\n",
    "    cramers_v_value = np.sqrt(phi2_corrected / denominator)\n",
    "    return cramers_v_value\n",
    "\n",
    "to_drop_cat = set()\n",
    "\n",
    "if len(category_att) > 1:\n",
    "    for i in range(len(category_att)):\n",
    "        for j in range(i):\n",
    "            v = cramers_v(df_S10[category_att[i]], df_S10[category_att[j]])\n",
    "            if v > corr_threshold:\n",
    "                col_i = category_att[i]\n",
    "                col_j = category_att[j]\n",
    "                if col_i not in to_drop_cat:\n",
    "                    to_drop_cat.add(col_i)\n",
    "\n",
    "if to_drop_cat:\n",
    "    df_S10 = df_S10.drop(columns=list(to_drop_cat), axis=1)\n",
    "    print(f\"- ⚠️ High CATEGORICAL attributes association detected (Cramer’s V > {corr_threshold}). Dropped:\")\n",
    "    for col in to_drop_cat:\n",
    "        print(f\"   • {col}\")\n",
    "else:\n",
    "    print(f\"- ✅ No CATEGORICAL attributes exceeded {corr_threshold} Cramer’s V\")\n",
    "\n",
    "#  Print results\n",
    "print(f\"- ℹ️ Previous df's columns: {len(df_S9.columns)}\")\n",
    "print(f\"- ℹ️ Cleaned df's  columns: {len(df_S10.columns)}\")\n",
    "print(f\"- ℹ️ Final DataFrame shape: {df_S10.shape}\")\n",
    "display(df_S10.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8495fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 11) SPLIT\n",
    "# -------------------------------\n",
    "print(\"STEP 11) SPLIT\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "my_test_size = 0.2\n",
    "random_seed = 42\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_SPLIT = df_S10.copy()\n",
    "\n",
    "# Separate attributes from target variable\n",
    "X = df_SPLIT.drop(labels = y_var, axis = 1)\n",
    "y = df_SPLIT[y_var]\n",
    "\n",
    "# Make split between Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = my_test_size, random_state = random_seed)\n",
    "\n",
    "print(\"- ℹ️ Shape of DataFrames after SPLIT:\")\n",
    "print(\" - X_train:\",X_train.shape)\n",
    "print(\" - X_test:\",X_test.shape)\n",
    "print(\" - y_train:\",y_train.shape)\n",
    "print(\" - y_test:\",y_test.shape)\n",
    "\n",
    "print(\"\\n- ℹ️ Content of DataFrames after SPLIT:\")\n",
    "print(\" - X_train:\\n\",X_train.head(5))\n",
    "print(\" - X_test:\\n\",X_test.head(5))\n",
    "print(\" - y_train:\\n\",y_train.head(5))\n",
    "print(\" - y_test:\\n\",y_test.head(5))\n",
    "\n",
    "print(\"\\n- ℹ️ Info of DataFrames after SPLIT:\")\n",
    "print(\" - X_train:\",X_train.info())\n",
    "print(\" - X_test:\",X_test.info())\n",
    "print(\" - y_train:\",y_train.info())\n",
    "print(\" - y_test:\",y_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 12) SCALLING\n",
    "# -------------------------------\n",
    "print(\"STEP 12) SCALLING\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "make_scalling = False\n",
    "scaler_dic = { # Instance scaler for each numeric attribute - change manually StandardScaler() or MinMaxScaler()\n",
    "    \"Pregnancies\": StandardScaler(),\n",
    "    \"Glucose\": StandardScaler(),\n",
    "    \"BloodPressure\": StandardScaler(),\n",
    "    \"SkinThickness\": StandardScaler(),\n",
    "    \"Insulin\": StandardScaler(),\n",
    "    \"BMI\": StandardScaler(),\n",
    "    \"DiabetesPedigreeFunction\": StandardScaler(),\n",
    "    \"Age\": StandardScaler(),\n",
    "}\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "X_train_es = X_train.copy()\n",
    "X_test_es = X_test.copy()\n",
    "\n",
    "if not numeric_att:\n",
    "    X_train_es = X_train_es[numeric_att]\n",
    "    X_test_es = X_test_es[numeric_att]\n",
    "    print(\"  ⚠️ SCALLING is non applicable for this case, because there are not NUMERIC attributes in the DataFrame\")\n",
    "elif not make_scalling:\n",
    "    X_train_es = X_train_es[numeric_att]\n",
    "    X_test_es = X_test_es[numeric_att]\n",
    "    print(\"  ⚠️ SCALLING is is not carried out, set make_scalling = True\")\n",
    "    display(X_train_es.head())\n",
    "else:\n",
    "\n",
    "    # Fit scalers ONLY on train data\n",
    "    for col in numeric_att:\n",
    "        scaler_dic[col].fit(X_train_es[[col]])\n",
    "    print(\"- ✅ All Scalers have been trained successfully\")\n",
    "\n",
    "    # Apply scalers and create scaled columns\n",
    "    scaled_cols = []\n",
    "\n",
    "    for col in numeric_att:\n",
    "        # Detect scaler type\n",
    "        scaler_name = scaler_dic[col].__class__.__name__\n",
    "        # Set suffix\n",
    "        if scaler_name == \"StandardScaler\":\n",
    "            suffix = \"_SS\"\n",
    "        elif scaler_name == \"MinMaxScaler\":\n",
    "            suffix = \"_MM\"\n",
    "        else:\n",
    "            suffix = \"_Scaled\"\n",
    "\n",
    "        # Transform\n",
    "        X_train_es[col + suffix] = scaler_dic[col].transform(X_train_es[[col]])\n",
    "        X_test_es[col + suffix]  = scaler_dic[col].transform(X_test_es[[col]])\n",
    "\n",
    "        scaled_cols.append(col + suffix)\n",
    "        print(f\"  - ℹ️ Train/Test scaled for: {col} using {scaler_name} → new column: {col + suffix}\")\n",
    "\n",
    "    # Keep only scaled columns\n",
    "    X_train_es = X_train_es[scaled_cols]\n",
    "    X_test_es  = X_test_es[scaled_cols]\n",
    "\n",
    "    print(\"- ✅ Final scaled datasets created successfully!\")\n",
    "    display(X_train_es.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb96b995",
   "metadata": {},
   "source": [
    "CONCLUSION:\n",
    "- I do not make scalling beause Decision Tree is an algorithm that does not need scalled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866305f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 13) ENCODING\n",
    "# -------------------------------\n",
    "print(\"STEP 13) ENCODING\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "make_encoding = True\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "X_train_en = X_train.copy()\n",
    "X_test_en = X_test.copy()\n",
    "\n",
    "if not category_att:\n",
    "    X_train_en = X_train_en[category_att]\n",
    "    X_test_en = X_test_en[category_att]\n",
    "    print(\"  ⚠️ ENCODING is non applicable for this case, because there are not CATEGORIC attributes in the DataFrame\")\n",
    "elif not make_encoding:\n",
    "    X_train_en = X_train_en[category_att]\n",
    "    X_test_en = X_test_en[category_att]\n",
    "    print(\"  ⚠️ ENCODING is is not carried out, set make_encoding = True\")\n",
    "    display(X_train_en.head())\n",
    "else:\n",
    "    # List of columns\n",
    "    columns = X_train_en.columns.tolist()\n",
    "\n",
    "    # Create encoder instance for each categorical attribute\n",
    "    encoder_dic = {}\n",
    "    for col in category_att:\n",
    "\n",
    "        if col in binary_att:\n",
    "            encoder_dic[col] = LabelEncoder()\n",
    "            print(f\"- Encoder instanced for {col}: LabelEncoder()\")\n",
    "\n",
    "        elif col in multiclass_att:\n",
    "            encoder_dic[col] = OneHotEncoder(sparse_output=False)\n",
    "            print(f\"- Encoder instanced for {col}: OneHotEncoder()\")\n",
    "\n",
    "    print(\"- ✅ All Encoders have been instanced successfully\")\n",
    "\n",
    "    # Train encoders with TRAIN data only\n",
    "    for col in category_att:\n",
    "\n",
    "        encoder = encoder_dic[col]\n",
    "\n",
    "        if isinstance(encoder, LabelEncoder):\n",
    "            encoder.fit(X_train_en[col])        # LabelEncoder needs 1D\n",
    "\n",
    "        elif isinstance(encoder, OneHotEncoder):\n",
    "            encoder.fit(X_train_en[[col]])      # OHE needs 2D\n",
    "\n",
    "    print(\"- ✅ All Encoders have been trained successfully\")\n",
    "\n",
    "    # Apply encoders to TRAIN + TEST\n",
    "    for col in category_att:\n",
    "\n",
    "        encoder = encoder_dic[col]\n",
    "\n",
    "        if isinstance(encoder, LabelEncoder):\n",
    "\n",
    "            X_train_en[col + \"_LE\"] = encoder.transform(X_train_en[col])\n",
    "            X_test_en[col + \"_LE\"] = encoder.transform(X_test_en[col])\n",
    "\n",
    "            print(f\"- ✅ {col} encoded with LabelEncoder()\")\n",
    "\n",
    "        elif isinstance(encoder, OneHotEncoder):\n",
    "\n",
    "            # Transform train and test\n",
    "            train_encoded = encoder.transform(X_train_en[[col]])\n",
    "            test_encoded = encoder.transform(X_test_en[[col]])\n",
    "\n",
    "            # New names\n",
    "            ohe_colnames = encoder.get_feature_names_out([col])\n",
    "            ohe_colnames = [name + \"_OHE\" for name in ohe_colnames]\n",
    "\n",
    "            # Convert to DataFrames\n",
    "            train_ohe_df = pd.DataFrame(train_encoded, index=X_train_en.index, columns=ohe_colnames)\n",
    "            test_ohe_df = pd.DataFrame(test_encoded, index=X_test_en.index, columns=ohe_colnames)\n",
    "\n",
    "            # Concatenate new cols\n",
    "            X_train_en = pd.concat([X_train_en, train_ohe_df], axis=1)\n",
    "            X_test_en = pd.concat([X_test_en, test_ohe_df], axis=1)\n",
    "\n",
    "            print(f\"- ✅ {col} encoded with OneHotEncoder()\")\n",
    "\n",
    "    # Keep only encoded columns\n",
    "    encoded_cols = []\n",
    "    for col in category_att:\n",
    "        encoder = encoder_dic[col]\n",
    "        if isinstance(encoder, LabelEncoder):\n",
    "            encoded_cols.append(col + \"_LE\")\n",
    "        elif isinstance(encoder, OneHotEncoder):\n",
    "            ohe_colnames = encoder.get_feature_names_out([col])\n",
    "            for name in ohe_colnames:\n",
    "                encoded_cols.append(name + \"_OHE\")\n",
    "\n",
    "    X_train_en = X_train_en[encoded_cols]\n",
    "    X_test_en = X_test_en[encoded_cols]\n",
    "\n",
    "    print(\"- ✅ Final encoded datasets created successfully\")\n",
    "    X_train_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 14) FEATURE SELECTION\n",
    "# -------------------------------\n",
    "print(\"STEP 14) FEATURE SELECTION\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "feature_keeping_threshold = 100 # [%] Percentaje of features to keep (SelectKBest) \n",
    "\n",
    "# Concatenate NUMERIC_var_scaled with CATEGORY_var_encoded\n",
    "X_train_assembled = pd.concat([X_train_es, X_train_en], axis=1)\n",
    "X_test_assembled = pd.concat([X_test_es, X_test_en], axis=1)\n",
    "\n",
    "# Instance selector\n",
    "num_features_to_keep = round(feature_keeping_threshold/100 * len(X_train_assembled.columns))\n",
    "selection_model = SelectKBest(score_func = f_classif, k = num_features_to_keep)\n",
    "print(\"- ✅ Selector have been instanced successfully to keep \" + str(num_features_to_keep) + \" features\")\n",
    "\n",
    "# Train selector with ONLY train data (y_train must be included because this is SUPERVISED selector)\n",
    "selection_model.fit(X_train_assembled, y_train)\n",
    "print(\"- ✅ Selector have been trained successfully\")\n",
    "\n",
    "# Drop non-selected features\n",
    "keeping_mask = selection_model.get_support()\n",
    "X_train_assembled = pd.DataFrame(selection_model.transform(X_train_assembled), columns = X_train_assembled.columns.values[keeping_mask])\n",
    "X_test_assembled = pd.DataFrame(selection_model.transform(X_test_assembled), columns = X_test_assembled.columns.values[keeping_mask])\n",
    "\n",
    "# Print results\n",
    "print(\"\\n 🧮 X_train_assembled\", X_train_assembled.shape)\n",
    "display(X_train_assembled.head())\n",
    "print(\"\\n 🧮 X_test_assembled\", X_test_assembled.shape)\n",
    "display(X_test_assembled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ae6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 15) SAVE PROCESSED DATA\n",
    "# -------------------------------\n",
    "print(\"STEP 15) SAVE PROCESSED DATA\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "output_path = \"../data/processed/\"\n",
    "\n",
    "# Set from previous DataFrame\n",
    "X_train_final = X_train_assembled\n",
    "X_test_final = X_test_assembled\n",
    "y_train_final = y_train\n",
    "y_test_final = y_test\n",
    "\n",
    "# Get revision number - Returns the next free integer revision based on existing files\n",
    "import os\n",
    "def get_revision_number(base_path, base_name):\n",
    "    rev = 0\n",
    "    while True:\n",
    "        full_path = os.path.join(base_path, base_name + \"_\" + str(rev) + \".csv\")\n",
    "        if not os.path.exists(full_path):\n",
    "            return rev\n",
    "        rev += 1\n",
    "\n",
    "# Build filenames WITH revision number\n",
    "rev_number = get_revision_number(output_path, \"X_train_final\")\n",
    "suffix = \"_\" + str(rev_number)\n",
    "\n",
    "output_path_X_train = output_path + \"X_train_final\" + suffix + \".csv\"\n",
    "output_path_X_test  = output_path + \"X_test_final\"  + suffix + \".csv\"\n",
    "output_path_y_train = output_path + \"y_train_final\" + suffix + \".csv\"\n",
    "output_path_y_test  = output_path + \"y_test_final\"  + suffix + \".csv\"\n",
    "\n",
    "# Save all datasets\n",
    "X_train_final.to_csv(output_path_X_train, index=False)\n",
    "X_test_final.to_csv(output_path_X_test, index=False)\n",
    "y_train_final.to_csv(output_path_y_train, index=False)\n",
    "y_test_final.to_csv(output_path_y_test, index=False)\n",
    "\n",
    "print(\"- ✅ Files saved with revision number:\", rev_number)\n",
    "print(\"- 💡 Reminder: data/processed folder is ignored in .gitignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 16) PREDICTION MODELS\n",
    "# -------------------------------\n",
    "print(\"STEP 16) PREDICTION MODEL\")\n",
    "\n",
    "# |||||||||||||||||| \n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "rev_to_use = 0  # Enter the desired revision number\n",
    "grid_cross_validation = 10\n",
    "grid_classification_scoring = \"f1\" # Choose between: \"accuracy\", \"precision\", \"recall\", \"f1\"\n",
    "grid_regression_scoring = \"r2\" # Choose between: \"neg_root_mean_squared_error\", \"r2\"\n",
    "\n",
    "classification_models_selection = {\n",
    "    \"LogisticRegression\": False,\n",
    "    \"RandomForestClassifier\": False,\n",
    "    \"DecisionTreeClassifier\": True\n",
    "}\n",
    "\n",
    "regression_models_selection = {\n",
    "    \"LinearRegression\": False,\n",
    "    \"DecisionTreeRegressor\": True,\n",
    "    \"RandomForestRegressor\": False,\n",
    "    \"Lasso\": False,\n",
    "    \"Ridge\": False\n",
    "}\n",
    "\n",
    "classification_grids = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
    "    },\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"max_depth\": [None, 5, 10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10, 20, 30]\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [None, 5, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "regression_grids = {\n",
    "    \"LinearRegression\": {},\n",
    "    \"Lasso\": {\n",
    "        \"alpha\": [0.001, 0.01, 0.1, 1, 10],\n",
    "        \"max_iter\": [1000, 5000, 10000]\n",
    "    },\n",
    "    \"Ridge\": {\n",
    "        \"alpha\": [0.001, 0.01, 0.1, 1, 10],\n",
    "        \"max_iter\": [1000, 5000, 10000]\n",
    "    },\n",
    "    \"DecisionTreeRegressor\": {\n",
    "        \"criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"],\n",
    "        \"max_depth\": [None, 5, 10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10, 20, 30]\n",
    "    },\n",
    "    \"RandomForestRegressor\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [None, 5, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load processed data according to selected revision\n",
    "try:\n",
    "    X_train_model = pd.read_csv(output_path + \"X_train_final_\" + str(rev_to_use) + \".csv\")\n",
    "    X_test_model  = pd.read_csv(output_path + \"X_test_final_\" + str(rev_to_use) + \".csv\")\n",
    "    y_train_model = pd.read_csv(output_path + \"y_train_final_\" + str(rev_to_use) + \".csv\").squeeze()\n",
    "    y_test_model  = pd.read_csv(output_path + \"y_test_final_\" + str(rev_to_use) + \".csv\").squeeze()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(\n",
    "        f\"❌ The selected revision rev={rev_to_use} does NOT exist.\\n\"\n",
    "        f\"Missing file: {e.filename}\\n\"\n",
    "        f\"Please check available revision numbers in '../data/processed/'.\"\n",
    "    )\n",
    "print(f\"➡️ Loaded revision {rev_to_use}\")\n",
    "\n",
    "# Available models\n",
    "classification_available = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=random_seed),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(random_state=random_seed),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=random_seed)\n",
    "}\n",
    "\n",
    "regression_available = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Lasso\": Lasso(random_state=random_seed),\n",
    "    \"Ridge\": Ridge(random_state=random_seed),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=random_seed),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=random_seed)\n",
    "}\n",
    "\n",
    "#  Auxiliary functions\n",
    "def compute_classification_metrics(y_true, y_pred, avg, pos_label):\n",
    "    metrics = {}\n",
    "    metrics[\"Accuracy\"]  = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"Precision\"] = precision_score(y_true, y_pred, average=avg, pos_label=pos_label)\n",
    "    metrics[\"Recall\"]    = recall_score(y_true, y_pred, average=avg, pos_label=pos_label)\n",
    "    metrics[\"F1_score\"]  = f1_score(y_true, y_pred, average=avg, pos_label=pos_label)\n",
    "    return metrics\n",
    "\n",
    "def compute_regression_metrics(y_true, y_pred):\n",
    "    metrics = {}\n",
    "    metrics[\"MAE\"]  = mean_absolute_error(y_true, y_pred)\n",
    "    metrics[\"MSE\"]  = mean_squared_error(y_true, y_pred)\n",
    "    metrics[\"RMSE\"] = np.sqrt(metrics[\"MSE\"])\n",
    "    metrics[\"R2\"]   = r2_score(y_true, y_pred)\n",
    "    return metrics\n",
    "\n",
    "def set_average_proposal(y):\n",
    "    unique_count = y.nunique()\n",
    "    if unique_count == 2:\n",
    "        freq = y.value_counts()\n",
    "        pos_label = freq.index[-1]\n",
    "        return \"binary\", pos_label, grid_classification_scoring\n",
    "\n",
    "    freq_norm = y.value_counts(normalize=True)\n",
    "    imbalance_ratio = freq_norm.max() / freq_norm.min()\n",
    "\n",
    "    if imbalance_ratio <= 1.2:\n",
    "        return \"micro\", None, grid_classification_scoring + \"_micro\" if grid_classification_scoring != \"accuracy\" else grid_classification_scoring\n",
    "    if imbalance_ratio <= 1.5:\n",
    "        return \"macro\", None, grid_classification_scoring + \"_macro\" if grid_classification_scoring != \"accuracy\" else grid_classification_scoring\n",
    "    return \"weighted\", None, grid_classification_scoring + \"_weighted\" if grid_classification_scoring != \"accuracy\" else grid_classification_scoring\n",
    "\n",
    "def left_align(df):\n",
    "    return df.style.set_table_styles(\n",
    "        [{'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "\n",
    "# List with selected models\n",
    "classification_selected_list = []\n",
    "regression_selected_list = []\n",
    "for model_name, active in classification_models_selection.items():\n",
    "    if active:\n",
    "        classification_selected_list.append(model_name)\n",
    "for model_name, active in regression_models_selection.items():\n",
    "    if active:\n",
    "        regression_selected_list.append(model_name)\n",
    "\n",
    "# Auto-add Lasso/Ridge if LinearRegression is selected\n",
    "if \"LinearRegression\" in regression_selected_list:\n",
    "    if \"Lasso\" not in regression_selected_list:\n",
    "        regression_selected_list.append(\"Lasso\")\n",
    "    if \"Ridge\" not in regression_selected_list:\n",
    "        regression_selected_list.append(\"Ridge\")\n",
    "\n",
    "print(f\"\\n - Classification models selected: {classification_selected_list}\")\n",
    "print(f\" - Regression models selected:     {regression_selected_list}\")\n",
    "\n",
    "# Parameters needed for classification metrics\n",
    "if len(classification_selected_list) > 0:\n",
    "    proposed_avg, proposed_pos, proposed_score = set_average_proposal(y_train_model)\n",
    "\n",
    "# ======================================================\n",
    "#  CLASSIFICATION MODELS\n",
    "# ======================================================\n",
    "trained_models = {} \n",
    "default_results_class = {}\n",
    "optimized_results_class = {}\n",
    "for model_name in classification_selected_list:\n",
    "    # Instance DEFAULT model\n",
    "    default_model = classification_available[model_name] \n",
    "    # Train DEFAULT model\n",
    "    default_model.fit(X_train_model, y_train_model)\n",
    "    # Predict with trained DEFAULT model\n",
    "    y_pred_train = default_model.predict(X_train_model)\n",
    "    y_pred_test  = default_model.predict(X_test_model)\n",
    "    # Calculate metricts for DEFAULT model\n",
    "    metrics_train = compute_classification_metrics(y_train_model, y_pred_train, proposed_avg, proposed_pos)\n",
    "    metrics_test  = compute_classification_metrics(y_test_model, y_pred_test, proposed_avg, proposed_pos)\n",
    "    # Build final table with results for DEFAULT model\n",
    "    default_results_class[\"DEFAULT \" + model_name + \" - 🏋️ TRAIN\"] = metrics_train\n",
    "    default_results_class[\"DEFAULT \" + model_name + \" - 🧪 TEST\"]  = metrics_test\n",
    "    # Store trained model\n",
    "    trained_models[model_name] = {\"default\": default_model, \"optimized\": None}\n",
    "\n",
    "    # Set GRID parameters\n",
    "    grid_params = classification_grids[model_name]\n",
    "    if len(grid_params) > 0:\n",
    "        # Instance GRID\n",
    "        grid = GridSearchCV(\n",
    "            estimator = classification_available[model_name],\n",
    "            param_grid = grid_params,\n",
    "            scoring = proposed_score,\n",
    "            cv = grid_cross_validation)\n",
    "        # Train GRID\n",
    "        grid.fit(X_train_model, y_train_model)\n",
    "        # Get best estimator configuration (OPTIMIZED model)\n",
    "        best_model = grid.best_estimator_\n",
    "        # Predict with trained OPTIMIZED model\n",
    "        y_train_opt = best_model.predict(X_train_model)\n",
    "        y_test_opt  = best_model.predict(X_test_model)\n",
    "        # Calculate metricts for OPTIMIZED model\n",
    "        metrics_train_opt = compute_classification_metrics(y_train_model, y_train_opt, proposed_avg, proposed_pos)\n",
    "        metrics_test_opt  = compute_classification_metrics(y_test_model,  y_test_opt,  proposed_avg, proposed_pos)\n",
    "        # Build final table with results for OPTIMIZED model\n",
    "        optimized_results_class[\"OPTIMIZED \" + model_name + \" - 🏋️ TRAIN\"] = {**metrics_train_opt, \"Best Parameters\": grid.best_params_}\n",
    "        optimized_results_class[\"OPTIMIZED \" + model_name + \" - 🧪 TEST\"]  = {**metrics_test_opt,  \"Best Parameters\": grid.best_params_}\n",
    "        # Store trained model\n",
    "        trained_models[model_name][\"optimized\"] = best_model\n",
    "    else:\n",
    "        optimized_results_class[\"DEFAULT \" + model_name + \" - 🏋️ TRAIN\"] = {**metrics_train, \"Best Parameters\": \"N/A\"}\n",
    "        optimized_results_class[\"DEFAULT \" + model_name + \" - 🧪 TEST\"]  = {**metrics_test,  \"Best Parameters\": \"N/A\"}\n",
    "\n",
    "# ======================================================\n",
    "#  REGRESSION MODELS\n",
    "# ======================================================\n",
    "default_results_reg = {}\n",
    "optimized_results_reg = {}\n",
    "for model_name in regression_selected_list:\n",
    "    # Instance DEFAULT model\n",
    "    default_model = regression_available[model_name]\n",
    "    # Train DEFAULT model\n",
    "    default_model.fit(X_train_model, y_train_model)\n",
    "    # Predict with trained DEFAULT model\n",
    "    y_pred_train = default_model.predict(X_train_model)\n",
    "    y_pred_test  = default_model.predict(X_test_model)\n",
    "    # Calculate metricts for DEFAULT model\n",
    "    metrics_train = compute_regression_metrics(y_train_model, y_pred_train)\n",
    "    metrics_test  = compute_regression_metrics(y_test_model, y_pred_test)\n",
    "    # Build final table with results for DEFAULT model\n",
    "    default_results_reg[\"DEFAULT \" + model_name + \" - 🏋️ TRAIN\"] = metrics_train\n",
    "    default_results_reg[\"DEFAULT \" + model_name + \" - 🧪 TEST\"]  = metrics_test\n",
    "    # Store trained model\n",
    "    trained_models[model_name] = {\"default\": default_model, \"optimized\": None}\n",
    "\n",
    "    # Set GRID parameters\n",
    "    grid_params = regression_grids[model_name]\n",
    "    if len(grid_params) > 0:\n",
    "        # Instance GRID\n",
    "        grid = GridSearchCV(\n",
    "            estimator = regression_available[model_name],\n",
    "            param_grid = grid_params,\n",
    "            scoring = grid_regression_scoring,\n",
    "            cv = grid_cross_validation)\n",
    "        # Train GRID\n",
    "        grid.fit(X_train_model, y_train_model)\n",
    "        # Get best estimator configuration (OPTIMIZED model)\n",
    "        best_model = grid.best_estimator_\n",
    "        # Predict with trained OPTIMIZED model\n",
    "        y_train_opt = best_model.predict(X_train_model)\n",
    "        y_test_opt  = best_model.predict(X_test_model)\n",
    "        # Calculate metricts for OPTIMIZED model\n",
    "        metrics_train_opt = compute_regression_metrics(y_train_model, y_train_opt)\n",
    "        metrics_test_opt  = compute_regression_metrics(y_test_model,  y_test_opt)\n",
    "        # Build final table with results for OPTIMIZED model\n",
    "        optimized_results_reg[\"OPTIMIZED \" + model_name + \" - 🏋️ TRAIN\"] = {**metrics_train_opt, \"Best Parameters\": grid.best_params_}\n",
    "        optimized_results_reg[\"OPTIMIZED \" + model_name + \" - 🧪 TEST\"]  = {**metrics_test_opt,  \"Best Parameters\": grid.best_params_}\n",
    "        # Store trained model\n",
    "        trained_models[model_name][\"optimized\"] = best_model\n",
    "    else:\n",
    "        optimized_results_reg[\"DEFAULT \" + model_name + \" - 🏋️ TRAIN\"] = {**metrics_train, \"Best Parameters\": \"N/A\"}\n",
    "        optimized_results_reg[\"DEFAULT \" + model_name + \" - 🧪 TEST\"]  = {**metrics_test,  \"Best Parameters\": \"N/A\"}\n",
    "\n",
    "# ======================================================\n",
    "#  FINAL TABLES\n",
    "# ======================================================\n",
    "print(\"\\n==================== ⚖️ FINAL CLASSIFICATION TABLES ====================\")\n",
    "display(left_align(pd.DataFrame(default_results_class).T))\n",
    "display(left_align(pd.DataFrame(optimized_results_class).T))\n",
    "\n",
    "print(\"\\n==================== 📈 FINAL REGRESSION TABLES ========================\")\n",
    "display(left_align(pd.DataFrame(default_results_reg).T))\n",
    "display(left_align(pd.DataFrame(optimized_results_reg).T))\n",
    "\n",
    "# ======================================================\n",
    "# PLOTTING (DEFAULT vs OPTIMIZED)\n",
    "# ======================================================\n",
    "\n",
    "# registry of plotters\n",
    "model_plotters = {}\n",
    "\n",
    "# --- Register Decision Tree plotter ---\n",
    "def plot_tree_model(model, X, ax):\n",
    "    plot_tree(model, feature_names=X.columns, filled=True, ax=ax)\n",
    "\n",
    "model_plotters[\"DecisionTreeClassifier\"] = plot_tree_model\n",
    "model_plotters[\"DecisionTreeRegressor\"]  = plot_tree_model\n",
    "\n",
    "# ======================================================\n",
    "# EXECUTE PLOTS FOR MODELS THAT SUPPORT IT\n",
    "# ======================================================\n",
    "\n",
    "for model_name, model_dict in trained_models.items():\n",
    "    # Only plot models that have a registered plotter\n",
    "    if model_name not in model_plotters:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n==================== 📊 MODEL PLOT: {model_name} ====================\")\n",
    "\n",
    "    # Retrieve the specific plotter for this model type\n",
    "    plotter = model_plotters[model_name]\n",
    "\n",
    "    # Create figure with 2 subplots (default vs optimized)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(28, 10))\n",
    "\n",
    "    # DEFAULT\n",
    "    plotter(model_dict[\"default\"], X_train_model, axes[0])\n",
    "    axes[0].set_title(f\"{model_name} - DEFAULT\", fontsize=my_font_size)\n",
    "\n",
    "    # OPTIMIZED\n",
    "    plotter(model_dict[\"optimized\"], X_train_model, axes[1])\n",
    "    axes[1].set_title(f\"{model_name} - OPTIMIZED\", fontsize=my_font_size)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88178137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 17) SAVE MODELS\n",
    "# -------------------------------\n",
    "print(\"STEP 17) SAVE MODELS\")\n",
    "\n",
    "# |||||||||||||||||| \n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "models_output_path = \"../models/\"   # Folder where models will be saved\n",
    "\n",
    "# Iterate over all trained models (default + optimized)\n",
    "for model_name, model_dict in trained_models.items():\n",
    "    # DEFAULT\n",
    "    if model_dict[\"default\"] is not None:\n",
    "        filename_default = models_output_path + f\"{model_name}_DEFAULT_{rev_to_use}.sav\"\n",
    "        dump(model_dict[\"default\"], open(filename_default, \"wb\"))\n",
    "\n",
    "    # OPTIMIZED\n",
    "    if model_dict[\"optimized\"] is not None:\n",
    "        filename_optimized = models_output_path + f\"{model_name}_OPTIMIZED_{rev_to_use}.sav\"\n",
    "        dump(model_dict[\"optimized\"], open(filename_optimized, \"wb\"))\n",
    "\n",
    "print(\"\\n✅ All models saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
